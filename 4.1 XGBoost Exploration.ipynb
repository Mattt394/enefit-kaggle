{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "854b4408-4409-443f-90f4-e4a75e8dc6f7",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2066051b-3084-4df6-90b9-4f9fa7a617d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30aee0c5-dfd9-4b3e-b075-78768874b51e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    \"\"\"Processes Train data, using train data as a warm start, and prepares it for inference.\"\"\"\n",
    "\n",
    "    def __init__(self, train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices):\n",
    "        self.test_orig_dfs = self.get_test_orig_dfs([train.copy(), revealed_targets.copy(), client.copy(), historical_weather.copy(),\n",
    "                 forecast_weather.copy(), electricity_prices.copy(), gas_prices.copy()])\n",
    "        self.train = self.init_train(train)\n",
    "        self.revealed_targets = self.init_revealed_targets(revealed_targets)\n",
    "        self.client = self.init_client(client)\n",
    "        self.weather_mapping = self.init_weather_mapping()\n",
    "        self.historical_weather = self.init_historical_weather(historical_weather)\n",
    "        self.forecast_weather = self.init_forecast_weather(forecast_weather)\n",
    "        self.electricity_prices = self.init_electricity(electricity_prices)\n",
    "        self.gas_prices = self.init_gas_prices(gas_prices)\n",
    "        \n",
    "        self.df_all_cols = self.join_data(self.train, self.revealed_targets, self.client, self.historical_weather, self.forecast_weather, self.electricity_prices, self.gas_prices)\n",
    "        self.df = self.remove_cols(self.df_all_cols)\n",
    "        \n",
    "    def get_test_orig_dfs(self, dfs):\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['prediction_datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'prediction_datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df.date).dt.date\n",
    "                col = 'date'\n",
    "\n",
    "            test_date = df[col].iloc[-1]  # Assuming test is a DataFrame\n",
    "            start_date = test_date - pd.Timedelta(days=14)\n",
    "            historical_subset = df[df[col] >= start_date]\n",
    "            dfs[i] = historical_subset\n",
    "        return dfs\n",
    "        \n",
    "    def init_train(self, df):\n",
    "        \"\"\"Prepares the training data for model training.\"\"\"\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        except Exception as e:\n",
    "            df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "        df['date'] = df.datetime.dt.date\n",
    "            \n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        return df\n",
    "    \n",
    "    def add_electricity_lag_features(self, df):\n",
    "        \"\"\"Chatgpt summary:\n",
    "        Enhances a DataFrame with electricity price lag features:\n",
    "        - Sets 'datetime' as Index for time series analysis.\n",
    "        - Calculates rolling 7-day mean price, lagged by one day.\n",
    "        - Computes rolling 7-day mean for same hour, lagged.\n",
    "        - Adds column for yesterday's price, shifted by 24 hours.\n",
    "        - Calculates 24-hour rolling average of electricity prices.\n",
    "        - Resets index and drops 'forecast_date', 'origin_date', 'hour'.\n",
    "        \"\"\"\n",
    "        ##### mean from entire last week\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # Use rolling to calculate mean price of the last week\n",
    "        # The window is 7 days, min_periods can be set as per requirement\n",
    "        # 'closed' determines which side of the interval is closed; it can be 'right' or 'left'\n",
    "        df['mean_euros_per_mwh_last_week'] = df['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()\n",
    "        # Shift the results to align with the requirement of lagging\n",
    "        df['mean_euros_per_mwh_last_week'] = df['mean_euros_per_mwh_last_week'].shift()\n",
    "        \n",
    "        ##### mean from last week this hour only\n",
    "        # Extract hour from datetime\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        # Group by hour and apply rolling mean for each group\n",
    "        hourly_groups = df.groupby('hour')\n",
    "        dff = hourly_groups['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()#.shift()#.reset_index(level=0, drop=True)\n",
    "        dff = dff.reset_index().set_index('datetime').groupby('hour')['euros_per_mwh'].shift()\n",
    "        dff = dff.rename('mean_euros_per_mwh_same_hour_last_week')\n",
    "        df = df.join(dff)\n",
    "        #### yesterday's power price\n",
    "        df['yesterdays_euros_per_mwh'] = df['euros_per_mwh'].shift(24)\n",
    "        \n",
    "        ### 24h average\n",
    "        # Calculate the 24-hour rolling average\n",
    "        df['euros_per_mwh_24h_average_price'] = df['euros_per_mwh'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "        # Resetting the index if needed\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['forecast_date', 'origin_date', 'hour'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def init_electricity(self, df):\n",
    "        ## LAG = 1 Day\n",
    "        ## Move forecast datetime ahead by 1 day\n",
    "        ## change name to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_date'])\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        df = self.add_electricity_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_historical_weather_lag_features(self, df):\n",
    "        \"\"\"Chatgpt summary:\n",
    "        Enhances a DataFrame with historical weather lag features:\n",
    "        - Converts 'datetime' to Datetime object and sets as index.\n",
    "        - Sorts data by 'datetime', 'latitude', 'longitude'.\n",
    "        - Creates 'location_id' as a unique identifier for each location.\n",
    "        - Filters for 10:00 AM entries and shifts features by 1 day.\n",
    "        - Merges lagged features with original DataFrame.\n",
    "        - Calculates mean and variance for weather features over the last 24 hours.\n",
    "        - Merges these statistical summaries back into the original DataFrame.\n",
    "        \"\"\"\n",
    "        ##### LATEST WEATHER\n",
    "        def add_latest_weather(df):\n",
    "            # Assuming df is your original DataFrame\n",
    "            # Step 1: Convert datetime to a Datetime Object\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 2: Sorting the Data\n",
    "            df.sort_values(by=['datetime', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "            # Step 3: Creating a Unique Identifier for each location\n",
    "            df['location_id'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Step 4: Filtering for 10:00 AM Entries\n",
    "            df.reset_index(inplace=True)\n",
    "            df_10am = df[df['datetime'].dt.hour == 10]\n",
    "            df_10am.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 5: Shifting the Features by 1 day\n",
    "            lagged_features = df_10am.groupby('location_id').shift(periods=1, freq='D')\n",
    "\n",
    "            # Renaming columns to indicate lag\n",
    "            lagged_features = lagged_features.add_suffix('_hw_lagged')\n",
    "            lagged_features['location_id'] = lagged_features['location_id_hw_lagged']\n",
    "            lagged_features.reset_index(inplace=True)\n",
    "            lagged_features['date'] = lagged_features.datetime.dt.date\n",
    "\n",
    "            df['date'] = df.datetime.dt.date\n",
    "            return lagged_features\n",
    "            # Step 6: Merging Lagged Features with Original DataFrame\n",
    "            df = df.merge(lagged_features, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "            return df\n",
    "        \n",
    "        ##### mean from last day\n",
    "        def add_24h_mean_var(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 10:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, left_on='time_code', right_on='group', suffixes=('', '_hw_means'), how='left')\n",
    "            my_df = my_df.merge(variances, left_on='time_code', right_on='group', how='left', suffixes=('', '_hw_variances'))\n",
    "\n",
    "            return my_df\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        weather_features = df.columns.drop(['datetime', 'latitude', 'longitude'])\n",
    "\n",
    "        # Apply the function\n",
    "        df = add_24h_mean_var(df, weather_features)       \n",
    "        latest = add_latest_weather(df)\n",
    "        df = df.merge(latest, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_historical_weather(self, df):\n",
    "        ## LAG: From 11:00 AM 2 days ago to 10:00 AM 1 day ago\n",
    "        ## What to do? Give most recent weather forecast? Give average over the last day?\n",
    "        \"\"\"\n",
    "        Processes the historical weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        \n",
    "        df = self.add_historical_weather_lag_features(df)\n",
    "        \n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "\n",
    "    def init_forecast_weather(self, df):\n",
    "        \"\"\"Chatgpt summary:\n",
    "        Processes forecast weather data:\n",
    "        - Converts 'forecast_datetime' to 'datetime' and adjusts it forward by 1 day.\n",
    "        - Filters data to keep records with 'hours_ahead' between 22 and 45.\n",
    "        - Merges with a weather mapping based on 'latitude' and 'longitude'.\n",
    "        \"\"\"\n",
    "        ## LAG: DON't ADJUST\n",
    "        ##      The forecast is from yesterday, but can forecast today, which is 22 hours ahead\n",
    "        ## Drop any columns where:\n",
    "        ##                        hours_ahead < 22 and hours_ahead > 45\n",
    "        ## Then rename forecast_datetime to datetime and join on datetime\n",
    "        \"\"\"\n",
    "        Processes the forecast weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "        # keep only datetimes from our relevant period\n",
    "        df = df[(df['hours_ahead'] < 46) & (df['hours_ahead'] > 21)]\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "    \n",
    "    def add_gas_prices_lag_features(self, df):\n",
    "        \"\"\"Chatgpt summary\n",
    "        Augments a DataFrame with rolling average lag features for gas prices:\n",
    "        - Converts 'date' to Datetime object and sets as index.\n",
    "        - Sorts DataFrame by date.\n",
    "        - Calculates rolling averages for lowest and highest gas prices over 3, 7, and 14 days.\n",
    "        - Resets the index to include 'date' as a column again.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by date, if it's not already sorted\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Calculate rolling averages for different time windows\n",
    "        df['lowest_price_3d_avg'] = df['lowest_price_per_mwh'].rolling(window=3).mean()\n",
    "        df['highest_price_3d_avg'] = df['highest_price_per_mwh'].rolling(window=3).mean()\n",
    "\n",
    "        df['lowest_price_7d_avg'] = df['lowest_price_per_mwh'].rolling(window=7).mean()\n",
    "        df['highest_price_7d_avg'] = df['highest_price_per_mwh'].rolling(window=7).mean()\n",
    "\n",
    "        df['lowest_price_14d_avg'] = df['lowest_price_per_mwh'].rolling(window=14).mean()\n",
    "        df['highest_price_14d_avg'] = df['highest_price_per_mwh'].rolling(window=14).mean()\n",
    "\n",
    "        # Reset the index if you want the 'date' column back\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def init_gas_prices(self, df):\n",
    "        ## LAG: 1 DAY\n",
    "        ## Predictions are made from 2 days ago and predict for yesterday\n",
    "        ## add one day to forecast_date\n",
    "        ## Rename forecast_date to date, join on date\n",
    "        \"\"\"\n",
    "        Processes the gas prices data.\n",
    "        Implement the logic to handle gas prices data processing here.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['forecast_date']).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=1)\n",
    "        df = self.add_gas_prices_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_revealed_target_features(self, df):\n",
    "        \"\"\"Chatgpt summary:\n",
    "        Enhances DataFrame with rolling average target features:\n",
    "        - Converts 'datetime' to Datetime object, extracts 'hour' and 'day' of week.\n",
    "        - Sets 'datetime' as index.\n",
    "        - Calculates various rolling averages of 'target' based on different groupings:\n",
    "          - 24-hour rolling average by county, business status, product type, and consumption status.\n",
    "          - 7-day hourly rolling average by county, business status, product type, consumption status, and hour.\n",
    "          - 4-week rolling average by county, business status, product type, consumption status, hour, and day.\n",
    "          - Similar calculations considering all product types.\n",
    "        - Drops 'hour' and 'day' columns after processing.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['hour'] = df.datetime.dt.hour\n",
    "        df['day'] = df.datetime.dt.dayofweek\n",
    "        df.set_index('datetime', inplace=True)\n",
    "\n",
    "        window_size = 7\n",
    "        # Group by the specified columns and then apply the rolling mean\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_allp_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        df = df.drop(['hour', 'day'], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def init_revealed_targets(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=2)\n",
    "        df = self.add_revealed_target_features(df)\n",
    "        return df\n",
    "    \n",
    "    def init_client(self, df):\n",
    "        ## LAG: 2 days\n",
    "        ## Add 2 days to date, join on date\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=2)\n",
    "        # df = self.get_data_block_id(df, 'date')\n",
    "        return df\n",
    "\n",
    "    def init_weather_mapping(self):\n",
    "        # https://www.kaggle.com/code/tsunotsuno/enefit-eda-baseline/notebook#Baseline\n",
    "        county_point_map = {\n",
    "            0: (59.4, 24.7), # \"HARJUMAA\"\n",
    "            1 : (58.8, 22.7), # \"HIIUMAA\"\n",
    "            2 : (59.1, 27.2), # \"IDA-VIRUMAA\"\n",
    "            3 : (58.8, 25.7), # \"JÄRVAMAA\"\n",
    "            4 : (58.8, 26.2), # \"JÕGEVAMAA\"\n",
    "            5 : (59.1, 23.7), # \"LÄÄNE-VIRUMAA\"\n",
    "            6 : (59.1, 23.7), # \"LÄÄNEMAA\"\n",
    "            7 : (58.5, 24.7), # \"PÄRNUMAA\"\n",
    "            8 : (58.2, 27.2), # \"PÕLVAMAA\"\n",
    "            9 : (58.8, 24.7), # \"RAPLAMAA\"\n",
    "            10 : (58.5, 22.7),# \"SAAREMAA\"\n",
    "            11 : (58.5, 26.7),# \"TARTUMAA\"\n",
    "            12 : (58.5, 25.2),# \"UNKNOWNN\" (center of the map)\n",
    "            13 : (57.9, 26.2),# \"VALGAMAA\"\n",
    "            14 : (58.2, 25.7),# \"VILJANDIMAA\"\n",
    "            15 : (57.9, 27.2) # \"VÕRUMAA\"\n",
    "        }\n",
    "        # Convert the dictionary to a list of tuples\n",
    "        data = [(county_code, lat, lon) for county_code, (lat, lon) in county_point_map.items()]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=['county', 'latitude', 'longitude'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        df['day_of_week'] = df['datetime'].dt.day_of_week\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        df['is_weekend'] = df['datetime'].dt.day_of_week >= 5\n",
    "        df['is_month_start'] = df['datetime'].dt.is_month_start\n",
    "        df['is_month_end'] = df['datetime'].dt.is_month_end\n",
    "        df['is_quarter_start'] = df['datetime'].dt.is_quarter_start\n",
    "        df['is_quarter_end'] = df['datetime'].dt.is_quarter_end\n",
    "        df['is_year_start'] = df['datetime'].dt.is_year_start\n",
    "        df['is_year_end'] = df['datetime'].dt.is_year_end\n",
    "        df['season'] = df['datetime'].dt.month % 12 // 3 + 1\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        # Calculate sin and cos for day of year\n",
    "        days_in_year = 365.25  # accounts for leap year\n",
    "        df['day_of_year_sin'] = np.sin((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        df['day_of_year_cos'] = np.cos((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        return df\n",
    "    \n",
    "    def add_ee_holidays(self, df):\n",
    "        import holidays\n",
    "        # Define Estonia public holidays\n",
    "        ee_holidays = holidays.CountryHoliday('EE')\n",
    "        \n",
    "        print(df['date'].isna().sum())\n",
    "        \n",
    "        def find_problem(x):\n",
    "            try:\n",
    "                return x in ee_holidays\n",
    "            except Exception as e:\n",
    "                print(x)\n",
    "                raise e\n",
    "\n",
    "        # Function to check if the date is a holiday\n",
    "        df['is_ee_holiday'] = df['date'].apply(lambda x: x in ee_holidays)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def remove_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'row_id',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_test_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def join_data(self, train, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices):\n",
    "        df = train\n",
    "        df = df.merge(revealed_targets, how='left', on=('datetime', 'county', 'is_business', 'product_type', 'is_consumption'), suffixes=('', '_rt'))\n",
    "        df = df.merge(client, how='left', on=('date', 'county', 'is_business', 'product_type'), suffixes=('', '_client'))\n",
    "        df = df.merge(historical_weather, how='left', on=('datetime', 'county'), suffixes=('', '_hw'))\n",
    "        df = df.merge(forecast_weather, how='left', on=('datetime', 'county'), suffixes=('', '_fw'))\n",
    "        df = df.merge(electricity_prices, how='left', on='datetime', suffixes=('', '_elec'))\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.merge(gas_prices, how='left', on='date', suffixes=('', '_gasp'))\n",
    "        df = self.add_date_features(df)\n",
    "        df = self.add_ee_holidays(df)\n",
    "        return df\n",
    "    \n",
    "    def add_test_data(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        dfs = [test.copy(), revealed_targets.copy(), client.copy(), historical_weather.copy(),\n",
    "                 forecast_weather.copy(), electricity_prices.copy(), gas_prices.copy()]\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "                \n",
    "            self.test_orig_dfs[i] = pd.concat([ self.test_orig_dfs[i], df ])          \n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_test_data_timestep(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        #append test data to test data cache\n",
    "        self.add_test_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        # process test data\n",
    "        test = self.init_train(self.test_orig_dfs[0].copy())\n",
    "        revealed_targets = self.init_revealed_targets(self.test_orig_dfs[1].copy())\n",
    "        client = self.init_client(self.test_orig_dfs[2].copy())\n",
    "        historical_weather = self.init_historical_weather(self.test_orig_dfs[3].copy())\n",
    "        forecast_weather = self.init_forecast_weather(self.test_orig_dfs[4].copy())\n",
    "        electricity_prices = self.init_electricity(self.test_orig_dfs[5].copy())\n",
    "        gas_prices = self.init_gas_prices(self.test_orig_dfs[6].copy())\n",
    "        \n",
    "        df_all_cols = self.join_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        df = self.remove_test_cols(df_all_cols)\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1783a1-aaa1-4857-b335-99b1308ccef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>is_business</th>\n",
       "      <th>product_type</th>\n",
       "      <th>target</th>\n",
       "      <th>is_consumption</th>\n",
       "      <th>target_rt</th>\n",
       "      <th>target_lag_1h</th>\n",
       "      <th>target_lag_2h</th>\n",
       "      <th>target_lag_3h</th>\n",
       "      <th>target_lag_4h</th>\n",
       "      <th>...</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>season</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "      <th>is_ee_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>96.590</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.314</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018609</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197.233</td>\n",
       "      <td>1</td>\n",
       "      <td>184.072</td>\n",
       "      <td>171.092</td>\n",
       "      <td>168.933</td>\n",
       "      <td>174.920</td>\n",
       "      <td>170.068</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018610</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.501</td>\n",
       "      <td>25.884</td>\n",
       "      <td>83.535</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018611</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.404</td>\n",
       "      <td>1</td>\n",
       "      <td>38.646</td>\n",
       "      <td>47.690</td>\n",
       "      <td>34.806</td>\n",
       "      <td>29.202</td>\n",
       "      <td>21.654</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018612</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.512</td>\n",
       "      <td>34.657</td>\n",
       "      <td>122.195</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018613</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>196.240</td>\n",
       "      <td>1</td>\n",
       "      <td>183.756</td>\n",
       "      <td>190.316</td>\n",
       "      <td>172.973</td>\n",
       "      <td>141.646</td>\n",
       "      <td>93.817</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2018614 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         county  is_business  product_type   target  is_consumption  \\\n",
       "0             0            0             1    0.713               0   \n",
       "1             0            0             1   96.590               1   \n",
       "2             0            0             2    0.000               0   \n",
       "3             0            0             2   17.314               1   \n",
       "4             0            0             3    2.904               0   \n",
       "...         ...          ...           ...      ...             ...   \n",
       "2018609      15            1             0  197.233               1   \n",
       "2018610      15            1             1    0.000               0   \n",
       "2018611      15            1             1   28.404               1   \n",
       "2018612      15            1             3    0.000               0   \n",
       "2018613      15            1             3  196.240               1   \n",
       "\n",
       "         target_rt  target_lag_1h  target_lag_2h  target_lag_3h  \\\n",
       "0              NaN            NaN            NaN            NaN   \n",
       "1              NaN            NaN            NaN            NaN   \n",
       "2              NaN            NaN            NaN            NaN   \n",
       "3              NaN            NaN            NaN            NaN   \n",
       "4              NaN            NaN            NaN            NaN   \n",
       "...            ...            ...            ...            ...   \n",
       "2018609    184.072        171.092        168.933        174.920   \n",
       "2018610      0.000          0.000          2.501         25.884   \n",
       "2018611     38.646         47.690         34.806         29.202   \n",
       "2018612      0.000          0.000          4.512         34.657   \n",
       "2018613    183.756        190.316        172.973        141.646   \n",
       "\n",
       "         target_lag_4h  ...  is_quarter_start  is_quarter_end  is_year_start  \\\n",
       "0                  NaN  ...             False           False          False   \n",
       "1                  NaN  ...             False           False          False   \n",
       "2                  NaN  ...             False           False          False   \n",
       "3                  NaN  ...             False           False          False   \n",
       "4                  NaN  ...             False           False          False   \n",
       "...                ...  ...               ...             ...            ...   \n",
       "2018609        170.068  ...             False           False          False   \n",
       "2018610         83.535  ...             False           False          False   \n",
       "2018611         21.654  ...             False           False          False   \n",
       "2018612        122.195  ...             False           False          False   \n",
       "2018613         93.817  ...             False           False          False   \n",
       "\n",
       "         is_year_end  season  hour_sin  hour_cos  day_of_year_sin  \\\n",
       "0              False       4  0.000000  1.000000        -0.861693   \n",
       "1              False       4  0.000000  1.000000        -0.861693   \n",
       "2              False       4  0.000000  1.000000        -0.861693   \n",
       "3              False       4  0.000000  1.000000        -0.861693   \n",
       "4              False       4  0.000000  1.000000        -0.861693   \n",
       "...              ...     ...       ...       ...              ...   \n",
       "2018609        False       2 -0.258819  0.965926         0.532227   \n",
       "2018610        False       2 -0.258819  0.965926         0.532227   \n",
       "2018611        False       2 -0.258819  0.965926         0.532227   \n",
       "2018612        False       2 -0.258819  0.965926         0.532227   \n",
       "2018613        False       2 -0.258819  0.965926         0.532227   \n",
       "\n",
       "         day_of_year_cos  is_ee_holiday  \n",
       "0              -0.507430          False  \n",
       "1              -0.507430          False  \n",
       "2              -0.507430          False  \n",
       "3              -0.507430          False  \n",
       "4              -0.507430          False  \n",
       "...                  ...            ...  \n",
       "2018609        -0.846602          False  \n",
       "2018610        -0.846602          False  \n",
       "2018611        -0.846602          False  \n",
       "2018612        -0.846602          False  \n",
       "2018613        -0.846602          False  \n",
       "\n",
       "[2018614 rows x 240 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_processor_lgbm2_new_pandas.pkl', 'rb') as f:\n",
    "    data_processor = pickle.load(f)\n",
    "data_processor.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57e681-f170-4287-8b80-002e80e12839",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2a4a2-ac48-48d6-a8d6-b9e3563e4760",
   "metadata": {},
   "source": [
    "For my experimental CV, I want to take the approach of doing a stratified CV by time - splitting the year into 4 different parts, basically testing the model on each season, 3 months at a time. There was something in the kaggle forums that recommended something like this:\n",
    "\n",
    "Key: \n",
    "= -> training data\n",
    "+ -> CV data\n",
    "\n",
    "4 splits in time:\n",
    "1. =============+++\n",
    "2. ================+++\n",
    "3. ===================+++\n",
    "4. ======================+++\n",
    "\n",
    "\n",
    "\n",
    "The data starts on 2021-09-01 and ends on 2023-05-31\n",
    "\n",
    "BUT we don't have enough data to do that properly. So, my CV will instead be:\n",
    "\n",
    "\n",
    "(Thanks chatgpt)\n",
    "\n",
    "Splitting the period from 2022-09-01 to 2023-05-31 into five equal parts, here are the date ranges for each segment:\n",
    "\n",
    "#### First Segment:\n",
    "\n",
    "From 2022-09-01 to 2022-10-24\n",
    "\n",
    "#### Second Segment:\n",
    "\n",
    "From 2022-10-25 to 2022-12-17\n",
    "\n",
    "#### Third Segment:\n",
    "\n",
    "From 2022-12-18 to 2023-02-09\n",
    "\n",
    "#### Fourth Segment:\n",
    "\n",
    "From 2023-02-10 to 2023-04-04\n",
    "\n",
    "#### Fifth Segment:\n",
    "\n",
    "From 2023-04-05 to 2023-05-29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1505e0-cc5e-4cfb-90b8-f82f9034cd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb95938-f64f-4509-ad4b-e9075698b721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_drop_na(df):\n",
    "    df = df[~df.target.isna()]\n",
    "    df = df[~df.target_rolling_avg_24h.isna()]\n",
    "    means = df.mean()\n",
    "    # For each column, add an indicator column for NA values\n",
    "    # for col in df.columns:\n",
    "    #     if df[col].isna().any():\n",
    "    #         df[f'{col}_is_na'] = df[col].isna()\n",
    "    df = df.fillna(means)\n",
    "    return df, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5b7aef-e1ac-4067-831a-dded4567605a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.44 s\n",
      "Wall time: 8.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "county             0\n",
       "is_business        0\n",
       "product_type       0\n",
       "target             0\n",
       "is_consumption     0\n",
       "                  ..\n",
       "hour_sin           0\n",
       "hour_cos           0\n",
       "day_of_year_sin    0\n",
       "day_of_year_cos    0\n",
       "is_ee_holiday      0\n",
       "Length: 240, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "processed_df_no_na, means = fill_drop_na(data_processor.df)\n",
    "processed_df_no_na.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8583e570-0b4f-4156-81da-cbd66807dc9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mskel\\AppData\\Local\\Temp\\ipykernel_7388\\2244001451.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  processed_df_no_na['target_installed_capacity'] = processed_df_no_na['target'] / processed_df_no_na['installed_capacity'] * 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>is_business</th>\n",
       "      <th>product_type</th>\n",
       "      <th>target</th>\n",
       "      <th>is_consumption</th>\n",
       "      <th>target_rt</th>\n",
       "      <th>target_lag_1h</th>\n",
       "      <th>target_lag_2h</th>\n",
       "      <th>target_lag_3h</th>\n",
       "      <th>target_lag_4h</th>\n",
       "      <th>...</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>season</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "      <th>is_ee_holiday</th>\n",
       "      <th>target_installed_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11712</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>274.689353</td>\n",
       "      <td>274.69907</td>\n",
       "      <td>274.708302</td>\n",
       "      <td>274.717501</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.894542</td>\n",
       "      <td>-0.446983</td>\n",
       "      <td>False</td>\n",
       "      <td>0.975978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11713</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123.214</td>\n",
       "      <td>1</td>\n",
       "      <td>96.590</td>\n",
       "      <td>274.689353</td>\n",
       "      <td>274.69907</td>\n",
       "      <td>274.708302</td>\n",
       "      <td>274.717501</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.894542</td>\n",
       "      <td>-0.446983</td>\n",
       "      <td>False</td>\n",
       "      <td>129.305586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11714</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>274.689353</td>\n",
       "      <td>274.69907</td>\n",
       "      <td>274.708302</td>\n",
       "      <td>274.717501</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.894542</td>\n",
       "      <td>-0.446983</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11715</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21.940</td>\n",
       "      <td>1</td>\n",
       "      <td>17.314</td>\n",
       "      <td>274.689353</td>\n",
       "      <td>274.69907</td>\n",
       "      <td>274.708302</td>\n",
       "      <td>274.717501</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.894542</td>\n",
       "      <td>-0.446983</td>\n",
       "      <td>False</td>\n",
       "      <td>131.850962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11716</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.611</td>\n",
       "      <td>0</td>\n",
       "      <td>2.904</td>\n",
       "      <td>274.689353</td>\n",
       "      <td>274.69907</td>\n",
       "      <td>274.708302</td>\n",
       "      <td>274.717501</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.894542</td>\n",
       "      <td>-0.446983</td>\n",
       "      <td>False</td>\n",
       "      <td>0.223505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018609</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197.233</td>\n",
       "      <td>1</td>\n",
       "      <td>184.072</td>\n",
       "      <td>171.092000</td>\n",
       "      <td>168.93300</td>\n",
       "      <td>174.920000</td>\n",
       "      <td>170.068000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "      <td>318.117742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018610</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.50100</td>\n",
       "      <td>25.884000</td>\n",
       "      <td>83.535000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018611</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.404</td>\n",
       "      <td>1</td>\n",
       "      <td>38.646</td>\n",
       "      <td>47.690000</td>\n",
       "      <td>34.80600</td>\n",
       "      <td>29.202000</td>\n",
       "      <td>21.654000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "      <td>45.482786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018612</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.51200</td>\n",
       "      <td>34.657000</td>\n",
       "      <td>122.195000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018613</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>196.240</td>\n",
       "      <td>1</td>\n",
       "      <td>183.756</td>\n",
       "      <td>190.316000</td>\n",
       "      <td>172.97300</td>\n",
       "      <td>141.646000</td>\n",
       "      <td>93.817000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "      <td>89.681016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001094 rows × 241 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         county  is_business  product_type   target  is_consumption  \\\n",
       "11712         0            0             1    0.930               0   \n",
       "11713         0            0             1  123.214               1   \n",
       "11714         0            0             2    0.000               0   \n",
       "11715         0            0             2   21.940               1   \n",
       "11716         0            0             3    1.611               0   \n",
       "...         ...          ...           ...      ...             ...   \n",
       "2018609      15            1             0  197.233               1   \n",
       "2018610      15            1             1    0.000               0   \n",
       "2018611      15            1             1   28.404               1   \n",
       "2018612      15            1             3    0.000               0   \n",
       "2018613      15            1             3  196.240               1   \n",
       "\n",
       "         target_rt  target_lag_1h  target_lag_2h  target_lag_3h  \\\n",
       "11712        0.713     274.689353      274.69907     274.708302   \n",
       "11713       96.590     274.689353      274.69907     274.708302   \n",
       "11714        0.000     274.689353      274.69907     274.708302   \n",
       "11715       17.314     274.689353      274.69907     274.708302   \n",
       "11716        2.904     274.689353      274.69907     274.708302   \n",
       "...            ...            ...            ...            ...   \n",
       "2018609    184.072     171.092000      168.93300     174.920000   \n",
       "2018610      0.000       0.000000        2.50100      25.884000   \n",
       "2018611     38.646      47.690000       34.80600      29.202000   \n",
       "2018612      0.000       0.000000        4.51200      34.657000   \n",
       "2018613    183.756     190.316000      172.97300     141.646000   \n",
       "\n",
       "         target_lag_4h  ...  is_quarter_end  is_year_start  is_year_end  \\\n",
       "11712       274.717501  ...           False          False        False   \n",
       "11713       274.717501  ...           False          False        False   \n",
       "11714       274.717501  ...           False          False        False   \n",
       "11715       274.717501  ...           False          False        False   \n",
       "11716       274.717501  ...           False          False        False   \n",
       "...                ...  ...             ...            ...          ...   \n",
       "2018609     170.068000  ...           False          False        False   \n",
       "2018610      83.535000  ...           False          False        False   \n",
       "2018611      21.654000  ...           False          False        False   \n",
       "2018612     122.195000  ...           False          False        False   \n",
       "2018613      93.817000  ...           False          False        False   \n",
       "\n",
       "         season  hour_sin  hour_cos  day_of_year_sin  day_of_year_cos  \\\n",
       "11712         4  0.000000  1.000000        -0.894542        -0.446983   \n",
       "11713         4  0.000000  1.000000        -0.894542        -0.446983   \n",
       "11714         4  0.000000  1.000000        -0.894542        -0.446983   \n",
       "11715         4  0.000000  1.000000        -0.894542        -0.446983   \n",
       "11716         4  0.000000  1.000000        -0.894542        -0.446983   \n",
       "...         ...       ...       ...              ...              ...   \n",
       "2018609       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018610       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018611       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018612       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018613       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "\n",
       "         is_ee_holiday  target_installed_capacity  \n",
       "11712            False                   0.975978  \n",
       "11713            False                 129.305586  \n",
       "11714            False                   0.000000  \n",
       "11715            False                 131.850962  \n",
       "11716            False                   0.223505  \n",
       "...                ...                        ...  \n",
       "2018609          False                 318.117742  \n",
       "2018610          False                   0.000000  \n",
       "2018611          False                  45.482786  \n",
       "2018612          False                   0.000000  \n",
       "2018613          False                  89.681016  \n",
       "\n",
       "[2001094 rows x 241 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_no_na['target_installed_capacity'] = processed_df_no_na['target'] / processed_df_no_na['installed_capacity'] * 1000\n",
    "processed_df_no_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fde4de-0a2b-46d8-818d-a7e1f0a047ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "cv_ranges_corrected = [\n",
    "    ('2022-09-01', '2022-10-24'), \n",
    "    ('2022-10-25', '2022-12-17'), \n",
    "    ('2022-12-18', '2023-02-09'), \n",
    "    ('2023-02-10', '2023-04-04'), \n",
    "    ('2023-04-05', '2023-05-31')\n",
    "]\n",
    "\n",
    "# Function to convert a date string into a datetime object\n",
    "def to_datetime(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "# Converting the date strings in cv_ranges to datetime objects\n",
    "datetime_cv_ranges = [(to_datetime(start), to_datetime(end)) for start, end in cv_ranges_corrected]\n",
    "datetime_cv_ranges\n",
    "\n",
    "date_filter = data_processor.df_all_cols.date[processed_df_no_na.index]\n",
    "date_filter\n",
    "\n",
    "cv1_train = processed_df_no_na[date_filter <= datetime_cv_ranges[0][0]]\n",
    "cv1_test = processed_df_no_na[(date_filter <= datetime_cv_ranges[0][1]) & (date_filter > datetime_cv_ranges[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad9f11cc-2af1-4b8f-9689-1deb717c2e34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-19 00:00:00\n",
      "2023-05-23 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "print(to_datetime('2023-04-05') + dt.timedelta(days=14))\n",
    "print(to_datetime('2023-04-05') + dt.timedelta(days=48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341bfe1-29cf-4b7b-abce-634f3f9f9b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871c66a-fadc-4c18-bb3b-49e39b14f508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d6ec752-e0aa-4c32-bb95-3b12d7e4fb34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11712</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11713</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11714</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11715</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11716</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144249</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144250</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144251</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144252</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144253</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1129738 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         year  month  day\n",
       "11712    2021      9    5\n",
       "11713    2021      9    5\n",
       "11714    2021      9    5\n",
       "11715    2021      9    5\n",
       "11716    2021      9    5\n",
       "...       ...    ...  ...\n",
       "1144249  2022      9    1\n",
       "1144250  2022      9    1\n",
       "1144251  2022      9    1\n",
       "1144252  2022      9    1\n",
       "1144253  2022      9    1\n",
       "\n",
       "[1129738 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1_train[['year' ,'month', 'day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ceb22c-46e5-4245-bd27-45d32af655db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1144254</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144255</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144256</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144257</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144258</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315849</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315850</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315851</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315852</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315853</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         year  month  day\n",
       "1144254  2022      9    2\n",
       "1144255  2022      9    2\n",
       "1144256  2022      9    2\n",
       "1144257  2022      9    2\n",
       "1144258  2022      9    2\n",
       "...       ...    ...  ...\n",
       "1315849  2022     10   24\n",
       "1315850  2022     10   24\n",
       "1315851  2022     10   24\n",
       "1315852  2022     10   24\n",
       "1315853  2022     10   24\n",
       "\n",
       "[171264 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1_test[['year' ,'month', 'day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01409cfe-bc1f-4652-85a8-51d754512f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11712</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11713</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11714</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11715</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11716</th>\n",
       "      <td>2021</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018609</th>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018610</th>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018611</th>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018612</th>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018613</th>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001094 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         year  month  day\n",
       "11712    2021      9    5\n",
       "11713    2021      9    5\n",
       "11714    2021      9    5\n",
       "11715    2021      9    5\n",
       "11716    2021      9    5\n",
       "...       ...    ...  ...\n",
       "2018609  2023      5   31\n",
       "2018610  2023      5   31\n",
       "2018611  2023      5   31\n",
       "2018612  2023      5   31\n",
       "2018613  2023      5   31\n",
       "\n",
       "[2001094 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df_no_na[['year', 'month', 'day']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42030eb-8db5-474f-8094-8ac676700c79",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9efe30dd-843e-4d58-a48b-29039218659e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26d1777-6319-4ac7-8bb0-6f7500d955db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0428f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost.XGBRegressor(objective='reg:absoluteerror', n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2773b730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_dummies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e6c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05654611",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# create a one hot encoder to create the dummies and fit it to the data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ohe\u001b[38;5;241m=\u001b[39m OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m, sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m ohe\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdf\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# now let's simulate the two situations A and B\u001b[39;00m\n\u001b[0;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# create a one hot encoder to create the dummies and fit it to the data\n",
    "ohe= OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "ohe.fit(df[['x']])\n",
    "\n",
    "# now let's simulate the two situations A and B\n",
    "df.loc[1, 'x']= 1\n",
    "df= df.append(dict(x=5, y=5), ignore_index=True)\n",
    "\n",
    "# the actual feature generation is done in a separate step\n",
    "tr=ohe.transform(df[['x']])\n",
    "\n",
    "# if you need the columns in your existing data frame, you can glue them together\n",
    "df2=pd.DataFrame(tr, columns=['oh1', 'oh2', 'oh3'], index=df.index)\n",
    "result= pd.concat([df, df2], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6c39eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "processed_df_no_na['week_of_year'] = processed_df_no_na['week_of_year'].astype('int32')\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in [0]:\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target', 'target_installed_capacity']\n",
    "        drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        train = train.dropna()\n",
    "        val = val.dropna()\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end',  'week_of_year',\n",
    "                'is_year_start', 'is_year_end', 'season'] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]  \n",
    "        \n",
    "        for feature in cat_features:\n",
    "            df_train_data[feature] = df_train_data[feature].astype('category')\n",
    "            df_val_data2[feature] = df_val_data2[feature].astype('category')\n",
    "\n",
    "        #one hot encode because xgboost doesn't handle categorical variables well natively like lgbm does :(\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "        encoder.fit(df_train_data[cat_features])\n",
    "\n",
    "        train_hot = encoder.transform(df_train_data[cat_features])\n",
    "        df_train_data = df_train_data.drop(cat_features, axis=1)\n",
    "        train_hot_df = pd.DataFrame(train_hot, columns=[f'cat_{o}' for o in range(train_hot.shape[1])], index=df_train_data.index)\n",
    "        df_train_data = pd.concat([df_train_data, train_hot_df], axis=1)\n",
    "        # display(df_train_data)\n",
    "\n",
    "        val_hot = encoder.transform(df_val_data2[cat_features])\n",
    "        df_val_data2 = df_val_data2.drop(cat_features, axis=1)\n",
    "        val_hot_df = pd.DataFrame(val_hot, columns=[f'cat_{o}' for o in range(val_hot.shape[1])], index=df_val_data2.index)\n",
    "        df_val_data2 = pd.concat([df_val_data2, val_hot_df], axis=1)\n",
    "\n",
    "        # Xy = xgb.DMatrix(df_train_data, df_train_target['target'], enable_categorical=True)\n",
    "        # Xy_val = xgb.DMatrix(df_val_data2, df_val_target2['target'], enable_categorical=True)\n",
    "\n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it       \n",
    "        \n",
    "        # params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "        #         'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        \n",
    "        clf_consumer = xgb.XGBRegressor(objective='reg:absoluteerror', n_estimators=1000, device=\"cuda\"#, tree_method='gpu_hist'\n",
    "                                        )\n",
    "\n",
    "        # clf_consumer = xgb.train({'objective':'reg:absoluteerror', 'n_estimators':1000, 'enable_categorical':True, 'device':\"cuda\"}, Xy)\n",
    "        \n",
    "        # clf_consumer = VotingRegressor([\n",
    "        #     ('lgb_0', LGBMRegressor(**params, random_state=42, verbose=1, )),\n",
    "        #     ('lgb_1', LGBMRegressor(**params, random_state=69, verbose=1, )),\n",
    "        #     ('lgb_2', LGBMRegressor(**params, random_state=1337, verbose=1, )), \n",
    "        #     ('lgb_3', LGBMRegressor(**params, random_state=124, verbose=1, )),\n",
    "        #     ('lgb_4', LGBMRegressor(**params, random_state=12351, verbose=1, ))\n",
    "        #     ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "        \n",
    "        # clf_producer = VotingRegressor([\n",
    "        #     ('lgb_0', LGBMRegressor(**params, random_state=142, verbose=1, )),\n",
    "        #     ('lgb_1', LGBMRegressor(**params, random_state=169, verbose=1, )),\n",
    "        #     ('lgb_2', LGBMRegressor(**params, random_state=11337, verbose=1, )), \n",
    "        #     ('lgb_3', LGBMRegressor(**params, random_state=1124, verbose=1, )),\n",
    "        #     ('lgb_4', LGBMRegressor(**params, random_state=112351, verbose=1, ))\n",
    "        #     ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "        clf_consumer.fit(df_train_data, df_train_target.target)\n",
    "\n",
    "        # clf_producer.fit(df_train_data[df_train_data.is_consumption==0], df_train_target[df_train_data.is_consumption==0].target)\n",
    "        \n",
    "        # clf_consumer = lgb.train(params_consumer, dtrain)\n",
    "        # preds = gbm.predict(df_val_data2)\n",
    "        # mae = mean_absolute_error(df_val_target2[\"target\"], preds)\n",
    "\n",
    "        y_pred = clf_consumer.predict(df_train_data)\n",
    "        # y_pred_producer = clf_producer.predict(df_train_data[df_train_data.is_consumption==0])\n",
    "        # y_pred2 = y_pred.copy()\n",
    "        # y_pred2[df_train_data.is_consumption==0] = y_pred_producer \n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\" Train Mean Absolute Error_consumption:\", mae)\n",
    "        # mae = mean_absolute_error(df_train_target.target, y_pred2)\n",
    "        # print(f\" Train Mean w Producer Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf_consumer.predict(df_val_data2)\n",
    "        # y_pred_val_producer = clf_producer.predict(df_val_data2[df_val_data2.is_consumption==0])\n",
    "        # y_pred_val2 = y_pred_val.copy()\n",
    "        # y_pred_val2[df_val_data2.is_consumption==0] = y_pred_val_producer \n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(\"Val Mean Absolute Error:\", mae)\n",
    "        # mae = mean_absolute_error(df_val_target2.target, y_pred_val2)\n",
    "        # print(\"Val Mean w Producer Absolute Error:\", mae)\n",
    "        \n",
    "        # importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        # importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c236694-9a17-4b52-99aa-11a0344e0a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train rows: 1129738\n",
      "Val rows: 171264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:35:44] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"device\" } are not used.\n",
      "\n",
      " Train Mean Absolute Error_consumption: 170.22221717625746\n",
      "Val Mean Absolute Error: 172.03259466718384\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d5096e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "processed_df_no_na['week_of_year'] = processed_df_no_na['week_of_year'].astype('int32')\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in [0]:\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target', 'target_installed_capacity']\n",
    "        drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        train = train.dropna()\n",
    "        val = val.dropna()\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end',  'week_of_year',\n",
    "                'is_year_start', 'is_year_end', 'season'] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]  \n",
    "        \n",
    "        for feature in cat_features:\n",
    "            df_train_data[feature] = df_train_data[feature].astype('category')\n",
    "            df_val_data2[feature] = df_val_data2[feature].astype('category')\n",
    "\n",
    "        #one hot encode because xgboost doesn't handle categorical variables well natively like lgbm does :(\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "        encoder.fit(df_train_data[cat_features])\n",
    "\n",
    "        train_hot = encoder.transform(df_train_data[cat_features])\n",
    "        df_train_data = df_train_data.drop(cat_features, axis=1)\n",
    "        train_hot_df = pd.DataFrame(train_hot, columns=[f'cat_{o}' for o in range(train_hot.shape[1])], index=df_train_data.index)\n",
    "        df_train_data = pd.concat([df_train_data, train_hot_df], axis=1)\n",
    "        # display(df_train_data)\n",
    "\n",
    "        val_hot = encoder.transform(df_val_data2[cat_features])\n",
    "        df_val_data2 = df_val_data2.drop(cat_features, axis=1)\n",
    "        val_hot_df = pd.DataFrame(val_hot, columns=[f'cat_{o}' for o in range(val_hot.shape[1])], index=df_val_data2.index)\n",
    "        df_val_data2 = pd.concat([df_val_data2, val_hot_df], axis=1)\n",
    "\n",
    "        # Xy = xgb.DMatrix(df_train_data, df_train_target['target'], enable_categorical=True)\n",
    "        # Xy_val = xgb.DMatrix(df_val_data2, df_val_target2['target'], enable_categorical=True)\n",
    "\n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it       \n",
    "        \n",
    "        # params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "        #         'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "\n",
    "        clf_consumer = xgb.XGBRegressor(objective='reg:absoluteerror', n_estimators=1000, device='cuda:0'\n",
    "                                        )\n",
    "\n",
    "        # clf_consumer = xgb.train({'objective':'reg:absoluteerror', 'n_estimators':1000, 'enable_categorical':True, 'device':\"cuda\"}, Xy)\n",
    "        \n",
    "        # clf_consumer = VotingRegressor([\n",
    "        #     ('lgb_0', LGBMRegressor(**params, random_state=42, verbose=1, )),\n",
    "        #     ('lgb_1', LGBMRegressor(**params, random_state=69, verbose=1, )),\n",
    "        #     ('lgb_2', LGBMRegressor(**params, random_state=1337, verbose=1, )), \n",
    "        #     ('lgb_3', LGBMRegressor(**params, random_state=124, verbose=1, )),\n",
    "        #     ('lgb_4', LGBMRegressor(**params, random_state=12351, verbose=1, ))\n",
    "        #     ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "        \n",
    "        # clf_producer = VotingRegressor([\n",
    "        #     ('lgb_0', LGBMRegressor(**params, random_state=142, verbose=1, )),\n",
    "        #     ('lgb_1', LGBMRegressor(**params, random_state=169, verbose=1, )),\n",
    "        #     ('lgb_2', LGBMRegressor(**params, random_state=11337, verbose=1, )), \n",
    "        #     ('lgb_3', LGBMRegressor(**params, random_state=1124, verbose=1, )),\n",
    "        #     ('lgb_4', LGBMRegressor(**params, random_state=112351, verbose=1, ))\n",
    "        #     ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "        clf_consumer.fit(df_train_data, df_train_target.target)\n",
    "\n",
    "        # clf_producer.fit(df_train_data[df_train_data.is_consumption==0], df_train_target[df_train_data.is_consumption==0].target)\n",
    "        \n",
    "        # clf_consumer = lgb.train(params_consumer, dtrain)\n",
    "        # preds = gbm.predict(df_val_data2)\n",
    "        # mae = mean_absolute_error(df_val_target2[\"target\"], preds)\n",
    "\n",
    "        y_pred = clf_consumer.predict(df_train_data)\n",
    "        # y_pred_producer = clf_producer.predict(df_train_data[df_train_data.is_consumption==0])\n",
    "        # y_pred2 = y_pred.copy()\n",
    "        # y_pred2[df_train_data.is_consumption==0] = y_pred_producer \n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\" Train Mean Absolute Error_consumption:\", mae)\n",
    "        # mae = mean_absolute_error(df_train_target.target, y_pred2)\n",
    "        # print(f\" Train Mean w Producer Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf_consumer.predict(df_val_data2)\n",
    "        # y_pred_val_producer = clf_producer.predict(df_val_data2[df_val_data2.is_consumption==0])\n",
    "        # y_pred_val2 = y_pred_val.copy()\n",
    "        # y_pred_val2[df_val_data2.is_consumption==0] = y_pred_val_producer \n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(\"Val Mean Absolute Error:\", mae)\n",
    "        # mae = mean_absolute_error(df_val_target2.target, y_pred_val2)\n",
    "        # print(\"Val Mean w Producer Absolute Error:\", mae)\n",
    "        \n",
    "        # importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        # importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "075f136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train rows: 1129738\n",
      "Val rows: 171264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:50:17] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"device\" } are not used.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_df_no_na\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 86\u001b[0m, in \u001b[0;36mtrain_cv\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     65\u001b[0m clf_consumer \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:absoluteerror\u001b[39m\u001b[38;5;124m'\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     66\u001b[0m                                 )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# clf_consumer = xgb.train({'objective':'reg:absoluteerror', 'n_estimators':1000, 'enable_categorical':True, 'device':\"cuda\"}, Xy)\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# clf_consumer = VotingRegressor([\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#     ('lgb_4', LGBMRegressor(**params, random_state=112351, verbose=1, ))\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#     ], weights=[0.2,0.2,0.2,0.2,0.2])\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[43mclf_consumer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# clf_producer.fit(df_train_data[df_train_data.is_consumption==0], df_train_target[df_train_data.is_consumption==0].target)\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# clf_consumer = lgb.train(params_consumer, dtrain)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# preds = gbm.predict(df_val_data2)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# mae = mean_absolute_error(df_val_target2[\"target\"], preds)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf_consumer\u001b[38;5;241m.\u001b[39mpredict(df_train_data)\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m (\n\u001b[0;32m   1017\u001b[0m     model,\n\u001b[0;32m   1018\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1024\u001b[0m )\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2a649",
   "metadata": {},
   "source": [
    "## Hyper param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f28b4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT - filter warnings for long running jobs or all the output text will crash the computer :(\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13817bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "train data: 0\n",
      "val data: 0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://www.kaggle.com/code/chaozhuang/enefit-eda-w-fft-ssa-arima-lgbm?scriptVersionId=156414824#Predictive-Modelling\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "def tune_lgbm_model(base_params, df, i, n_iter=8, cv=3):\n",
    "    \"\"\"\n",
    "    Tune a LightGBM model based on a base set of parameters.\n",
    "\n",
    "    :param base_params: Dictionary of base parameters for the model\n",
    "    :param X_train: Training features\n",
    "    :param y_train: Training target variable\n",
    "    :param n_iter: Number of iterations for RandomizedSearchCV\n",
    "    :param cv: Number of cross-validation folds\n",
    "    :return: Best estimator and best parameters\n",
    "    \"\"\"\n",
    "\n",
    "    train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "    val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "    print(f\"Fold {i}\")\n",
    "    print(f\"Train rows: {len(train)}\")\n",
    "    print(f\"Val rows: {len(val)}\")\n",
    "\n",
    "    target_cols = ['target', 'target_installed_capacity']\n",
    "    drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                'snowfall_fw', 'snowfall_hw_means']\n",
    "    \n",
    "    train = train.dropna()\n",
    "    val = val.dropna()\n",
    "    \n",
    "    df_train_target = train[target_cols]\n",
    "    df_train_data = train.drop(drop_cols, axis=1)\n",
    "    \n",
    "    df_val_target2 = val[target_cols]\n",
    "    df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "    \n",
    "    cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "            'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end',  'week_of_year',\n",
    "            'is_year_start', 'is_year_end', 'season'] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "    cat_features = [c for c in cat_features if c in df_train_data.columns]  \n",
    "    \n",
    "    for feature in cat_features:\n",
    "        df_train_data[feature] = df_train_data[feature].astype('category')\n",
    "        df_val_data2[feature] = df_val_data2[feature].astype('category')\n",
    "\n",
    "    #one hot encode because xgboost doesn't handle categorical variables well natively like lgbm does :(\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "    encoder.fit(df_train_data[cat_features])\n",
    "\n",
    "    train_hot = encoder.transform(df_train_data[cat_features])\n",
    "    df_train_data = df_train_data.drop(cat_features, axis=1)\n",
    "    train_hot_df = pd.DataFrame(train_hot, columns=[f'cat_{o}' for o in range(train_hot.shape[1])], index=df_train_data.index)\n",
    "    df_train_data = pd.concat([df_train_data, train_hot_df], axis=1)\n",
    "    df_train_data = df_train_data.reset_index(drop=True)\n",
    "    # display(df_train_data)\n",
    "\n",
    "    val_hot = encoder.transform(df_val_data2[cat_features])\n",
    "    df_val_data2 = df_val_data2.drop(cat_features, axis=1)\n",
    "    val_hot_df = pd.DataFrame(val_hot, columns=[f'cat_{o}' for o in range(val_hot.shape[1])], index=df_val_data2.index)\n",
    "    df_val_data2 = pd.concat([df_val_data2, val_hot_df], axis=1)\n",
    "    df_val_data2 = df_val_data2.reset_index(drop=True)\n",
    "\n",
    "    print(f\"train data: {df_train_data.isna().sum().max()}\")\n",
    "    print(f\"val data: {df_val_data2.isna().sum().max()}\")\n",
    "\n",
    "    param_dist = {\n",
    "        'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "        'objective': ['reg:absoluteerror', 'reg:tweedie'],\n",
    "        'gamma': sp_uniform(0,1),\n",
    "        'max_depth': sp_randint(3, 50),\n",
    "        'tweedie_variance_power': sp_uniform(1,2),\n",
    "        'min_child_weight': sp_uniform(0,3),\n",
    "        'learning_rate': sp_uniform(0.005, 0.5),\n",
    "\n",
    "        'lambda': sp_uniform(0, 4), \n",
    "        'alpha': sp_uniform(0, 4), \n",
    "        'grow_policy': ['depthwise', 'lossguide'],\n",
    "        'max_bin': sp_randint(100, 1000),\n",
    "\n",
    "        'n_estimators': sp_randint(2000, 3500),\n",
    "\n",
    "        # 'min_data_in_leaf': sp_randint(15, 300),\n",
    "        \n",
    "        # 'num_leaves': sp_randint(25, 150),\n",
    "        \n",
    "        # 'colsample_bytree' : sp_uniform(0.1, 1),\n",
    "        # 'colsample_bynode' : sp_uniform(0.1, 1),\n",
    "        # 'data_sample_strategy' : ['bagging', 'goss'],\n",
    "        \n",
    "        # 'drop_rate': sp_uniform(0, 1),\n",
    "        # 'skip_drop': sp_uniform(0, 1),\n",
    "        # 'min_data_per_group': sp_randint(10, 200),\n",
    "        # 'max_cat_threshold': sp_randint(10, 100),\n",
    "        # 'cat_l2': sp_randint(10, 100),\n",
    "        # 'cat_smooth': sp_randint(10, 100),\n",
    "    }\n",
    "\n",
    "    # Create a LightGBM regressor object\n",
    "    xgb_reg = xgb.XGBRegressor(**base_params)\n",
    "\n",
    "    # Create a RandomizedSearchCV object\n",
    "    random_search = HalvingRandomSearchCV(estimator=xgb_reg, param_distributions=param_dist,\n",
    "                                       scoring='neg_mean_absolute_error',\n",
    "                                       cv=TimeSeriesSplit(n_splits=cv), random_state=1337, verbose=0,\n",
    "                                         aggressive_elimination= True,\n",
    "                                         max_resources=1000, min_resources=5, )\n",
    "\n",
    "    results_dict = {}\n",
    "    # Fit the random search to the data\n",
    "    random_search.fit(df_train_data, df_train_target.target)\n",
    "\n",
    "    # Return the best estimator and best parameters\n",
    "    results_dict['best_estimator'] = random_search.best_estimator_\n",
    "    results_dict['best_params'] = random_search.best_params_\n",
    "    \n",
    "    \n",
    "#     random_search = HalvingRandomSearchCV(estimator=lgb_reg, param_distributions=param_dist,\n",
    "#                                        scoring='neg_mean_absolute_error',\n",
    "#                                        cv=cv, random_state=2024, verbose=1,\n",
    "#                                          aggressive_elimination= True,\n",
    "#                                          max_resources=20000, min_resources=5)\n",
    "#     # consumer\n",
    "#     X_train_consumer = X_train[~producer_mask]\n",
    "#     y_train_consumer = y_train[~producer_mask]\n",
    "#     # Fit the random search to the data\n",
    "#     random_search.fit(X_train_consumer, y_train_consumer, categorical_feature=cat_features)\n",
    "\n",
    "#     # Return the best estimator and best parameters\n",
    "#     results_dict['consumer_best_estimator'] = random_search.best_estimator_\n",
    "#     results_dict['consumer_best_params'] = random_search.best_params_\n",
    "\n",
    "    with open('experiments/xgb_hyperparam_search_object1.pkl', 'wb') as file:\n",
    "        pickle.dump(random_search, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "base_params_p1 = {\n",
    "    'verbosity':0, \n",
    "    'device':'gpu',\n",
    "    'n_jobs': 22,\n",
    "    'eval_metric': 'mae'\n",
    "}\n",
    "\n",
    "# Fit the model\n",
    "results_dict = tune_lgbm_model(base_params_p1, processed_df_no_na, 4)\n",
    "\n",
    "results_params = {'best_params': results_dict['best_params']}\n",
    "\n",
    "import pickle\n",
    "# save dictionary to pickle file\n",
    "with open('experiments/xgb_hyperparam_results1.pkl', 'wb') as file:\n",
    "    pickle.dump(results_params, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# print(\"Best parameters:\", results_dict['best_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", results_dict['best_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab40eb",
   "metadata": {},
   "source": [
    "## The best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48618451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "processed_df_no_na['week_of_year'] = processed_df_no_na['week_of_year'].astype('int32')\n",
    "\n",
    "mae_results = []\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in range(5):\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target', 'target_installed_capacity']\n",
    "        drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        train = train.dropna()\n",
    "        val = val.dropna()\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end',  'week_of_year',\n",
    "                'is_year_start', 'is_year_end', 'season'] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]  \n",
    "        \n",
    "        for feature in cat_features:\n",
    "            df_train_data[feature] = df_train_data[feature].astype('category')\n",
    "            df_val_data2[feature] = df_val_data2[feature].astype('category')\n",
    "\n",
    "        #one hot encode because xgboost doesn't handle categorical variables well natively like lgbm does :(\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "        encoder.fit(df_train_data[cat_features])\n",
    "\n",
    "        train_hot = encoder.transform(df_train_data[cat_features])\n",
    "        df_train_data = df_train_data.drop(cat_features, axis=1)\n",
    "        train_hot_df = pd.DataFrame(train_hot, columns=[f'cat_{o}' for o in range(train_hot.shape[1])], index=df_train_data.index)\n",
    "        df_train_data = pd.concat([df_train_data, train_hot_df], axis=1)\n",
    "        # display(df_train_data)\n",
    "\n",
    "        val_hot = encoder.transform(df_val_data2[cat_features])\n",
    "        df_val_data2 = df_val_data2.drop(cat_features, axis=1)\n",
    "        val_hot_df = pd.DataFrame(val_hot, columns=[f'cat_{o}' for o in range(val_hot.shape[1])], index=df_val_data2.index)\n",
    "        df_val_data2 = pd.concat([df_val_data2, val_hot_df], axis=1)\n",
    "\n",
    "        # Xy = xgb.DMatrix(df_train_data, df_train_target['target'], enable_categorical=True)\n",
    "        # Xy_val = xgb.DMatrix(df_val_data2, df_val_target2['target'], enable_categorical=True)\n",
    "\n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        base_params = {\n",
    "                # 'verbosity':1, \n",
    "                'device':'gpu',\n",
    "                'n_jobs': 22,\n",
    "                'eval_metric': 'mae'\n",
    "            } \n",
    "\n",
    "        params = base_params | results_params['best_params']\n",
    "        \n",
    "        # params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "        #         'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "\n",
    "        # clf_consumer = xgb.XGBRegressor(objective='reg:absoluteerror', n_estimators=1000, device='cuda:0')\n",
    "        clf_consumer = xgb.XGBRegressor(**params)\n",
    "\n",
    "        # clf_consumer = xgb.train({'objective':'reg:absoluteerror', 'n_estimators':1000, 'enable_categorical':True, 'device':\"cuda\"}, Xy)\n",
    "        \n",
    "        # clf_consumer = VotingRegressor([\n",
    "        #     ('lgb_0', LGBMRegressor(**params, random_state=42, verbose=1, )),\n",
    "        #     ('lgb_1', LGBMRegressor(**params, random_state=69, verbose=1, )),\n",
    "        #     ('lgb_2', LGBMRegressor(**params, random_state=1337, verbose=1, )), \n",
    "        #     ('lgb_3', LGBMRegressor(**params, random_state=124, verbose=1, )),\n",
    "        #     ('lgb_4', LGBMRegressor(**params, random_state=12351, verbose=1, ))\n",
    "        #     ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "        \n",
    "        # clf_producer = VotingRegressor([\n",
    "        #     ('lgb_0', LGBMRegressor(**params, random_state=142, verbose=1, )),\n",
    "        #     ('lgb_1', LGBMRegressor(**params, random_state=169, verbose=1, )),\n",
    "        #     ('lgb_2', LGBMRegressor(**params, random_state=11337, verbose=1, )), \n",
    "        #     ('lgb_3', LGBMRegressor(**params, random_state=1124, verbose=1, )),\n",
    "        #     ('lgb_4', LGBMRegressor(**params, random_state=112351, verbose=1, ))\n",
    "        #     ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "        clf_consumer.fit(df_train_data, df_train_target.target)\n",
    "\n",
    "        # clf_producer.fit(df_train_data[df_train_data.is_consumption==0], df_train_target[df_train_data.is_consumption==0].target)\n",
    "        \n",
    "        # clf_consumer = lgb.train(params_consumer, dtrain)\n",
    "        # preds = gbm.predict(df_val_data2)\n",
    "        # mae = mean_absolute_error(df_val_target2[\"target\"], preds)\n",
    "\n",
    "        y_pred = clf_consumer.predict(df_train_data)\n",
    "        # y_pred_producer = clf_producer.predict(df_train_data[df_train_data.is_consumption==0])\n",
    "        # y_pred2 = y_pred.copy()\n",
    "        # y_pred2[df_train_data.is_consumption==0] = y_pred_producer \n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\" Train Mean Absolute Error_consumption:\", mae)\n",
    "        # mae = mean_absolute_error(df_train_target.target, y_pred2)\n",
    "        # print(f\" Train Mean w Producer Absolute Error:\", mae)\n",
    "        results[f'Fold {i}: Train Mean Absolute Error_consumption:'] = mae\n",
    "\n",
    "        y_pred_val = clf_consumer.predict(df_val_data2)\n",
    "        # y_pred_val_producer = clf_producer.predict(df_val_data2[df_val_data2.is_consumption==0])\n",
    "        # y_pred_val2 = y_pred_val.copy()\n",
    "        # y_pred_val2[df_val_data2.is_consumption==0] = y_pred_val_producer \n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(\"Val Mean Absolute Error:\", mae)\n",
    "        results[f'Fold {i}: Val Mean Absolute Error:'] = mae\n",
    "        # mae = mean_absolute_error(df_val_target2.target, y_pred_val2)\n",
    "        # print(\"Val Mean w Producer Absolute Error:\", mae)\n",
    "        \n",
    "        # importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        # importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "\n",
    "        mae_results.append(results)\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    import pickle\n",
    "    # save dictionary to pickle file\n",
    "    with open('experiments/xgb_hyperparam_mae_cv_run1.pkl', 'wb') as file:\n",
    "        pickle.dump(mae_results, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31adcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train rows: 1129738\n",
      "Val rows: 171264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:21:39] WARNING: C:\\Users\\dev-admin\\croot2\\xgboost-split_1675461376218\\work\\src\\learner.cc:767: \n",
      "Parameters: { \"device\" } are not used.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_df_no_na\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 91\u001b[0m, in \u001b[0;36mtrain_cv\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     70\u001b[0m clf_consumer \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:absoluteerror\u001b[39m\u001b[38;5;124m'\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# clf_consumer = xgb.XGBRegressor(**params)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# clf_consumer = xgb.train({'objective':'reg:absoluteerror', 'n_estimators':1000, 'enable_categorical':True, 'device':\"cuda\"}, Xy)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#     ('lgb_4', LGBMRegressor(**params, random_state=112351, verbose=1, ))\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m#     ], weights=[0.2,0.2,0.2,0.2,0.2])\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m \u001b[43mclf_consumer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# clf_producer.fit(df_train_data[df_train_data.is_consumption==0], df_train_target[df_train_data.is_consumption==0].target)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# clf_consumer = lgb.train(params_consumer, dtrain)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# preds = gbm.predict(df_val_data2)\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# mae = mean_absolute_error(df_val_target2[\"target\"], preds)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf_consumer\u001b[38;5;241m.\u001b[39mpredict(df_train_data)\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m (\n\u001b[0;32m   1017\u001b[0m     model,\n\u001b[0;32m   1018\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1024\u001b[0m )\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\XGBoost\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7afe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da65aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe051847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e411e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94398318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "962b343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in [0]:\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target', 'target_installed_capacity']\n",
    "        drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        train = train.dropna()\n",
    "        val = val.dropna()\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season'] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]  \n",
    "        \n",
    "        for feature in cat_features:\n",
    "            df_train_data[feature] = df_train_data[feature].astype('category')\n",
    "            df_val_data2[feature] = df_val_data2[feature].astype('category')\n",
    "        \n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it       \n",
    "        \n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        \n",
    "        # clf_consumer = LGBMRegressor(**params, random_state=42, verbose=1, )\n",
    "        \n",
    "        clf_consumer = VotingRegressor([\n",
    "            ('lgb_0', LGBMRegressor(**params, random_state=42, verbose=1, )),\n",
    "            ('lgb_1', LGBMRegressor(**params, random_state=69, verbose=1, )),\n",
    "            ('lgb_2', LGBMRegressor(**params, random_state=1337, verbose=1, )), \n",
    "            ('lgb_3', LGBMRegressor(**params, random_state=124, verbose=1, )),\n",
    "            ('lgb_4', LGBMRegressor(**params, random_state=12351, verbose=1, ))\n",
    "            ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "        \n",
    "        # clf_producer = VotingRegressor([\n",
    "        #     ('lgb_0', LGBMRegressor(**params, random_state=142, verbose=1, )),\n",
    "        #     ('lgb_1', LGBMRegressor(**params, random_state=169, verbose=1, )),\n",
    "        #     ('lgb_2', LGBMRegressor(**params, random_state=11337, verbose=1, )), \n",
    "        #     ('lgb_3', LGBMRegressor(**params, random_state=1124, verbose=1, )),\n",
    "        #     ('lgb_4', LGBMRegressor(**params, random_state=112351, verbose=1, ))\n",
    "        #     ], weights=[0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "        clf_consumer.fit(df_train_data, df_train_target.target)\n",
    "        # clf_producer.fit(df_train_data[df_train_data.is_consumption==0], df_train_target[df_train_data.is_consumption==0].target)\n",
    "        \n",
    "        # clf_consumer = lgb.train(params_consumer, dtrain)\n",
    "        # preds = gbm.predict(df_val_data2)\n",
    "        # mae = mean_absolute_error(df_val_target2[\"target\"], preds)\n",
    "\n",
    "        y_pred = clf_consumer.predict(df_train_data)\n",
    "        # y_pred_producer = clf_producer.predict(df_train_data[df_train_data.is_consumption==0])\n",
    "        # y_pred2 = y_pred.copy()\n",
    "        # y_pred2[df_train_data.is_consumption==0] = y_pred_producer \n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\" Train Mean Absolute Error_consumption:\", mae)\n",
    "        # mae = mean_absolute_error(df_train_target.target, y_pred2)\n",
    "        # print(f\" Train Mean w Producer Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf_consumer.predict(df_val_data2)\n",
    "        # y_pred_val_producer = clf_producer.predict(df_val_data2[df_val_data2.is_consumption==0])\n",
    "        # y_pred_val2 = y_pred_val.copy()\n",
    "        # y_pred_val2[df_val_data2.is_consumption==0] = y_pred_val_producer \n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(\"Val Mean Absolute Error:\", mae)\n",
    "        # mae = mean_absolute_error(df_val_target2.target, y_pred_val2)\n",
    "        # print(\"Val Mean w Producer Absolute Error:\", mae)\n",
    "        \n",
    "        # importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        # importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00681bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train rows: 1158538\n",
      "Val rows: 842556\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 50105\n",
      "[LightGBM] [Info] Number of data points in the train set: 1158538, number of used features: 225\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 214 dense feature groups (238.65 MB) transferred to GPU in 0.069642 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 5.525093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 50105\n",
      "[LightGBM] [Info] Number of data points in the train set: 1158538, number of used features: 225\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 214 dense feature groups (238.65 MB) transferred to GPU in 0.069387 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 5.525093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 50098\n",
      "[LightGBM] [Info] Number of data points in the train set: 1158538, number of used features: 225\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 214 dense feature groups (238.65 MB) transferred to GPU in 0.074093 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 5.525093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 50101\n",
      "[LightGBM] [Info] Number of data points in the train set: 1158538, number of used features: 225\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 214 dense feature groups (238.65 MB) transferred to GPU in 0.068989 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 5.525093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 50118\n",
      "[LightGBM] [Info] Number of data points in the train set: 1158538, number of used features: 225\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 214 dense feature groups (238.65 MB) transferred to GPU in 0.076105 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 5.525093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      " Train Mean Absolute Error_consumption: 19.716214884140374\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "Val Mean Absolute Error: 52.74843411098012\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98f72595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, period 0\n",
      "Train rows: 1129738\n",
      "Val rows: 44880\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LGBMRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 42\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# We leave max_depth as -1\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Tune num_leaves, default is 31, let's double it       \u001b[39;00m\n\u001b[0;32m     39\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.7466999841658806\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3.2140838539606458\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.13753679743025782\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_bin\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m250\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_data_in_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m150\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5593\u001b[39m,  \n\u001b[0;32m     40\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m22\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweedie\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m---> 42\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mLGBMRegressor\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(df_train_data, df_train_target\u001b[38;5;241m.\u001b[39mtarget, categorical_feature\u001b[38;5;241m=\u001b[39mcat_features)\n\u001b[0;32m     46\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(df_train_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LGBMRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "train_pred_list = []\n",
    "train_mae_list = []\n",
    "train_targets_list = []\n",
    "\n",
    "pred_list = []\n",
    "mae_list = []\n",
    "val_targets_list = []\n",
    "\n",
    "df = processed_df_no_na\n",
    "i=0\n",
    "for f in range(((datetime_cv_ranges[i][1] - datetime_cv_ranges[i][0]).days//14)):\n",
    "    start = datetime_cv_ranges[i][0] + dt.timedelta(days=f*14)\n",
    "    stop = datetime_cv_ranges[i][0] + dt.timedelta(days=(f+1)*14)\n",
    "    train = processed_df_no_na[date_filter <= start]\n",
    "    val = processed_df_no_na[(date_filter <= stop) & (date_filter > start)]\n",
    "    \n",
    "    print(f\"Fold {i}, period {f}\")\n",
    "    print(f\"Train rows: {len(train)}\")\n",
    "    print(f\"Val rows: {len(val)}\")\n",
    "\n",
    "    target_cols = ['target', 'target_installed_capacity']\n",
    "    drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                'snowfall_fw', 'snowfall_hw_means']\n",
    "\n",
    "    df_train_target = train[target_cols]\n",
    "    df_train_data = train.drop(drop_cols, axis=1)\n",
    "\n",
    "    df_val_target2 = val[target_cols]\n",
    "    df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "\n",
    "    cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "           'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "            'is_year_start', 'is_year_end', 'season'] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "    cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "\n",
    "    # We leave max_depth as -1\n",
    "    # Tune num_leaves, default is 31, let's double it       \n",
    "\n",
    "    params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "    \n",
    "    clf = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "\n",
    "    clf.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "    y_pred = clf.predict(df_train_data)\n",
    "    train_pred_list.append(y_pred)\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "    # Assuming you have two pandas Series: y_true and y_pred\n",
    "    mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "    train_mae_list.append(mae)\n",
    "    train_targets_list.append(df_train_target.target)\n",
    "    print(f\" Train Mean Absolute Error_consumption:\", mae)\n",
    "\n",
    "    y_pred_val = clf.predict(df_val_data2)\n",
    "    pred_list.append(y_pred_val)\n",
    "\n",
    "    mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "    val_targets_list.append(df_val_target2.target)\n",
    "    mae_list.append(mae)\n",
    "    print(\"Val Mean Absolute Error:\", mae)\n",
    "\n",
    "# importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "# importance = importance.sort_values('importance', ascending=False)\n",
    "# display(importance.head(30))\n",
    "# display(importance.tail(30))\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6819b19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.6798865951825"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf85cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
