{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    \"\"\"Processes Train data, using train data as a warm start, and prepares it for inference.\"\"\"\n",
    "\n",
    "    def __init__(self, train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices):\n",
    "        self.test_orig_dfs = self.get_test_orig_dfs([train.copy(), revealed_targets.copy(), client.copy(), historical_weather.copy(),\n",
    "                 forecast_weather.copy(), electricity_prices.copy(), gas_prices.copy()])\n",
    "        self.train = self.init_train(train)\n",
    "        self.revealed_targets = self.init_revealed_targets(revealed_targets)\n",
    "        self.client = self.init_client(client)\n",
    "        self.weather_mapping = self.init_weather_mapping()\n",
    "        self.historical_weather = self.init_historical_weather(historical_weather)\n",
    "        self.forecast_weather = self.init_forecast_weather(forecast_weather)\n",
    "        self.electricity_prices = self.init_electricity(electricity_prices)\n",
    "        self.gas_prices = self.init_gas_prices(gas_prices)\n",
    "        \n",
    "        self.df_all_cols = self.join_data(self.train, self.revealed_targets, self.client, self.historical_weather, self.forecast_weather, self.electricity_prices, self.gas_prices)\n",
    "        self.df = self.remove_cols(self.df_all_cols)\n",
    "        \n",
    "    def get_test_orig_dfs(self, dfs):\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['prediction_datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'prediction_datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df.date).dt.date\n",
    "                col = 'date'\n",
    "\n",
    "            test_date = df[col].iloc[-1]  # Assuming test is a DataFrame\n",
    "            start_date = test_date - pd.Timedelta(days=14)\n",
    "            historical_subset = df[df[col] >= start_date]\n",
    "            dfs[i] = historical_subset\n",
    "        return dfs\n",
    "        \n",
    "    def init_train(self, df):\n",
    "        \"\"\"Prepares the training data for model training.\"\"\"\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        except Exception as e:\n",
    "            df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "        df['date'] = df.datetime.dt.date\n",
    "            \n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        return df\n",
    "    \n",
    "    def add_electricity_lag_features(self, df):\n",
    "        ##### mean from entire last week\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # Use rolling to calculate mean price of the last week\n",
    "        # The window is 7 days, min_periods can be set as per requirement\n",
    "        # 'closed' determines which side of the interval is closed; it can be 'right' or 'left'\n",
    "        df['mean_euros_per_mwh_last_week'] = df['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()\n",
    "        # Shift the results to align with the requirement of lagging\n",
    "        df['mean_euros_per_mwh_last_week'] = df['mean_euros_per_mwh_last_week'].shift()\n",
    "        \n",
    "        ##### mean from last week this hour only\n",
    "        # Extract hour from datetime\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        # Group by hour and apply rolling mean for each group\n",
    "        hourly_groups = df.groupby('hour')\n",
    "        dff = hourly_groups['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()#.shift()#.reset_index(level=0, drop=True)\n",
    "        dff = dff.reset_index().set_index('datetime').groupby('hour')['euros_per_mwh'].shift()\n",
    "        dff = dff.rename('mean_euros_per_mwh_same_hour_last_week')\n",
    "        df = df.join(dff)\n",
    "        #### yesterday's power price\n",
    "        df['yesterdays_euros_per_mwh'] = df['euros_per_mwh'].shift(24)\n",
    "        \n",
    "        ### 24h average\n",
    "        # Calculate the 24-hour rolling average\n",
    "        df['euros_per_mwh_24h_average_price'] = df['euros_per_mwh'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "        # Resetting the index if needed\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['forecast_date', 'origin_date', 'hour'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def init_electricity(self, df):\n",
    "        ## LAG = 1 Day\n",
    "        ## Move forecast datetime ahead by 1 day\n",
    "        ## change name to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_date'])\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        df = self.add_electricity_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_historical_weather_lag_features(self, df):\n",
    "        ##### LATEST WEATHER\n",
    "        def add_latest_weather(df):\n",
    "            # Assuming df is your original DataFrame\n",
    "            # Step 1: Convert datetime to a Datetime Object\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 2: Sorting the Data\n",
    "            df.sort_values(by=['datetime', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "            # Step 3: Creating a Unique Identifier for each location\n",
    "            df['location_id'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Step 4: Filtering for 10:00 AM Entries\n",
    "            df.reset_index(inplace=True)\n",
    "            df_10am = df[df['datetime'].dt.hour == 10]\n",
    "            df_10am.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 5: Shifting the Features by 1 day\n",
    "            lagged_features = df_10am.groupby('location_id').shift(periods=1, freq='D')\n",
    "\n",
    "            # Renaming columns to indicate lag\n",
    "            lagged_features = lagged_features.add_suffix('_hw_lagged')\n",
    "            lagged_features['location_id'] = lagged_features['location_id_hw_lagged']\n",
    "            lagged_features.reset_index(inplace=True)\n",
    "            lagged_features['date'] = lagged_features.datetime.dt.date\n",
    "\n",
    "            df['date'] = df.datetime.dt.date\n",
    "            return lagged_features\n",
    "            # Step 6: Merging Lagged Features with Original DataFrame\n",
    "            df = df.merge(lagged_features, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "            return df\n",
    "        \n",
    "        ##### mean from last day\n",
    "        def add_24h_mean_var(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 10:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, left_on='time_code', right_on='group', suffixes=('', '_hw_means'), how='left')\n",
    "            my_df = my_df.merge(variances, left_on='time_code', right_on='group', how='left', suffixes=('', '_hw_variances'))\n",
    "\n",
    "            return my_df\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        weather_features = df.columns.drop(['datetime', 'latitude', 'longitude'])\n",
    "\n",
    "        # Apply the function\n",
    "        df = add_24h_mean_var(df, weather_features)       \n",
    "        latest = add_latest_weather(df)\n",
    "        df = df.merge(latest, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_historical_weather(self, df):\n",
    "        ## LAG: From 11:00 AM 2 days ago to 10:00 AM 1 day ago\n",
    "        ## What to do? Give most recent weather forecast? Give average over the last day?\n",
    "        \"\"\"\n",
    "        Processes the historical weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        \n",
    "        df = self.add_historical_weather_lag_features(df)\n",
    "        \n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "\n",
    "    def init_forecast_weather(self, df):\n",
    "        ## LAG: DON't ADJUST\n",
    "        ##      The forecast is from yesterday, but can forecast today, which is 22 hours ahead\n",
    "        ## Drop any columns where:\n",
    "        ##                        hours_ahead < 22 and hours_ahead > 45\n",
    "        ## Then rename forecast_datetime to datetime and join on datetime\n",
    "        \"\"\"\n",
    "        Processes the forecast weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "        # keep only datetimes from our relevant period\n",
    "        df = df[(df['hours_ahead'] < 46) & (df['hours_ahead'] > 21)]\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "    \n",
    "    def add_gas_prices_lag_features(self, df):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by date, if it's not already sorted\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Calculate rolling averages for different time windows\n",
    "        df['lowest_price_3d_avg'] = df['lowest_price_per_mwh'].rolling(window=3).mean()\n",
    "        df['highest_price_3d_avg'] = df['highest_price_per_mwh'].rolling(window=3).mean()\n",
    "\n",
    "        df['lowest_price_7d_avg'] = df['lowest_price_per_mwh'].rolling(window=7).mean()\n",
    "        df['highest_price_7d_avg'] = df['highest_price_per_mwh'].rolling(window=7).mean()\n",
    "\n",
    "        df['lowest_price_14d_avg'] = df['lowest_price_per_mwh'].rolling(window=14).mean()\n",
    "        df['highest_price_14d_avg'] = df['highest_price_per_mwh'].rolling(window=14).mean()\n",
    "\n",
    "        # Reset the index if you want the 'date' column back\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def init_gas_prices(self, df):\n",
    "        ## LAG: 1 DAY\n",
    "        ## Predictions are made from 2 days ago and predict for yesterday\n",
    "        ## add one day to forecast_date\n",
    "        ## Rename forecast_date to date, join on date\n",
    "        \"\"\"\n",
    "        Processes the gas prices data.\n",
    "        Implement the logic to handle gas prices data processing here.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['forecast_date']).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=1)\n",
    "        df = self.add_gas_prices_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_revealed_target_features(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['hour'] = df.datetime.dt.hour\n",
    "        df['day'] = df.datetime.dt.dayofweek\n",
    "        df.set_index('datetime', inplace=True)\n",
    "\n",
    "        window_size = 7\n",
    "        # Group by the specified columns and then apply the rolling mean\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_allp_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        df = df.drop(['hour', 'day'], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def init_revealed_targets(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=2)\n",
    "        df = self.add_revealed_target_features(df)\n",
    "        return df\n",
    "    \n",
    "    def init_client(self, df):\n",
    "        ## LAG: 2 days\n",
    "        ## Add 2 days to date, join on date\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=2)\n",
    "        # df = self.get_data_block_id(df, 'date')\n",
    "        return df\n",
    "\n",
    "    def init_weather_mapping(self):\n",
    "        # https://www.kaggle.com/code/tsunotsuno/enefit-eda-baseline/notebook#Baseline\n",
    "        county_point_map = {\n",
    "            0: (59.4, 24.7), # \"HARJUMAA\"\n",
    "            1 : (58.8, 22.7), # \"HIIUMAA\"\n",
    "            2 : (59.1, 27.2), # \"IDA-VIRUMAA\"\n",
    "            3 : (58.8, 25.7), # \"JÄRVAMAA\"\n",
    "            4 : (58.8, 26.2), # \"JÕGEVAMAA\"\n",
    "            5 : (59.1, 23.7), # \"LÄÄNE-VIRUMAA\"\n",
    "            6 : (59.1, 23.7), # \"LÄÄNEMAA\"\n",
    "            7 : (58.5, 24.7), # \"PÄRNUMAA\"\n",
    "            8 : (58.2, 27.2), # \"PÕLVAMAA\"\n",
    "            9 : (58.8, 24.7), # \"RAPLAMAA\"\n",
    "            10 : (58.5, 22.7),# \"SAAREMAA\"\n",
    "            11 : (58.5, 26.7),# \"TARTUMAA\"\n",
    "            12 : (58.5, 25.2),# \"UNKNOWNN\" (center of the map)\n",
    "            13 : (57.9, 26.2),# \"VALGAMAA\"\n",
    "            14 : (58.2, 25.7),# \"VILJANDIMAA\"\n",
    "            15 : (57.9, 27.2) # \"VÕRUMAA\"\n",
    "        }\n",
    "        # Convert the dictionary to a list of tuples\n",
    "        data = [(county_code, lat, lon) for county_code, (lat, lon) in county_point_map.items()]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=['county', 'latitude', 'longitude'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        df['day_of_week'] = df['datetime'].dt.day_of_week\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        df['is_weekend'] = df['datetime'].dt.day_of_week >= 5\n",
    "        df['is_month_start'] = df['datetime'].dt.is_month_start\n",
    "        df['is_month_end'] = df['datetime'].dt.is_month_end\n",
    "        df['is_quarter_start'] = df['datetime'].dt.is_quarter_start\n",
    "        df['is_quarter_end'] = df['datetime'].dt.is_quarter_end\n",
    "        df['is_year_start'] = df['datetime'].dt.is_year_start\n",
    "        df['is_year_end'] = df['datetime'].dt.is_year_end\n",
    "        df['season'] = df['datetime'].dt.month % 12 // 3 + 1\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        # Calculate sin and cos for day of year\n",
    "        days_in_year = 365.25  # accounts for leap year\n",
    "        df['day_of_year_sin'] = np.sin((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        df['day_of_year_cos'] = np.cos((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        return df\n",
    "    \n",
    "    def add_ee_holidays(self, df):\n",
    "        import holidays\n",
    "        # Define Estonia public holidays\n",
    "        ee_holidays = holidays.CountryHoliday('EE')\n",
    "        \n",
    "        print(df['date'].isna().sum())\n",
    "        \n",
    "        def find_problem(x):\n",
    "            try:\n",
    "                return x in ee_holidays\n",
    "            except Exception as e:\n",
    "                print(x)\n",
    "                raise e\n",
    "\n",
    "        # Function to check if the date is a holiday\n",
    "        df['is_ee_holiday'] = df['date'].apply(lambda x: x in ee_holidays)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def remove_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'row_id',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_test_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def join_data(self, train, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices):\n",
    "        df = train\n",
    "        df = df.merge(revealed_targets, how='left', on=('datetime', 'county', 'is_business', 'product_type', 'is_consumption'), suffixes=('', '_rt'))\n",
    "        df = df.merge(client, how='left', on=('date', 'county', 'is_business', 'product_type'), suffixes=('', '_client'))\n",
    "        df = df.merge(historical_weather, how='left', on=('datetime', 'county'), suffixes=('', '_hw'))\n",
    "        df = df.merge(forecast_weather, how='left', on=('datetime', 'county'), suffixes=('', '_fw'))\n",
    "        df = df.merge(electricity_prices, how='left', on='datetime', suffixes=('', '_elec'))\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.merge(gas_prices, how='left', on='date', suffixes=('', '_gasp'))\n",
    "        df = self.add_date_features(df)\n",
    "        df = self.add_ee_holidays(df)\n",
    "        return df\n",
    "    \n",
    "    def add_test_data(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        dfs = [test.copy(), revealed_targets.copy(), client.copy(), historical_weather.copy(),\n",
    "                 forecast_weather.copy(), electricity_prices.copy(), gas_prices.copy()]\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "                \n",
    "            self.test_orig_dfs[i] = pd.concat([ self.test_orig_dfs[i], df ])          \n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_test_data_timestep(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        #append test data to test data cache\n",
    "        self.add_test_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        # process test data\n",
    "        test = self.init_train(self.test_orig_dfs[0].copy())\n",
    "        revealed_targets = self.init_revealed_targets(self.test_orig_dfs[1].copy())\n",
    "        client = self.init_client(self.test_orig_dfs[2].copy())\n",
    "        historical_weather = self.init_historical_weather(self.test_orig_dfs[3].copy())\n",
    "        forecast_weather = self.init_forecast_weather(self.test_orig_dfs[4].copy())\n",
    "        electricity_prices = self.init_electricity(self.test_orig_dfs[5].copy())\n",
    "        gas_prices = self.init_gas_prices(self.test_orig_dfs[6].copy())\n",
    "        \n",
    "        df_all_cols = self.join_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        df = self.remove_test_cols(df_all_cols)\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processor_new_pandas.pkl', 'rb') as f:\n",
    "    data_processor = pickle.load(f)\n",
    "data_processor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_drop_na(df):\n",
    "    df = df[~df.target.isna()]\n",
    "    df = df[~df.target_rolling_avg_24h.isna()]\n",
    "    means = df.mean()\n",
    "    # For each column, add an indicator column for NA values\n",
    "    # for col in df.columns:\n",
    "    #     if df[col].isna().any():\n",
    "    #         df[f'{col}_is_na'] = df[col].isna()\n",
    "    df = df.fillna(means)\n",
    "    return df, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processed_df_no_na, means = fill_drop_na(data_processor.df)\n",
    "processed_df_no_na.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "cv_ranges_corrected = [\n",
    "    ('2022-09-01', '2022-10-24'), \n",
    "    ('2022-10-25', '2022-12-17'), \n",
    "    ('2022-12-18', '2023-02-09'), \n",
    "    ('2023-02-10', '2023-04-04'), \n",
    "    ('2023-04-05', '2023-05-31')\n",
    "]\n",
    "\n",
    "# Function to convert a date string into a datetime object\n",
    "def to_datetime(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "# Converting the date strings in cv_ranges to datetime objects\n",
    "datetime_cv_ranges = [(to_datetime(start), to_datetime(end)) for start, end in cv_ranges_corrected]\n",
    "datetime_cv_ranges\n",
    "\n",
    "date_filter = data_processor.df_all_cols.date[processed_df_no_na.index]\n",
    "date_filter\n",
    "\n",
    "cv1_train = processed_df_no_na[date_filter <= datetime_cv_ranges[0][0]]\n",
    "cv1_test = processed_df_no_na[(date_filter <= datetime_cv_ranges[0][1]) & (date_filter > datetime_cv_ranges[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in range(5):\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target', 'target_installed_capacity']\n",
    "        drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    \"\"\"I am rewriting this training data processor to process a few more variables differently.\"\"\"\n",
    "\n",
    "    def __init__(self, train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices, for_testing=False,\n",
    "                add_log_cols=False):\n",
    "        self.add_log_cols = add_log_cols\n",
    "        self.test_orig_dfs = self.get_test_orig_dfs([train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices])\n",
    "        \n",
    "        self.weather_mapping = self.init_weather_mapping()\n",
    "        \n",
    "        if not for_testing:\n",
    "            self.train = self.init_train(train)\n",
    "            self.revealed_targets = self.init_revealed_targets(revealed_targets)\n",
    "            self.client = self.init_client(client)\n",
    "            \n",
    "            self.historical_weather = self.init_historical_weather(historical_weather)\n",
    "            self.forecast_weather = self.init_forecast_weather(forecast_weather)\n",
    "            self.electricity_prices = self.init_electricity(electricity_prices)\n",
    "            self.gas_prices = self.init_gas_prices(gas_prices)\n",
    "            \n",
    "            self.df_all_cols = self.join_data(self.train, self.revealed_targets, self.client, self.historical_weather, self.forecast_weather, self.electricity_prices, self.gas_prices)\n",
    "            if self.add_log_cols:\n",
    "                self.df_all_cols = self.create_log_cols(self.df_all_cols)\n",
    "            self.df = self.remove_cols(self.df_all_cols)\n",
    "            \n",
    "        \n",
    "    def get_test_orig_dfs(self, dfs):\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['prediction_datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'prediction_datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df.date).dt.date\n",
    "                col = 'date'\n",
    "\n",
    "            test_date = df[col].iloc[-1]  # Assuming test is a DataFrame\n",
    "            start_date = test_date - pd.Timedelta(days=14)\n",
    "            historical_subset = df[df[col] >= start_date]\n",
    "            dfs[i] = historical_subset.copy()\n",
    "        return dfs\n",
    "        \n",
    "    def init_train(self, df):\n",
    "        \"\"\"Prepares the training data for model training.\"\"\"\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        except Exception as e:\n",
    "            df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "        df['date'] = df.datetime.dt.date\n",
    "            \n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        return df\n",
    "    \n",
    "    def add_electricity_lag_features(self, df):\n",
    "        ##### mean from entire last week\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # Use rolling to calculate mean price of the last week\n",
    "        # The window is 7 days, min_periods can be set as per requirement\n",
    "        # 'closed' determines which side of the interval is closed; it can be 'right' or 'left'\n",
    "        df['mean_euros_per_mwh_last_week'] = df['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()\n",
    "        # Shift the results to align with the requirement of lagging\n",
    "        df['mean_euros_per_mwh_last_week'] = df['mean_euros_per_mwh_last_week'].shift()\n",
    "        \n",
    "        ##### mean from last week this hour only\n",
    "        # Extract hour from datetime\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        # Group by hour and apply rolling mean for each group\n",
    "        hourly_groups = df.groupby('hour')\n",
    "        dff = hourly_groups['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()#.shift()#.reset_index(level=0, drop=True)\n",
    "        dff = dff.reset_index().set_index('datetime').groupby('hour')['euros_per_mwh'].shift()\n",
    "        dff = dff.rename('mean_euros_per_mwh_same_hour_last_week')\n",
    "        df = df.join(dff)\n",
    "        #### yesterday's power price\n",
    "        df['yesterdays_euros_per_mwh'] = df['euros_per_mwh'].shift(24)\n",
    "        \n",
    "        ### 24h average\n",
    "        # Calculate the 24-hour rolling average\n",
    "        df['euros_per_mwh_24h_average_price'] = df['euros_per_mwh'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "        # Resetting the index if needed\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['forecast_date', 'origin_date', 'hour'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def init_electricity(self, df):\n",
    "        ## LAG = 1 Day\n",
    "        ## Move forecast datetime ahead by 1 day\n",
    "        ## change name to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_date'])\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        df = self.add_electricity_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_historical_weather_lag_features(self, df):\n",
    "        ##### LATEST WEATHER\n",
    "        def add_latest_weather(df):\n",
    "            # Assuming df is your original DataFrame\n",
    "            # Step 1: Convert datetime to a Datetime Object\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 2: Sorting the Data\n",
    "            df.sort_values(by=['datetime', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "            # Step 3: Creating a Unique Identifier for each location\n",
    "            df['location_id'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Step 4: Filtering for 10:00 AM Entries\n",
    "            df.reset_index(inplace=True)\n",
    "            df_10am = df[df['datetime'].dt.hour == 10]\n",
    "            df_10am.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 5: Shifting the Features by 1 day\n",
    "            lagged_features = df_10am.groupby('location_id').shift(periods=1, freq='D')\n",
    "            \n",
    "            # grouped = lagged_features.groupby('county')\n",
    "            # lagged_features = grouped[weather_features].mean()\n",
    "            \n",
    "            \n",
    "            # Renaming columns to indicate lag\n",
    "            lagged_features = lagged_features.add_suffix('_hw_lagged')\n",
    "            lagged_features['location_id'] = lagged_features['location_id_hw_lagged']\n",
    "            lagged_features.reset_index(inplace=True)\n",
    "            lagged_features['date'] = lagged_features.datetime.dt.date\n",
    "\n",
    "            df['date'] = df.datetime.dt.date\n",
    "            return lagged_features\n",
    "            # Step 6: Merging Lagged Features with Original DataFrame\n",
    "            df = df.merge(lagged_features, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "            return df\n",
    "        \n",
    "        ##### mean from last day\n",
    "        def add_24h_mean_var(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances'))\n",
    "\n",
    "            return my_df\n",
    "        \n",
    "        ##### mean from last day all estonia\n",
    "        def add_24h_mean_var_estonia(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means_estonia'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances_estonia'))\n",
    "\n",
    "            return my_df\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        weather_features = df.columns.drop(['datetime', 'latitude', 'longitude'])\n",
    "\n",
    "        # Apply the function\n",
    "        df = add_24h_mean_var(df, weather_features)    \n",
    "        df = add_24h_mean_var_estonia(df, weather_features)\n",
    "           \n",
    "        latest = add_latest_weather(df)\n",
    "        df = df.merge(latest, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_historical_weather(self, df):\n",
    "        ## LAG: From 11:00 AM 2 days ago to 10:00 AM 1 day ago\n",
    "        ## What to do? Give most recent weather forecast? Give average over the last day?\n",
    "        \"\"\"\n",
    "        Processes the historical weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df = self.add_historical_weather_lag_features(df)\n",
    "        \n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_forecast_weather(self, df):\n",
    "        ## LAG: DON't ADJUST\n",
    "        ##      The forecast is from yesterday, but can forecast today, which is 22 hours ahead\n",
    "        ## Drop any columns where:\n",
    "        ##                        hours_ahead < 22 and hours_ahead > 45\n",
    "        ## Then rename forecast_datetime to datetime and join on datetime\n",
    "        \"\"\"\n",
    "        Processes the forecast weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "        # keep only datetimes from our relevant period\n",
    "        df = df[(df['hours_ahead'] < 46) & (df['hours_ahead'] > 21)]\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "    \n",
    "    def add_gas_prices_lag_features(self, df):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by date, if it's not already sorted\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Calculate rolling averages for different time windows\n",
    "        df['lowest_price_3d_avg'] = df['lowest_price_per_mwh'].rolling(window=3).mean()\n",
    "        df['highest_price_3d_avg'] = df['highest_price_per_mwh'].rolling(window=3).mean()\n",
    "\n",
    "        df['lowest_price_7d_avg'] = df['lowest_price_per_mwh'].rolling(window=7).mean()\n",
    "        df['highest_price_7d_avg'] = df['highest_price_per_mwh'].rolling(window=7).mean()\n",
    "\n",
    "        df['lowest_price_14d_avg'] = df['lowest_price_per_mwh'].rolling(window=14).mean()\n",
    "        df['highest_price_14d_avg'] = df['highest_price_per_mwh'].rolling(window=14).mean()\n",
    "\n",
    "        # Reset the index if you want the 'date' column back\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def init_gas_prices(self, df):\n",
    "        ## LAG: 1 DAY\n",
    "        ## Predictions are made from 2 days ago and predict for yesterday\n",
    "        ## add one day to forecast_date\n",
    "        ## Rename forecast_date to date, join on date\n",
    "        \"\"\"\n",
    "        Processes the gas prices data.\n",
    "        Implement the logic to handle gas prices data processing here.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['forecast_date']).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=1)\n",
    "        df = self.add_gas_prices_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_revealed_target_features(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['hour'] = df.datetime.dt.hour\n",
    "        df['day'] = df.datetime.dt.dayofweek\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        # let me add some new features here too\n",
    "        # Adding lag features\n",
    "        # Step 2: Sorting the Data\n",
    "        df.sort_values(by=['datetime'], inplace=True)\n",
    "\n",
    "        # Step 3: Creating a Unique Identifier for each location\n",
    "        df['id'] = df['county'].astype(str) + '_' + df['is_business'].astype(str) + '_' + df['product_type'].astype(str) + '_' + df['is_consumption'].astype(str)\n",
    "        lagged_features = []\n",
    "        lagged_hours = []\n",
    "        ### Defining lagged target features\n",
    "\n",
    "        for lag_hours in range(1, 24):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "\n",
    "        for lag_hours in ([i*24 for i in range(1,8)] + [24*11, 24*12]):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "            \n",
    "        df.reset_index(inplace=True)\n",
    "        for lagged_feature, lag_hours in zip(lagged_features, lagged_hours):\n",
    "            lagged_feature.reset_index(inplace=True)\n",
    "            lagged_feature.dropna(inplace=True)\n",
    "            df = df.merge(lagged_feature[['datetime', 'target', 'id']], on=['id', 'datetime'], how='left', suffixes=('', f'_lag_{lag_hours}h'))\n",
    "\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        window_size = 7\n",
    "        # Group by the specified columns and then apply the rolling mean\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_allp_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        #All of estonia\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_allp_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        df = df.drop(['hour', 'day'], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def init_revealed_targets(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=2)\n",
    "        df = self.add_revealed_target_features(df)\n",
    "        return df\n",
    "    \n",
    "    def init_client(self, df):\n",
    "        ## LAG: 2 days\n",
    "        ## Add 2 days to date, join on date\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=2)\n",
    "        # df = self.get_data_block_id(df, 'date')\n",
    "        return df\n",
    "\n",
    "    def init_weather_mapping(self):\n",
    "        # https://www.kaggle.com/code/tsunotsuno/enefit-eda-baseline/notebook#Baseline\n",
    "        county_point_map = {\n",
    "            0: (59.4, 24.7), # \"HARJUMAA\"\n",
    "            1 : (58.8, 22.7), # \"HIIUMAA\"\n",
    "            2 : (59.1, 27.2), # \"IDA-VIRUMAA\"\n",
    "            3 : (58.8, 25.7), # \"JÄRVAMAA\"\n",
    "            4 : (58.8, 26.2), # \"JÕGEVAMAA\"\n",
    "            5 : (59.1, 23.7), # \"LÄÄNE-VIRUMAA\"\n",
    "            6 : (59.1, 23.7), # \"LÄÄNEMAA\"\n",
    "            7 : (58.5, 24.7), # \"PÄRNUMAA\"\n",
    "            8 : (58.2, 27.2), # \"PÕLVAMAA\"\n",
    "            9 : (58.8, 24.7), # \"RAPLAMAA\"\n",
    "            10 : (58.5, 22.7),# \"SAAREMAA\"\n",
    "            11 : (58.5, 26.7),# \"TARTUMAA\"\n",
    "            12 : (58.5, 25.2),# \"UNKNOWNN\" (center of the map)\n",
    "            13 : (57.9, 26.2),# \"VALGAMAA\"\n",
    "            14 : (58.2, 25.7),# \"VILJANDIMAA\"\n",
    "            15 : (57.9, 27.2) # \"VÕRUMAA\"\n",
    "        }\n",
    "        # Convert the dictionary to a list of tuples\n",
    "        data = [(county_code, lat, lon) for county_code, (lat, lon) in county_point_map.items()]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=['county', 'latitude', 'longitude'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        df['day_of_week'] = df['datetime'].dt.day_of_week\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        df['is_weekend'] = df['datetime'].dt.day_of_week >= 5\n",
    "        df['is_month_start'] = df['datetime'].dt.is_month_start\n",
    "        df['is_month_end'] = df['datetime'].dt.is_month_end\n",
    "        df['is_quarter_start'] = df['datetime'].dt.is_quarter_start\n",
    "        df['is_quarter_end'] = df['datetime'].dt.is_quarter_end\n",
    "        df['is_year_start'] = df['datetime'].dt.is_year_start\n",
    "        df['is_year_end'] = df['datetime'].dt.is_year_end\n",
    "        df['season'] = df['datetime'].dt.month % 12 // 3 + 1\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        # Calculate sin and cos for day of year\n",
    "        days_in_year = 365.25  # accounts for leap year\n",
    "        df['day_of_year_sin'] = np.sin((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        df['day_of_year_cos'] = np.cos((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        return df\n",
    "    \n",
    "    def add_ee_holidays(self, df):\n",
    "        import holidays\n",
    "        # Define Estonia public holidays\n",
    "        ee_holidays = holidays.CountryHoliday('EE')\n",
    "        \n",
    "        print(df['date'].isna().sum())\n",
    "        \n",
    "        def find_problem(x):\n",
    "            try:\n",
    "                return x in ee_holidays\n",
    "            except Exception as e:\n",
    "                print(x)\n",
    "                raise e\n",
    "\n",
    "        # Function to check if the date is a holiday\n",
    "        df['is_ee_holiday'] = df['date'].apply(lambda x: x in ee_holidays)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def create_log_cols(self, df):\n",
    "        log_cols = ['target_lag_1h', 'target_lag_2h', 'target_lag_3h', 'target_lag_4h',\n",
    "       'target_lag_5h', 'target_lag_6h', 'target_lag_7h', 'target_lag_8h',\n",
    "       'target_lag_9h', 'target_lag_10h', 'target_lag_11h', 'target_lag_12h',\n",
    "       'target_lag_13h', 'target_lag_14h', 'target_lag_15h', 'target_lag_16h',\n",
    "       'target_lag_17h', 'target_lag_18h', 'target_lag_19h', 'target_lag_20h',\n",
    "       'target_lag_21h', 'target_lag_22h', 'target_lag_23h', 'target_lag_24h',\n",
    "       'target_lag_48h', 'target_lag_72h', 'target_lag_96h', 'target_lag_120h',\n",
    "       'target_lag_144h', 'target_lag_168h', 'target_lag_264h',\n",
    "       'target_lag_288h', 'eic_count', 'installed_capacity', 'temperature', 'dewpoint', 'rain',\n",
    "       'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n",
    "       'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n",
    "       'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n",
    "       'diffuse_radiation', 'temperature_hw_means', 'dewpoint_hw_means',\n",
    "       'rain_hw_means', 'snowfall_hw_means', 'surface_pressure_hw_means',\n",
    "       'cloudcover_total_hw_means', 'cloudcover_low_hw_means',\n",
    "       'cloudcover_mid_hw_means', 'cloudcover_high_hw_means',\n",
    "       'windspeed_10m_hw_means', 'winddirection_10m_hw_means',\n",
    "       'shortwave_radiation_hw_means', 'direct_solar_radiation_hw_means',\n",
    "       'diffuse_radiation_hw_means', 'temperature_hw_variances',\n",
    "       'dewpoint_hw_variances', 'rain_hw_variances', 'snowfall_hw_variances',\n",
    "       'surface_pressure_hw_variances', 'cloudcover_total_hw_variances',\n",
    "       'cloudcover_low_hw_variances', 'cloudcover_mid_hw_variances',\n",
    "       'cloudcover_high_hw_variances', 'windspeed_10m_hw_variances',\n",
    "       'winddirection_10m_hw_variances', 'shortwave_radiation_hw_variances',\n",
    "       'direct_solar_radiation_hw_variances', 'diffuse_radiation_hw_variances',\n",
    "       'temperature_hw_lagged', 'dewpoint_hw_lagged', 'rain_hw_lagged',\n",
    "       'snowfall_hw_lagged', 'surface_pressure_hw_lagged',\n",
    "       'cloudcover_total_hw_lagged', 'cloudcover_low_hw_lagged', 'cloudcover_mid_hw_lagged',\n",
    "       'cloudcover_high_hw_lagged', 'windspeed_10m_hw_lagged',\n",
    "       'winddirection_10m_hw_lagged', 'shortwave_radiation_hw_lagged',\n",
    "       'direct_solar_radiation_hw_lagged', 'diffuse_radiation_hw_lagged',\n",
    "       'temperature_hw_means_hw_lagged', 'dewpoint_hw_means_hw_lagged',\n",
    "       'rain_hw_means_hw_lagged', 'snowfall_hw_means_hw_lagged',\n",
    "       'surface_pressure_hw_means_hw_lagged',\n",
    "       'cloudcover_total_hw_means_hw_lagged',\n",
    "       'cloudcover_low_hw_means_hw_lagged',\n",
    "       'cloudcover_mid_hw_means_hw_lagged',\n",
    "       'cloudcover_high_hw_means_hw_lagged',\n",
    "       'windspeed_10m_hw_means_hw_lagged',\n",
    "       'winddirection_10m_hw_means_hw_lagged',\n",
    "       'shortwave_radiation_hw_means_hw_lagged',\n",
    "       'direct_solar_radiation_hw_means_hw_lagged',\n",
    "       'diffuse_radiation_hw_means_hw_lagged',\n",
    "       'temperature_hw_variances_hw_lagged', 'dewpoint_hw_variances_hw_lagged',\n",
    "       'rain_hw_variances_hw_lagged', 'snowfall_hw_variances_hw_lagged',\n",
    "       'surface_pressure_hw_variances_hw_lagged',\n",
    "       'cloudcover_total_hw_variances_hw_lagged',\n",
    "       'cloudcover_low_hw_variances_hw_lagged',\n",
    "       'cloudcover_mid_hw_variances_hw_lagged',\n",
    "       'cloudcover_high_hw_variances_hw_lagged',\n",
    "       'windspeed_10m_hw_variances_hw_lagged',\n",
    "       'winddirection_10m_hw_variances_hw_lagged',\n",
    "       'shortwave_radiation_hw_variances_hw_lagged',\n",
    "       'direct_solar_radiation_hw_variances_hw_lagged',\n",
    "       'diffuse_radiation_hw_variances_hw_lagged', 'temperature_fw', 'dewpoint_fw', 'cloudcover_high_fw',\n",
    "       'cloudcover_low_fw', 'cloudcover_mid_fw', 'cloudcover_total_fw',\n",
    "       '10_metre_u_wind_component', '10_metre_v_wind_component',\n",
    "       'direct_solar_radiation_fw', 'surface_solar_radiation_downwards',\n",
    "       'snowfall_fw', 'total_precipitation', 'euros_per_mwh', 'mean_euros_per_mwh_last_week',\n",
    "       'mean_euros_per_mwh_same_hour_last_week', 'yesterdays_euros_per_mwh',\n",
    "       'euros_per_mwh_24h_average_price', 'lowest_price_per_mwh',\n",
    "       'highest_price_per_mwh', 'lowest_price_3d_avg', 'highest_price_3d_avg',\n",
    "       'lowest_price_7d_avg', 'highest_price_7d_avg', 'lowest_price_14d_avg',\n",
    "       'highest_price_14d_avg']\n",
    "        \n",
    "        log_cols = [col for col in log_cols if col in df.columns]\n",
    "        \n",
    "        dff = np.log1p(df[log_cols] )\n",
    "        dff.rename(columns={col: col + \"_log\" for col in log_cols}, inplace=True)\n",
    "        return pd.concat([df, dff], axis=1)\n",
    "        \n",
    "    \n",
    "    def remove_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'row_id',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                    'id',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_test_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                    'id'\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def join_data(self, train, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices):\n",
    "        df = train\n",
    "        df = df.merge(revealed_targets, how='left', on=('datetime', 'county', 'is_business', 'product_type', 'is_consumption'), suffixes=('', '_rt'))\n",
    "        df = df.merge(client, how='left', on=('date', 'county', 'is_business', 'product_type'), suffixes=('', '_client'))\n",
    "        df = df.merge(historical_weather, how='left', on=('datetime', 'county'), suffixes=('', '_hw'))\n",
    "        df = df.merge(forecast_weather, how='left', on=('datetime', 'county'), suffixes=('', '_fw'))\n",
    "        df = df.merge(electricity_prices, how='left', on='datetime', suffixes=('', '_elec'))\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.merge(gas_prices, how='left', on='date', suffixes=('', '_gasp'))\n",
    "        df = self.add_date_features(df)\n",
    "        df = self.add_ee_holidays(df)\n",
    "        return df\n",
    "    \n",
    "    def add_test_data(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        dfs = [test.copy(), revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices]\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "                \n",
    "            self.test_orig_dfs[i] = pd.concat([ self.test_orig_dfs[i], df ])          \n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_test_data_timestep(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        #append test data to test data cache\n",
    "        self.add_test_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        # process test data\n",
    "        test = self.init_train(self.test_orig_dfs[0])\n",
    "        revealed_targets = self.init_revealed_targets(self.test_orig_dfs[1])\n",
    "        client = self.init_client(self.test_orig_dfs[2])\n",
    "        historical_weather = self.init_historical_weather(self.test_orig_dfs[3])\n",
    "        forecast_weather = self.init_forecast_weather(self.test_orig_dfs[4])\n",
    "        electricity_prices = self.init_electricity(self.test_orig_dfs[5])\n",
    "        gas_prices = self.init_gas_prices(self.test_orig_dfs[6])\n",
    "        df_all_cols = self.join_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        if self.add_log_cols:\n",
    "            df_all_cols = self.create_log_cols(df_all_cols)\n",
    "        df = self.remove_test_cols(df_all_cols)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processor_lgbm1_new_pandas.pkl', 'rb') as f:\n",
    "    data_processor = pickle.load(f)\n",
    "data_processor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processed_df_no_na, means = fill_drop_na(data_processor.df)\n",
    "processed_df_no_na.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the date strings in cv_ranges to datetime objects\n",
    "datetime_cv_ranges = [(to_datetime(start), to_datetime(end)) for start, end in cv_ranges_corrected]\n",
    "datetime_cv_ranges\n",
    "\n",
    "date_filter = data_processor.df_all_cols.date[processed_df_no_na.index]\n",
    "date_filter\n",
    "\n",
    "cv1_train = processed_df_no_na[date_filter <= datetime_cv_ranges[0][0]]\n",
    "cv1_test = processed_df_no_na[(date_filter <= datetime_cv_ranges[0][1]) & (date_filter > datetime_cv_ranges[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in range(5):\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target', 'target_installed_capacity']\n",
    "        drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    \"\"\"I am rewriting this training data processor to process a few more variables differently.\"\"\"\n",
    "\n",
    "    def __init__(self, train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices, for_testing=False,\n",
    "                add_log_cols=False):\n",
    "        self.add_log_cols = add_log_cols\n",
    "        self.test_orig_dfs = self.get_test_orig_dfs([train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices])\n",
    "        \n",
    "        self.weather_mapping = self.init_weather_mapping()\n",
    "        \n",
    "        if not for_testing:\n",
    "            self.train = self.init_train(train)\n",
    "            self.revealed_targets = self.init_revealed_targets(revealed_targets)\n",
    "            self.client = self.init_client(client)\n",
    "            \n",
    "            self.historical_weather = self.init_historical_weather(historical_weather)\n",
    "            self.forecast_weather = self.init_forecast_weather(forecast_weather)\n",
    "            self.electricity_prices = self.init_electricity(electricity_prices)\n",
    "            self.gas_prices = self.init_gas_prices(gas_prices)\n",
    "            \n",
    "            self.df_all_cols = self.join_data(self.train, self.revealed_targets, self.client, self.historical_weather, self.forecast_weather, self.electricity_prices, self.gas_prices)\n",
    "            if self.add_log_cols:\n",
    "                self.df_all_cols = self.create_log_cols(self.df_all_cols)\n",
    "            self.df = self.remove_cols(self.df_all_cols)\n",
    "            \n",
    "        \n",
    "    def get_test_orig_dfs(self, dfs):\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['prediction_datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'prediction_datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df.date).dt.date\n",
    "                col = 'date'\n",
    "\n",
    "            test_date = df[col].iloc[-1]  # Assuming test is a DataFrame\n",
    "            start_date = test_date - pd.Timedelta(days=14)\n",
    "            historical_subset = df[df[col] >= start_date]\n",
    "            dfs[i] = historical_subset.copy()\n",
    "        return dfs\n",
    "        \n",
    "    def init_train(self, df):\n",
    "        \"\"\"Prepares the training data for model training.\"\"\"\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        except Exception as e:\n",
    "            df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "        df['date'] = df.datetime.dt.date\n",
    "            \n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        return df\n",
    "    \n",
    "    def add_electricity_lag_features(self, df):\n",
    "        ##### mean from entire last week\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # Use rolling to calculate mean price of the last week\n",
    "        # The window is 7 days, min_periods can be set as per requirement\n",
    "        # 'closed' determines which side of the interval is closed; it can be 'right' or 'left'\n",
    "        df['mean_euros_per_mwh_last_week'] = df['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()\n",
    "        # Shift the results to align with the requirement of lagging\n",
    "        df['mean_euros_per_mwh_last_week'] = df['mean_euros_per_mwh_last_week'].shift()\n",
    "        \n",
    "        ##### mean from last week this hour only\n",
    "        # Extract hour from datetime\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        # Group by hour and apply rolling mean for each group\n",
    "        hourly_groups = df.groupby('hour')\n",
    "        dff = hourly_groups['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()#.shift()#.reset_index(level=0, drop=True)\n",
    "        dff = dff.reset_index().set_index('datetime').groupby('hour')['euros_per_mwh'].shift()\n",
    "        dff = dff.rename('mean_euros_per_mwh_same_hour_last_week')\n",
    "        df = df.join(dff)\n",
    "        #### yesterday's power price\n",
    "        df['yesterdays_euros_per_mwh'] = df['euros_per_mwh'].shift(24)\n",
    "        \n",
    "        ### 24h average\n",
    "        # Calculate the 24-hour rolling average\n",
    "        df['euros_per_mwh_24h_average_price'] = df['euros_per_mwh'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "        # Resetting the index if needed\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['forecast_date', 'origin_date', 'hour'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def init_electricity(self, df):\n",
    "        ## LAG = 1 Day\n",
    "        ## Move forecast datetime ahead by 1 day\n",
    "        ## change name to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_date'])\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        df = self.add_electricity_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_historical_weather_lag_features(self, df):\n",
    "        ##### LATEST WEATHER\n",
    "        def add_latest_weather(df):\n",
    "            # Assuming df is your original DataFrame\n",
    "            # Step 1: Convert datetime to a Datetime Object\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 2: Sorting the Data\n",
    "            df.sort_values(by=['datetime', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "            # Step 3: Creating a Unique Identifier for each location\n",
    "            df['location_id'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Step 4: Filtering for 10:00 AM Entries\n",
    "            df.reset_index(inplace=True)\n",
    "            df_10am = df[df['datetime'].dt.hour == 10]\n",
    "            df_10am.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 5: Shifting the Features by 1 day\n",
    "            lagged_features = df_10am.groupby('location_id').shift(periods=1, freq='D')\n",
    "            \n",
    "            # grouped = lagged_features.groupby('county')\n",
    "            # lagged_features = grouped[weather_features].mean()\n",
    "            \n",
    "            \n",
    "            # Renaming columns to indicate lag\n",
    "            lagged_features = lagged_features.add_suffix('_hw_lagged')\n",
    "            lagged_features['location_id'] = lagged_features['location_id_hw_lagged']\n",
    "            lagged_features.reset_index(inplace=True)\n",
    "            lagged_features['date'] = lagged_features.datetime.dt.date\n",
    "\n",
    "            df['date'] = df.datetime.dt.date\n",
    "            return lagged_features\n",
    "            # Step 6: Merging Lagged Features with Original DataFrame\n",
    "            df = df.merge(lagged_features, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "            return df\n",
    "        \n",
    "        ##### mean from last day\n",
    "        def add_24h_mean_var(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances'))\n",
    "\n",
    "            return my_df\n",
    "        \n",
    "        ##### mean from last day all estonia\n",
    "        def add_24h_mean_var_estonia(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means_estonia'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances_estonia'))\n",
    "\n",
    "            return my_df\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        weather_features = df.columns.drop(['datetime', 'latitude', 'longitude'])\n",
    "\n",
    "        # Apply the function\n",
    "        df = add_24h_mean_var(df, weather_features)    \n",
    "        df = add_24h_mean_var_estonia(df, weather_features)\n",
    "           \n",
    "        latest = add_latest_weather(df)\n",
    "        df = df.merge(latest, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_historical_weather(self, df):\n",
    "        ## LAG: From 11:00 AM 2 days ago to 10:00 AM 1 day ago\n",
    "        ## What to do? Give most recent weather forecast? Give average over the last day?\n",
    "        \"\"\"\n",
    "        Processes the historical weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df = self.add_historical_weather_lag_features(df)\n",
    "        \n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_forecast_weather(self, df):\n",
    "        ## LAG: DON't ADJUST\n",
    "        ##      The forecast is from yesterday, but can forecast today, which is 22 hours ahead\n",
    "        ## Drop any columns where:\n",
    "        ##                        hours_ahead < 22 and hours_ahead > 45\n",
    "        ## Then rename forecast_datetime to datetime and join on datetime\n",
    "        \"\"\"\n",
    "        Processes the forecast weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "        # keep only datetimes from our relevant period\n",
    "        df = df[(df['hours_ahead'] < 46) & (df['hours_ahead'] > 21)]\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "    \n",
    "    def add_gas_prices_lag_features(self, df):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by date, if it's not already sorted\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Calculate rolling averages for different time windows\n",
    "        df['lowest_price_3d_avg'] = df['lowest_price_per_mwh'].rolling(window=3).mean()\n",
    "        df['highest_price_3d_avg'] = df['highest_price_per_mwh'].rolling(window=3).mean()\n",
    "\n",
    "        df['lowest_price_7d_avg'] = df['lowest_price_per_mwh'].rolling(window=7).mean()\n",
    "        df['highest_price_7d_avg'] = df['highest_price_per_mwh'].rolling(window=7).mean()\n",
    "\n",
    "        df['lowest_price_14d_avg'] = df['lowest_price_per_mwh'].rolling(window=14).mean()\n",
    "        df['highest_price_14d_avg'] = df['highest_price_per_mwh'].rolling(window=14).mean()\n",
    "\n",
    "        # Reset the index if you want the 'date' column back\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def init_gas_prices(self, df):\n",
    "        ## LAG: 1 DAY\n",
    "        ## Predictions are made from 2 days ago and predict for yesterday\n",
    "        ## add one day to forecast_date\n",
    "        ## Rename forecast_date to date, join on date\n",
    "        \"\"\"\n",
    "        Processes the gas prices data.\n",
    "        Implement the logic to handle gas prices data processing here.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['forecast_date']).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=1)\n",
    "        df = self.add_gas_prices_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_revealed_target_features(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['hour'] = df.datetime.dt.hour\n",
    "        df['day'] = df.datetime.dt.dayofweek\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        # let me add some new features here too\n",
    "        # Adding lag features\n",
    "        # Step 2: Sorting the Data\n",
    "        df.sort_values(by=['datetime'], inplace=True)\n",
    "\n",
    "        # Step 3: Creating a Unique Identifier for each location\n",
    "        df['id'] = df['county'].astype(str) + '_' + df['is_business'].astype(str) + '_' + df['product_type'].astype(str) + '_' + df['is_consumption'].astype(str)\n",
    "        lagged_features = []\n",
    "        lagged_hours = []\n",
    "        ### Defining lagged target features\n",
    "\n",
    "        for lag_hours in range(1, 24):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "\n",
    "        for lag_hours in ([i*24 for i in range(1,8)] + [24*11, 24*12]):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "            \n",
    "        df.reset_index(inplace=True)\n",
    "        for lagged_feature, lag_hours in zip(lagged_features, lagged_hours):\n",
    "            lagged_feature.reset_index(inplace=True)\n",
    "            lagged_feature.dropna(inplace=True)\n",
    "            df = df.merge(lagged_feature[['datetime', 'target', 'id']], on=['id', 'datetime'], how='left', suffixes=('', f'_lag_{lag_hours}h'))\n",
    "\n",
    "        # add exponentially weighted mean features\n",
    "        alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\n",
    "        lags   = [1,2,3] + [i*24 for i in range(1,8)] + [24*11, 24*12]\n",
    "        for alpha in alphas:\n",
    "            print(f'starting alpha: {alpha}')\n",
    "            for lag in lags:\n",
    "                print(f'starting lag: {lag}')\n",
    "                df['target_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = df.groupby('id')['target'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        window_size = 7\n",
    "        # Group by the specified columns and then apply the rolling mean\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_allp_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        #All of estonia\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_allp_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        df = df.drop(['hour', 'day'], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def init_revealed_targets(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=2)\n",
    "        df = self.add_revealed_target_features(df)\n",
    "        return df\n",
    "    \n",
    "    def init_client(self, df):\n",
    "        ## LAG: 2 days\n",
    "        ## Add 2 days to date, join on date\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=2)\n",
    "        # df = self.get_data_block_id(df, 'date')\n",
    "        return df\n",
    "\n",
    "    def init_weather_mapping(self):\n",
    "        # https://www.kaggle.com/code/tsunotsuno/enefit-eda-baseline/notebook#Baseline\n",
    "        county_point_map = {\n",
    "            0: (59.4, 24.7), # \"HARJUMAA\"\n",
    "            1 : (58.8, 22.7), # \"HIIUMAA\"\n",
    "            2 : (59.1, 27.2), # \"IDA-VIRUMAA\"\n",
    "            3 : (58.8, 25.7), # \"JÄRVAMAA\"\n",
    "            4 : (58.8, 26.2), # \"JÕGEVAMAA\"\n",
    "            5 : (59.1, 23.7), # \"LÄÄNE-VIRUMAA\"\n",
    "            6 : (59.1, 23.7), # \"LÄÄNEMAA\"\n",
    "            7 : (58.5, 24.7), # \"PÄRNUMAA\"\n",
    "            8 : (58.2, 27.2), # \"PÕLVAMAA\"\n",
    "            9 : (58.8, 24.7), # \"RAPLAMAA\"\n",
    "            10 : (58.5, 22.7),# \"SAAREMAA\"\n",
    "            11 : (58.5, 26.7),# \"TARTUMAA\"\n",
    "            12 : (58.5, 25.2),# \"UNKNOWNN\" (center of the map)\n",
    "            13 : (57.9, 26.2),# \"VALGAMAA\"\n",
    "            14 : (58.2, 25.7),# \"VILJANDIMAA\"\n",
    "            15 : (57.9, 27.2) # \"VÕRUMAA\"\n",
    "        }\n",
    "        # Convert the dictionary to a list of tuples\n",
    "        data = [(county_code, lat, lon) for county_code, (lat, lon) in county_point_map.items()]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=['county', 'latitude', 'longitude'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        df['day_of_week'] = df['datetime'].dt.day_of_week\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        df['is_weekend'] = df['datetime'].dt.day_of_week >= 5\n",
    "        df['is_month_start'] = df['datetime'].dt.is_month_start\n",
    "        df['is_month_end'] = df['datetime'].dt.is_month_end\n",
    "        df['is_quarter_start'] = df['datetime'].dt.is_quarter_start\n",
    "        df['is_quarter_end'] = df['datetime'].dt.is_quarter_end\n",
    "        df['is_year_start'] = df['datetime'].dt.is_year_start\n",
    "        df['is_year_end'] = df['datetime'].dt.is_year_end\n",
    "        df['season'] = df['datetime'].dt.month % 12 // 3 + 1\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        # Calculate sin and cos for day of year\n",
    "        days_in_year = 365.25  # accounts for leap year\n",
    "        df['day_of_year_sin'] = np.sin((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        df['day_of_year_cos'] = np.cos((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        return df\n",
    "    \n",
    "    def add_ee_holidays(self, df):\n",
    "        import holidays\n",
    "        # Define Estonia public holidays\n",
    "        ee_holidays = holidays.CountryHoliday('EE')\n",
    "        \n",
    "        print(df['date'].isna().sum())\n",
    "        \n",
    "        def find_problem(x):\n",
    "            try:\n",
    "                return x in ee_holidays\n",
    "            except Exception as e:\n",
    "                print(x)\n",
    "                raise e\n",
    "\n",
    "        # Function to check if the date is a holiday\n",
    "        df['is_ee_holiday'] = df['date'].apply(lambda x: x in ee_holidays)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def create_log_cols(self, df):\n",
    "        log_cols = ['target_lag_1h', 'target_lag_2h', 'target_lag_3h', 'target_lag_4h',\n",
    "       'target_lag_5h', 'target_lag_6h', 'target_lag_7h', 'target_lag_8h',\n",
    "       'target_lag_9h', 'target_lag_10h', 'target_lag_11h', 'target_lag_12h',\n",
    "       'target_lag_13h', 'target_lag_14h', 'target_lag_15h', 'target_lag_16h',\n",
    "       'target_lag_17h', 'target_lag_18h', 'target_lag_19h', 'target_lag_20h',\n",
    "       'target_lag_21h', 'target_lag_22h', 'target_lag_23h', 'target_lag_24h',\n",
    "       'target_lag_48h', 'target_lag_72h', 'target_lag_96h', 'target_lag_120h',\n",
    "       'target_lag_144h', 'target_lag_168h', 'target_lag_264h',\n",
    "       'target_lag_288h', 'eic_count', 'installed_capacity', 'temperature', 'dewpoint', 'rain',\n",
    "       'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n",
    "       'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n",
    "       'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n",
    "       'diffuse_radiation', 'temperature_hw_means', 'dewpoint_hw_means',\n",
    "       'rain_hw_means', 'snowfall_hw_means', 'surface_pressure_hw_means',\n",
    "       'cloudcover_total_hw_means', 'cloudcover_low_hw_means',\n",
    "       'cloudcover_mid_hw_means', 'cloudcover_high_hw_means',\n",
    "       'windspeed_10m_hw_means', 'winddirection_10m_hw_means',\n",
    "       'shortwave_radiation_hw_means', 'direct_solar_radiation_hw_means',\n",
    "       'diffuse_radiation_hw_means', 'temperature_hw_variances',\n",
    "       'dewpoint_hw_variances', 'rain_hw_variances', 'snowfall_hw_variances',\n",
    "       'surface_pressure_hw_variances', 'cloudcover_total_hw_variances',\n",
    "       'cloudcover_low_hw_variances', 'cloudcover_mid_hw_variances',\n",
    "       'cloudcover_high_hw_variances', 'windspeed_10m_hw_variances',\n",
    "       'winddirection_10m_hw_variances', 'shortwave_radiation_hw_variances',\n",
    "       'direct_solar_radiation_hw_variances', 'diffuse_radiation_hw_variances',\n",
    "       'temperature_hw_lagged', 'dewpoint_hw_lagged', 'rain_hw_lagged',\n",
    "       'snowfall_hw_lagged', 'surface_pressure_hw_lagged',\n",
    "       'cloudcover_total_hw_lagged', 'cloudcover_low_hw_lagged', 'cloudcover_mid_hw_lagged',\n",
    "       'cloudcover_high_hw_lagged', 'windspeed_10m_hw_lagged',\n",
    "       'winddirection_10m_hw_lagged', 'shortwave_radiation_hw_lagged',\n",
    "       'direct_solar_radiation_hw_lagged', 'diffuse_radiation_hw_lagged',\n",
    "       'temperature_hw_means_hw_lagged', 'dewpoint_hw_means_hw_lagged',\n",
    "       'rain_hw_means_hw_lagged', 'snowfall_hw_means_hw_lagged',\n",
    "       'surface_pressure_hw_means_hw_lagged',\n",
    "       'cloudcover_total_hw_means_hw_lagged',\n",
    "       'cloudcover_low_hw_means_hw_lagged',\n",
    "       'cloudcover_mid_hw_means_hw_lagged',\n",
    "       'cloudcover_high_hw_means_hw_lagged',\n",
    "       'windspeed_10m_hw_means_hw_lagged',\n",
    "       'winddirection_10m_hw_means_hw_lagged',\n",
    "       'shortwave_radiation_hw_means_hw_lagged',\n",
    "       'direct_solar_radiation_hw_means_hw_lagged',\n",
    "       'diffuse_radiation_hw_means_hw_lagged',\n",
    "       'temperature_hw_variances_hw_lagged', 'dewpoint_hw_variances_hw_lagged',\n",
    "       'rain_hw_variances_hw_lagged', 'snowfall_hw_variances_hw_lagged',\n",
    "       'surface_pressure_hw_variances_hw_lagged',\n",
    "       'cloudcover_total_hw_variances_hw_lagged',\n",
    "       'cloudcover_low_hw_variances_hw_lagged',\n",
    "       'cloudcover_mid_hw_variances_hw_lagged',\n",
    "       'cloudcover_high_hw_variances_hw_lagged',\n",
    "       'windspeed_10m_hw_variances_hw_lagged',\n",
    "       'winddirection_10m_hw_variances_hw_lagged',\n",
    "       'shortwave_radiation_hw_variances_hw_lagged',\n",
    "       'direct_solar_radiation_hw_variances_hw_lagged',\n",
    "       'diffuse_radiation_hw_variances_hw_lagged', 'temperature_fw', 'dewpoint_fw', 'cloudcover_high_fw',\n",
    "       'cloudcover_low_fw', 'cloudcover_mid_fw', 'cloudcover_total_fw',\n",
    "       '10_metre_u_wind_component', '10_metre_v_wind_component',\n",
    "       'direct_solar_radiation_fw', 'surface_solar_radiation_downwards',\n",
    "       'snowfall_fw', 'total_precipitation', 'euros_per_mwh', 'mean_euros_per_mwh_last_week',\n",
    "       'mean_euros_per_mwh_same_hour_last_week', 'yesterdays_euros_per_mwh',\n",
    "       'euros_per_mwh_24h_average_price', 'lowest_price_per_mwh',\n",
    "       'highest_price_per_mwh', 'lowest_price_3d_avg', 'highest_price_3d_avg',\n",
    "       'lowest_price_7d_avg', 'highest_price_7d_avg', 'lowest_price_14d_avg',\n",
    "       'highest_price_14d_avg']\n",
    "        \n",
    "        log_cols = [col for col in log_cols if col in df.columns]\n",
    "        \n",
    "        dff = np.log1p(df[log_cols] )\n",
    "        dff.rename(columns={col: col + \"_log\" for col in log_cols}, inplace=True)\n",
    "        return pd.concat([df, dff], axis=1)\n",
    "        \n",
    "    \n",
    "    def remove_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'row_id',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                    'id',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_test_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                    'id'\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def join_data(self, train, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices):\n",
    "        df = train\n",
    "        df = df.merge(revealed_targets, how='left', on=('datetime', 'county', 'is_business', 'product_type', 'is_consumption'), suffixes=('', '_rt'))\n",
    "        df = df.merge(client, how='left', on=('date', 'county', 'is_business', 'product_type'), suffixes=('', '_client'))\n",
    "        df = df.merge(historical_weather, how='left', on=('datetime', 'county'), suffixes=('', '_hw'))\n",
    "        df = df.merge(forecast_weather, how='left', on=('datetime', 'county'), suffixes=('', '_fw'))\n",
    "        df = df.merge(electricity_prices, how='left', on='datetime', suffixes=('', '_elec'))\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.merge(gas_prices, how='left', on='date', suffixes=('', '_gasp'))\n",
    "        df = self.add_date_features(df)\n",
    "        df = self.add_ee_holidays(df)\n",
    "        return df\n",
    "    \n",
    "    def add_test_data(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        dfs = [test.copy(), revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices]\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "                \n",
    "            self.test_orig_dfs[i] = pd.concat([ self.test_orig_dfs[i], df ])          \n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_test_data_timestep(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        #append test data to test data cache\n",
    "        self.add_test_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        # process test data\n",
    "        test = self.init_train(self.test_orig_dfs[0])\n",
    "        revealed_targets = self.init_revealed_targets(self.test_orig_dfs[1])\n",
    "        client = self.init_client(self.test_orig_dfs[2])\n",
    "        historical_weather = self.init_historical_weather(self.test_orig_dfs[3])\n",
    "        forecast_weather = self.init_forecast_weather(self.test_orig_dfs[4])\n",
    "        electricity_prices = self.init_electricity(self.test_orig_dfs[5])\n",
    "        gas_prices = self.init_gas_prices(self.test_orig_dfs[6])\n",
    "        df_all_cols = self.join_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        if self.add_log_cols:\n",
    "            df_all_cols = self.create_log_cols(df_all_cols)\n",
    "        df = self.remove_test_cols(df_all_cols)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_processor_lgbm3_new_pandas.pkl', 'rb') as f:\n",
    "    data_processor = pickle.load(f)\n",
    "data_processor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processed_df_no_na, means = fill_drop_na(data_processor.df)\n",
    "processed_df_no_na.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the date strings in cv_ranges to datetime objects\n",
    "datetime_cv_ranges = [(to_datetime(start), to_datetime(end)) for start, end in cv_ranges_corrected]\n",
    "datetime_cv_ranges\n",
    "\n",
    "date_filter = data_processor.df_all_cols.date[processed_df_no_na.index]\n",
    "date_filter\n",
    "\n",
    "cv1_train = processed_df_no_na[date_filter <= datetime_cv_ranges[0][0]]\n",
    "cv1_test = processed_df_no_na[(date_filter <= datetime_cv_ranges[0][1]) & (date_filter > datetime_cv_ranges[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in range(5):\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target', 'target_installed_capacity']\n",
    "        drop_cols = ['target', 'target_installed_capacity', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
