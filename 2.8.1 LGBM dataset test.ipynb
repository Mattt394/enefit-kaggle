{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    \"\"\"Processes Train data, using train data as a warm start, and prepares it for inference.\"\"\"\n",
    "\n",
    "    def __init__(self, train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices):\n",
    "        self.test_orig_dfs = self.get_test_orig_dfs([train.copy(), revealed_targets.copy(), client.copy(), historical_weather.copy(),\n",
    "                 forecast_weather.copy(), electricity_prices.copy(), gas_prices.copy()])\n",
    "        self.train = self.init_train(train)\n",
    "        self.revealed_targets = self.init_revealed_targets(revealed_targets)\n",
    "        self.client = self.init_client(client)\n",
    "        self.weather_mapping = self.init_weather_mapping()\n",
    "        self.historical_weather = self.init_historical_weather(historical_weather)\n",
    "        self.forecast_weather = self.init_forecast_weather(forecast_weather)\n",
    "        self.electricity_prices = self.init_electricity(electricity_prices)\n",
    "        self.gas_prices = self.init_gas_prices(gas_prices)\n",
    "        \n",
    "        self.df_all_cols = self.join_data(self.train, self.revealed_targets, self.client, self.historical_weather, self.forecast_weather, self.electricity_prices, self.gas_prices)\n",
    "        self.df = self.remove_cols(self.df_all_cols)\n",
    "        \n",
    "    def get_test_orig_dfs(self, dfs):\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['prediction_datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'prediction_datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df.date).dt.date\n",
    "                col = 'date'\n",
    "\n",
    "            test_date = df[col].iloc[-1]  # Assuming test is a DataFrame\n",
    "            start_date = test_date - pd.Timedelta(days=14)\n",
    "            historical_subset = df[df[col] >= start_date]\n",
    "            dfs[i] = historical_subset\n",
    "        return dfs\n",
    "        \n",
    "    def init_train(self, df):\n",
    "        \"\"\"Prepares the training data for model training.\"\"\"\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        except Exception as e:\n",
    "            df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "        df['date'] = df.datetime.dt.date\n",
    "            \n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        return df\n",
    "    \n",
    "    def add_electricity_lag_features(self, df):\n",
    "        ##### mean from entire last week\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # Use rolling to calculate mean price of the last week\n",
    "        # The window is 7 days, min_periods can be set as per requirement\n",
    "        # 'closed' determines which side of the interval is closed; it can be 'right' or 'left'\n",
    "        df['mean_euros_per_mwh_last_week'] = df['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()\n",
    "        # Shift the results to align with the requirement of lagging\n",
    "        df['mean_euros_per_mwh_last_week'] = df['mean_euros_per_mwh_last_week'].shift()\n",
    "        \n",
    "        ##### mean from last week this hour only\n",
    "        # Extract hour from datetime\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        # Group by hour and apply rolling mean for each group\n",
    "        hourly_groups = df.groupby('hour')\n",
    "        dff = hourly_groups['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()#.shift()#.reset_index(level=0, drop=True)\n",
    "        dff = dff.reset_index().set_index('datetime').groupby('hour')['euros_per_mwh'].shift()\n",
    "        dff = dff.rename('mean_euros_per_mwh_same_hour_last_week')\n",
    "        df = df.join(dff)\n",
    "        #### yesterday's power price\n",
    "        df['yesterdays_euros_per_mwh'] = df['euros_per_mwh'].shift(24)\n",
    "        \n",
    "        ### 24h average\n",
    "        # Calculate the 24-hour rolling average\n",
    "        df['euros_per_mwh_24h_average_price'] = df['euros_per_mwh'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "        # Resetting the index if needed\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['forecast_date', 'origin_date', 'hour'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def init_electricity(self, df):\n",
    "        ## LAG = 1 Day\n",
    "        ## Move forecast datetime ahead by 1 day\n",
    "        ## change name to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_date'])\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        df = self.add_electricity_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_historical_weather_lag_features(self, df):\n",
    "        ##### LATEST WEATHER\n",
    "        def add_latest_weather(df):\n",
    "            # Assuming df is your original DataFrame\n",
    "            # Step 1: Convert datetime to a Datetime Object\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 2: Sorting the Data\n",
    "            df.sort_values(by=['datetime', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "            # Step 3: Creating a Unique Identifier for each location\n",
    "            df['location_id'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Step 4: Filtering for 10:00 AM Entries\n",
    "            df.reset_index(inplace=True)\n",
    "            df_10am = df[df['datetime'].dt.hour == 10]\n",
    "            df_10am.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 5: Shifting the Features by 1 day\n",
    "            lagged_features = df_10am.groupby('location_id').shift(periods=1, freq='D')\n",
    "\n",
    "            # Renaming columns to indicate lag\n",
    "            lagged_features = lagged_features.add_suffix('_hw_lagged')\n",
    "            lagged_features['location_id'] = lagged_features['location_id_hw_lagged']\n",
    "            lagged_features.reset_index(inplace=True)\n",
    "            lagged_features['date'] = lagged_features.datetime.dt.date\n",
    "\n",
    "            df['date'] = df.datetime.dt.date\n",
    "            return lagged_features\n",
    "            # Step 6: Merging Lagged Features with Original DataFrame\n",
    "            df = df.merge(lagged_features, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "            return df\n",
    "        \n",
    "        ##### mean from last day\n",
    "        def add_24h_mean_var(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 10:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, left_on='time_code', right_on='group', suffixes=('', '_hw_means'), how='left')\n",
    "            my_df = my_df.merge(variances, left_on='time_code', right_on='group', how='left', suffixes=('', '_hw_variances'))\n",
    "\n",
    "            return my_df\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        weather_features = df.columns.drop(['datetime', 'latitude', 'longitude'])\n",
    "\n",
    "        # Apply the function\n",
    "        df = add_24h_mean_var(df, weather_features)       \n",
    "        latest = add_latest_weather(df)\n",
    "        df = df.merge(latest, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_historical_weather(self, df):\n",
    "        ## LAG: From 11:00 AM 2 days ago to 10:00 AM 1 day ago\n",
    "        ## What to do? Give most recent weather forecast? Give average over the last day?\n",
    "        \"\"\"\n",
    "        Processes the historical weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        \n",
    "        df = self.add_historical_weather_lag_features(df)\n",
    "        \n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "\n",
    "    def init_forecast_weather(self, df):\n",
    "        ## LAG: DON't ADJUST\n",
    "        ##      The forecast is from yesterday, but can forecast today, which is 22 hours ahead\n",
    "        ## Drop any columns where:\n",
    "        ##                        hours_ahead < 22 and hours_ahead > 45\n",
    "        ## Then rename forecast_datetime to datetime and join on datetime\n",
    "        \"\"\"\n",
    "        Processes the forecast weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "        # keep only datetimes from our relevant period\n",
    "        df = df[(df['hours_ahead'] < 46) & (df['hours_ahead'] > 21)]\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "    \n",
    "    def add_gas_prices_lag_features(self, df):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by date, if it's not already sorted\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Calculate rolling averages for different time windows\n",
    "        df['lowest_price_3d_avg'] = df['lowest_price_per_mwh'].rolling(window=3).mean()\n",
    "        df['highest_price_3d_avg'] = df['highest_price_per_mwh'].rolling(window=3).mean()\n",
    "\n",
    "        df['lowest_price_7d_avg'] = df['lowest_price_per_mwh'].rolling(window=7).mean()\n",
    "        df['highest_price_7d_avg'] = df['highest_price_per_mwh'].rolling(window=7).mean()\n",
    "\n",
    "        df['lowest_price_14d_avg'] = df['lowest_price_per_mwh'].rolling(window=14).mean()\n",
    "        df['highest_price_14d_avg'] = df['highest_price_per_mwh'].rolling(window=14).mean()\n",
    "\n",
    "        # Reset the index if you want the 'date' column back\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def init_gas_prices(self, df):\n",
    "        ## LAG: 1 DAY\n",
    "        ## Predictions are made from 2 days ago and predict for yesterday\n",
    "        ## add one day to forecast_date\n",
    "        ## Rename forecast_date to date, join on date\n",
    "        \"\"\"\n",
    "        Processes the gas prices data.\n",
    "        Implement the logic to handle gas prices data processing here.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['forecast_date']).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=1)\n",
    "        df = self.add_gas_prices_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_revealed_target_features(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['hour'] = df.datetime.dt.hour\n",
    "        df['day'] = df.datetime.dt.dayofweek\n",
    "        df.set_index('datetime', inplace=True)\n",
    "\n",
    "        window_size = 7\n",
    "        # Group by the specified columns and then apply the rolling mean\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_allp_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        df = df.drop(['hour', 'day'], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def init_revealed_targets(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=2)\n",
    "        df = self.add_revealed_target_features(df)\n",
    "        return df\n",
    "    \n",
    "    def init_client(self, df):\n",
    "        ## LAG: 2 days\n",
    "        ## Add 2 days to date, join on date\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=2)\n",
    "        # df = self.get_data_block_id(df, 'date')\n",
    "        return df\n",
    "\n",
    "    def init_weather_mapping(self):\n",
    "        # https://www.kaggle.com/code/tsunotsuno/enefit-eda-baseline/notebook#Baseline\n",
    "        county_point_map = {\n",
    "            0: (59.4, 24.7), # \"HARJUMAA\"\n",
    "            1 : (58.8, 22.7), # \"HIIUMAA\"\n",
    "            2 : (59.1, 27.2), # \"IDA-VIRUMAA\"\n",
    "            3 : (58.8, 25.7), # \"JÄRVAMAA\"\n",
    "            4 : (58.8, 26.2), # \"JÕGEVAMAA\"\n",
    "            5 : (59.1, 23.7), # \"LÄÄNE-VIRUMAA\"\n",
    "            6 : (59.1, 23.7), # \"LÄÄNEMAA\"\n",
    "            7 : (58.5, 24.7), # \"PÄRNUMAA\"\n",
    "            8 : (58.2, 27.2), # \"PÕLVAMAA\"\n",
    "            9 : (58.8, 24.7), # \"RAPLAMAA\"\n",
    "            10 : (58.5, 22.7),# \"SAAREMAA\"\n",
    "            11 : (58.5, 26.7),# \"TARTUMAA\"\n",
    "            12 : (58.5, 25.2),# \"UNKNOWNN\" (center of the map)\n",
    "            13 : (57.9, 26.2),# \"VALGAMAA\"\n",
    "            14 : (58.2, 25.7),# \"VILJANDIMAA\"\n",
    "            15 : (57.9, 27.2) # \"VÕRUMAA\"\n",
    "        }\n",
    "        # Convert the dictionary to a list of tuples\n",
    "        data = [(county_code, lat, lon) for county_code, (lat, lon) in county_point_map.items()]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=['county', 'latitude', 'longitude'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        df['day_of_week'] = df['datetime'].dt.day_of_week\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        df['is_weekend'] = df['datetime'].dt.day_of_week >= 5\n",
    "        df['is_month_start'] = df['datetime'].dt.is_month_start\n",
    "        df['is_month_end'] = df['datetime'].dt.is_month_end\n",
    "        df['is_quarter_start'] = df['datetime'].dt.is_quarter_start\n",
    "        df['is_quarter_end'] = df['datetime'].dt.is_quarter_end\n",
    "        df['is_year_start'] = df['datetime'].dt.is_year_start\n",
    "        df['is_year_end'] = df['datetime'].dt.is_year_end\n",
    "        df['season'] = df['datetime'].dt.month % 12 // 3 + 1\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        # Calculate sin and cos for day of year\n",
    "        days_in_year = 365.25  # accounts for leap year\n",
    "        df['day_of_year_sin'] = np.sin((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        df['day_of_year_cos'] = np.cos((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        return df\n",
    "    \n",
    "    def add_ee_holidays(self, df):\n",
    "        import holidays\n",
    "        # Define Estonia public holidays\n",
    "        ee_holidays = holidays.CountryHoliday('EE')\n",
    "        \n",
    "        print(df['date'].isna().sum())\n",
    "        \n",
    "        def find_problem(x):\n",
    "            try:\n",
    "                return x in ee_holidays\n",
    "            except Exception as e:\n",
    "                print(x)\n",
    "                raise e\n",
    "\n",
    "        # Function to check if the date is a holiday\n",
    "        df['is_ee_holiday'] = df['date'].apply(lambda x: x in ee_holidays)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def remove_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'row_id',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_test_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def join_data(self, train, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices):\n",
    "        df = train\n",
    "        df = df.merge(revealed_targets, how='left', on=('datetime', 'county', 'is_business', 'product_type', 'is_consumption'), suffixes=('', '_rt'))\n",
    "        df = df.merge(client, how='left', on=('date', 'county', 'is_business', 'product_type'), suffixes=('', '_client'))\n",
    "        df = df.merge(historical_weather, how='left', on=('datetime', 'county'), suffixes=('', '_hw'))\n",
    "        df = df.merge(forecast_weather, how='left', on=('datetime', 'county'), suffixes=('', '_fw'))\n",
    "        df = df.merge(electricity_prices, how='left', on='datetime', suffixes=('', '_elec'))\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.merge(gas_prices, how='left', on='date', suffixes=('', '_gasp'))\n",
    "        df = self.add_date_features(df)\n",
    "        df = self.add_ee_holidays(df)\n",
    "        return df\n",
    "    \n",
    "    def add_test_data(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        dfs = [test.copy(), revealed_targets.copy(), client.copy(), historical_weather.copy(),\n",
    "                 forecast_weather.copy(), electricity_prices.copy(), gas_prices.copy()]\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "                \n",
    "            self.test_orig_dfs[i] = pd.concat([ self.test_orig_dfs[i], df ])          \n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_test_data_timestep(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        #append test data to test data cache\n",
    "        self.add_test_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        # process test data\n",
    "        test = self.init_train(self.test_orig_dfs[0].copy())\n",
    "        revealed_targets = self.init_revealed_targets(self.test_orig_dfs[1].copy())\n",
    "        client = self.init_client(self.test_orig_dfs[2].copy())\n",
    "        historical_weather = self.init_historical_weather(self.test_orig_dfs[3].copy())\n",
    "        forecast_weather = self.init_forecast_weather(self.test_orig_dfs[4].copy())\n",
    "        electricity_prices = self.init_electricity(self.test_orig_dfs[5].copy())\n",
    "        gas_prices = self.init_gas_prices(self.test_orig_dfs[6].copy())\n",
    "        \n",
    "        df_all_cols = self.join_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        df = self.remove_test_cols(df_all_cols)\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>is_business</th>\n",
       "      <th>product_type</th>\n",
       "      <th>target</th>\n",
       "      <th>is_consumption</th>\n",
       "      <th>target_rt</th>\n",
       "      <th>target_rolling_avg_24h</th>\n",
       "      <th>target_rolling_avg_hour_7d</th>\n",
       "      <th>target_rolling_avg_hour_hour_day_4w</th>\n",
       "      <th>target_rolling_allp_avg_24h</th>\n",
       "      <th>...</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>season</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "      <th>is_ee_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>96.590</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.314</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018609</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197.233</td>\n",
       "      <td>1</td>\n",
       "      <td>184.072</td>\n",
       "      <td>295.118417</td>\n",
       "      <td>278.497143</td>\n",
       "      <td>184.71275</td>\n",
       "      <td>90.640000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018610</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>156.335208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>170.148000</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018611</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.404</td>\n",
       "      <td>1</td>\n",
       "      <td>38.646</td>\n",
       "      <td>18.873583</td>\n",
       "      <td>34.405143</td>\n",
       "      <td>42.90750</td>\n",
       "      <td>92.029875</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018612</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>403.044625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>139.132958</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018613</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>196.240</td>\n",
       "      <td>1</td>\n",
       "      <td>183.756</td>\n",
       "      <td>105.720042</td>\n",
       "      <td>197.610286</td>\n",
       "      <td>191.84675</td>\n",
       "      <td>97.453167</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2018614 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         county  is_business  product_type   target  is_consumption  \\\n",
       "0             0            0             1    0.713               0   \n",
       "1             0            0             1   96.590               1   \n",
       "2             0            0             2    0.000               0   \n",
       "3             0            0             2   17.314               1   \n",
       "4             0            0             3    2.904               0   \n",
       "...         ...          ...           ...      ...             ...   \n",
       "2018609      15            1             0  197.233               1   \n",
       "2018610      15            1             1    0.000               0   \n",
       "2018611      15            1             1   28.404               1   \n",
       "2018612      15            1             3    0.000               0   \n",
       "2018613      15            1             3  196.240               1   \n",
       "\n",
       "         target_rt  target_rolling_avg_24h  target_rolling_avg_hour_7d  \\\n",
       "0              NaN                     NaN                         NaN   \n",
       "1              NaN                     NaN                         NaN   \n",
       "2              NaN                     NaN                         NaN   \n",
       "3              NaN                     NaN                         NaN   \n",
       "4              NaN                     NaN                         NaN   \n",
       "...            ...                     ...                         ...   \n",
       "2018609    184.072              295.118417                  278.497143   \n",
       "2018610      0.000              156.335208                    0.000000   \n",
       "2018611     38.646               18.873583                   34.405143   \n",
       "2018612      0.000              403.044625                    0.000000   \n",
       "2018613    183.756              105.720042                  197.610286   \n",
       "\n",
       "         target_rolling_avg_hour_hour_day_4w  target_rolling_allp_avg_24h  \\\n",
       "0                                        NaN                          NaN   \n",
       "1                                        NaN                          NaN   \n",
       "2                                        NaN                          NaN   \n",
       "3                                        NaN                          NaN   \n",
       "4                                        NaN                          NaN   \n",
       "...                                      ...                          ...   \n",
       "2018609                            184.71275                    90.640000   \n",
       "2018610                              0.00000                   170.148000   \n",
       "2018611                             42.90750                    92.029875   \n",
       "2018612                              0.00000                   139.132958   \n",
       "2018613                            191.84675                    97.453167   \n",
       "\n",
       "         ...  is_quarter_start  is_quarter_end  is_year_start  is_year_end  \\\n",
       "0        ...             False           False          False        False   \n",
       "1        ...             False           False          False        False   \n",
       "2        ...             False           False          False        False   \n",
       "3        ...             False           False          False        False   \n",
       "4        ...             False           False          False        False   \n",
       "...      ...               ...             ...            ...          ...   \n",
       "2018609  ...             False           False          False        False   \n",
       "2018610  ...             False           False          False        False   \n",
       "2018611  ...             False           False          False        False   \n",
       "2018612  ...             False           False          False        False   \n",
       "2018613  ...             False           False          False        False   \n",
       "\n",
       "         season  hour_sin  hour_cos  day_of_year_sin  day_of_year_cos  \\\n",
       "0             4  0.000000  1.000000        -0.861693        -0.507430   \n",
       "1             4  0.000000  1.000000        -0.861693        -0.507430   \n",
       "2             4  0.000000  1.000000        -0.861693        -0.507430   \n",
       "3             4  0.000000  1.000000        -0.861693        -0.507430   \n",
       "4             4  0.000000  1.000000        -0.861693        -0.507430   \n",
       "...         ...       ...       ...              ...              ...   \n",
       "2018609       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018610       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018611       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018612       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "2018613       2 -0.258819  0.965926         0.532227        -0.846602   \n",
       "\n",
       "         is_ee_holiday  \n",
       "0                False  \n",
       "1                False  \n",
       "2                False  \n",
       "3                False  \n",
       "4                False  \n",
       "...                ...  \n",
       "2018609          False  \n",
       "2018610          False  \n",
       "2018611          False  \n",
       "2018612          False  \n",
       "2018613          False  \n",
       "\n",
       "[2018614 rows x 145 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_processor.pkl', 'rb') as f:\n",
    "    data_processor = pickle.load(f)\n",
    "data_processor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_drop_na(df):\n",
    "    df = df[~df.target.isna()]\n",
    "    df = df[~df.target_rolling_avg_24h.isna()]\n",
    "    means = df.mean()\n",
    "    # For each column, add an indicator column for NA values\n",
    "    # for col in df.columns:\n",
    "    #     if df[col].isna().any():\n",
    "    #         df[f'{col}_is_na'] = df[col].isna()\n",
    "    df = df.fillna(means)\n",
    "    return df, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.92 s\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "county             0\n",
       "is_business        0\n",
       "product_type       0\n",
       "target             0\n",
       "is_consumption     0\n",
       "                  ..\n",
       "hour_sin           0\n",
       "hour_cos           0\n",
       "day_of_year_sin    0\n",
       "day_of_year_cos    0\n",
       "is_ee_holiday      0\n",
       "Length: 145, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "processed_df_no_na, means = fill_drop_na(data_processor.df)\n",
    "processed_df_no_na.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m datetime_cv_ranges \u001b[38;5;241m=\u001b[39m [(to_datetime(start), to_datetime(end)) \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m cv_ranges_corrected]\n\u001b[0;32m     17\u001b[0m datetime_cv_ranges\n\u001b[1;32m---> 19\u001b[0m date_filter \u001b[38;5;241m=\u001b[39m \u001b[43mdata_processor\u001b[49m\u001b[38;5;241m.\u001b[39mdf_all_cols\u001b[38;5;241m.\u001b[39mdate[processed_df_no_na\u001b[38;5;241m.\u001b[39mindex]\n\u001b[0;32m     20\u001b[0m date_filter\n\u001b[0;32m     22\u001b[0m cv1_train \u001b[38;5;241m=\u001b[39m processed_df_no_na[date_filter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m datetime_cv_ranges[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_processor' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "cv_ranges_corrected = [\n",
    "    ('2022-09-01', '2022-10-24'), \n",
    "    ('2022-10-25', '2022-12-17'), \n",
    "    ('2022-12-18', '2023-02-09'), \n",
    "    ('2023-02-10', '2023-04-04'), \n",
    "    ('2023-04-05', '2023-05-31')\n",
    "]\n",
    "\n",
    "# Function to convert a date string into a datetime object\n",
    "def to_datetime(date_str):\n",
    "    return datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "# Converting the date strings in cv_ranges to datetime objects\n",
    "datetime_cv_ranges = [(to_datetime(start), to_datetime(end)) for start, end in cv_ranges_corrected]\n",
    "datetime_cv_ranges\n",
    "\n",
    "date_filter = data_processor.df_all_cols.date[processed_df_no_na.index]\n",
    "date_filter\n",
    "\n",
    "cv1_train = processed_df_no_na[date_filter <= datetime_cv_ranges[0][0]]\n",
    "cv1_test = processed_df_no_na[(date_filter <= datetime_cv_ranges[0][1]) & (date_filter > datetime_cv_ranges[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in range(5):\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target']\n",
    "        drop_cols = ['target', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train rows: 1129738\n",
      "Val rows: 171264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 0: Train Mean Absolute Error: 20.30041571566987\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 0: Fold Val Mean Absolute Error: 43.17323074822631\n",
      "\n",
      "\n",
      "Fold 1\n",
      "Train rows: 1304266\n",
      "Val rows: 173328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 1: Train Mean Absolute Error: 21.231934992465064\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 1: Fold Val Mean Absolute Error: 35.31728936265493\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train rows: 1480810\n",
      "Val rows: 169632\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 2: Train Mean Absolute Error: 21.286468471214523\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 2: Fold Val Mean Absolute Error: 35.311975731019984\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train rows: 1653658\n",
      "Val rows: 167820\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 3: Train Mean Absolute Error: 21.42580551917969\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 3: Fold Val Mean Absolute Error: 54.433782348957436\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 4: Train Mean Absolute Error: 22.56845076635404\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 4: Fold Val Mean Absolute Error: 69.70795690892457\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    \"\"\"I am rewriting this training data processor to process a few more variables differently.\"\"\"\n",
    "\n",
    "    def __init__(self, train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices, for_testing=False,\n",
    "                add_log_cols=False):\n",
    "        self.add_log_cols = add_log_cols\n",
    "        self.test_orig_dfs = self.get_test_orig_dfs([train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices])\n",
    "        \n",
    "        self.weather_mapping = self.init_weather_mapping()\n",
    "        \n",
    "        if not for_testing:\n",
    "            self.train = self.init_train(train)\n",
    "            self.revealed_targets = self.init_revealed_targets(revealed_targets)\n",
    "            self.client = self.init_client(client)\n",
    "            \n",
    "            self.historical_weather = self.init_historical_weather(historical_weather)\n",
    "            self.forecast_weather = self.init_forecast_weather(forecast_weather)\n",
    "            self.electricity_prices = self.init_electricity(electricity_prices)\n",
    "            self.gas_prices = self.init_gas_prices(gas_prices)\n",
    "            \n",
    "            self.df_all_cols = self.join_data(self.train, self.revealed_targets, self.client, self.historical_weather, self.forecast_weather, self.electricity_prices, self.gas_prices)\n",
    "            if self.add_log_cols:\n",
    "                self.df_all_cols = self.create_log_cols(self.df_all_cols)\n",
    "            self.df = self.remove_cols(self.df_all_cols)\n",
    "            \n",
    "        \n",
    "    def get_test_orig_dfs(self, dfs):\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['prediction_datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'prediction_datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df.date).dt.date\n",
    "                col = 'date'\n",
    "\n",
    "            test_date = df[col].iloc[-1]  # Assuming test is a DataFrame\n",
    "            start_date = test_date - pd.Timedelta(days=14)\n",
    "            historical_subset = df[df[col] >= start_date]\n",
    "            dfs[i] = historical_subset.copy()\n",
    "        return dfs\n",
    "        \n",
    "    def init_train(self, df):\n",
    "        \"\"\"Prepares the training data for model training.\"\"\"\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        except Exception as e:\n",
    "            df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "        df['date'] = df.datetime.dt.date\n",
    "            \n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        return df\n",
    "    \n",
    "    def add_electricity_lag_features(self, df):\n",
    "        ##### mean from entire last week\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # Use rolling to calculate mean price of the last week\n",
    "        # The window is 7 days, min_periods can be set as per requirement\n",
    "        # 'closed' determines which side of the interval is closed; it can be 'right' or 'left'\n",
    "        df['mean_euros_per_mwh_last_week'] = df['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()\n",
    "        # Shift the results to align with the requirement of lagging\n",
    "        df['mean_euros_per_mwh_last_week'] = df['mean_euros_per_mwh_last_week'].shift()\n",
    "        \n",
    "        ##### mean from last week this hour only\n",
    "        # Extract hour from datetime\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        # Group by hour and apply rolling mean for each group\n",
    "        hourly_groups = df.groupby('hour')\n",
    "        dff = hourly_groups['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()#.shift()#.reset_index(level=0, drop=True)\n",
    "        dff = dff.reset_index().set_index('datetime').groupby('hour')['euros_per_mwh'].shift()\n",
    "        dff = dff.rename('mean_euros_per_mwh_same_hour_last_week')\n",
    "        df = df.join(dff)\n",
    "        #### yesterday's power price\n",
    "        df['yesterdays_euros_per_mwh'] = df['euros_per_mwh'].shift(24)\n",
    "        \n",
    "        ### 24h average\n",
    "        # Calculate the 24-hour rolling average\n",
    "        df['euros_per_mwh_24h_average_price'] = df['euros_per_mwh'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "        # Resetting the index if needed\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['forecast_date', 'origin_date', 'hour'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def init_electricity(self, df):\n",
    "        ## LAG = 1 Day\n",
    "        ## Move forecast datetime ahead by 1 day\n",
    "        ## change name to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_date'])\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        df = self.add_electricity_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_historical_weather_lag_features(self, df):\n",
    "        ##### LATEST WEATHER\n",
    "        def add_latest_weather(df):\n",
    "            # Assuming df is your original DataFrame\n",
    "            # Step 1: Convert datetime to a Datetime Object\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 2: Sorting the Data\n",
    "            df.sort_values(by=['datetime', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "            # Step 3: Creating a Unique Identifier for each location\n",
    "            df['location_id'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Step 4: Filtering for 10:00 AM Entries\n",
    "            df.reset_index(inplace=True)\n",
    "            df_10am = df[df['datetime'].dt.hour == 10]\n",
    "            df_10am.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 5: Shifting the Features by 1 day\n",
    "            lagged_features = df_10am.groupby('location_id').shift(periods=1, freq='D')\n",
    "            \n",
    "            # grouped = lagged_features.groupby('county')\n",
    "            # lagged_features = grouped[weather_features].mean()\n",
    "            \n",
    "            \n",
    "            # Renaming columns to indicate lag\n",
    "            lagged_features = lagged_features.add_suffix('_hw_lagged')\n",
    "            lagged_features['location_id'] = lagged_features['location_id_hw_lagged']\n",
    "            lagged_features.reset_index(inplace=True)\n",
    "            lagged_features['date'] = lagged_features.datetime.dt.date\n",
    "\n",
    "            df['date'] = df.datetime.dt.date\n",
    "            return lagged_features\n",
    "            # Step 6: Merging Lagged Features with Original DataFrame\n",
    "            df = df.merge(lagged_features, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "            return df\n",
    "        \n",
    "        ##### mean from last day\n",
    "        def add_24h_mean_var(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances'))\n",
    "\n",
    "            return my_df\n",
    "        \n",
    "        ##### mean from last day all estonia\n",
    "        def add_24h_mean_var_estonia(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means_estonia'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances_estonia'))\n",
    "\n",
    "            return my_df\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        weather_features = df.columns.drop(['datetime', 'latitude', 'longitude'])\n",
    "\n",
    "        # Apply the function\n",
    "        df = add_24h_mean_var(df, weather_features)    \n",
    "        df = add_24h_mean_var_estonia(df, weather_features)\n",
    "           \n",
    "        latest = add_latest_weather(df)\n",
    "        df = df.merge(latest, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_historical_weather(self, df):\n",
    "        ## LAG: From 11:00 AM 2 days ago to 10:00 AM 1 day ago\n",
    "        ## What to do? Give most recent weather forecast? Give average over the last day?\n",
    "        \"\"\"\n",
    "        Processes the historical weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df = self.add_historical_weather_lag_features(df)\n",
    "        \n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_forecast_weather(self, df):\n",
    "        ## LAG: DON't ADJUST\n",
    "        ##      The forecast is from yesterday, but can forecast today, which is 22 hours ahead\n",
    "        ## Drop any columns where:\n",
    "        ##                        hours_ahead < 22 and hours_ahead > 45\n",
    "        ## Then rename forecast_datetime to datetime and join on datetime\n",
    "        \"\"\"\n",
    "        Processes the forecast weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "        # keep only datetimes from our relevant period\n",
    "        df = df[(df['hours_ahead'] < 46) & (df['hours_ahead'] > 21)]\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "    \n",
    "    def add_gas_prices_lag_features(self, df):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by date, if it's not already sorted\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Calculate rolling averages for different time windows\n",
    "        df['lowest_price_3d_avg'] = df['lowest_price_per_mwh'].rolling(window=3).mean()\n",
    "        df['highest_price_3d_avg'] = df['highest_price_per_mwh'].rolling(window=3).mean()\n",
    "\n",
    "        df['lowest_price_7d_avg'] = df['lowest_price_per_mwh'].rolling(window=7).mean()\n",
    "        df['highest_price_7d_avg'] = df['highest_price_per_mwh'].rolling(window=7).mean()\n",
    "\n",
    "        df['lowest_price_14d_avg'] = df['lowest_price_per_mwh'].rolling(window=14).mean()\n",
    "        df['highest_price_14d_avg'] = df['highest_price_per_mwh'].rolling(window=14).mean()\n",
    "\n",
    "        # Reset the index if you want the 'date' column back\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def init_gas_prices(self, df):\n",
    "        ## LAG: 1 DAY\n",
    "        ## Predictions are made from 2 days ago and predict for yesterday\n",
    "        ## add one day to forecast_date\n",
    "        ## Rename forecast_date to date, join on date\n",
    "        \"\"\"\n",
    "        Processes the gas prices data.\n",
    "        Implement the logic to handle gas prices data processing here.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['forecast_date']).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=1)\n",
    "        df = self.add_gas_prices_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_revealed_target_features(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['hour'] = df.datetime.dt.hour\n",
    "        df['day'] = df.datetime.dt.dayofweek\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        # let me add some new features here too\n",
    "        # Adding lag features\n",
    "        # Step 2: Sorting the Data\n",
    "        df.sort_values(by=['datetime'], inplace=True)\n",
    "\n",
    "        # Step 3: Creating a Unique Identifier for each location\n",
    "        df['id'] = df['county'].astype(str) + '_' + df['is_business'].astype(str) + '_' + df['product_type'].astype(str) + '_' + df['is_consumption'].astype(str)\n",
    "        lagged_features = []\n",
    "        lagged_hours = []\n",
    "        ### Defining lagged target features\n",
    "\n",
    "        for lag_hours in range(1, 24):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "\n",
    "        for lag_hours in ([i*24 for i in range(1,8)] + [24*11, 24*12]):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "            \n",
    "        df.reset_index(inplace=True)\n",
    "        for lagged_feature, lag_hours in zip(lagged_features, lagged_hours):\n",
    "            lagged_feature.reset_index(inplace=True)\n",
    "            lagged_feature.dropna(inplace=True)\n",
    "            df = df.merge(lagged_feature[['datetime', 'target', 'id']], on=['id', 'datetime'], how='left', suffixes=('', f'_lag_{lag_hours}h'))\n",
    "\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        window_size = 7\n",
    "        # Group by the specified columns and then apply the rolling mean\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_allp_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        #All of estonia\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_allp_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        df = df.drop(['hour', 'day'], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def init_revealed_targets(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=2)\n",
    "        df = self.add_revealed_target_features(df)\n",
    "        return df\n",
    "    \n",
    "    def init_client(self, df):\n",
    "        ## LAG: 2 days\n",
    "        ## Add 2 days to date, join on date\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=2)\n",
    "        # df = self.get_data_block_id(df, 'date')\n",
    "        return df\n",
    "\n",
    "    def init_weather_mapping(self):\n",
    "        # https://www.kaggle.com/code/tsunotsuno/enefit-eda-baseline/notebook#Baseline\n",
    "        county_point_map = {\n",
    "            0: (59.4, 24.7), # \"HARJUMAA\"\n",
    "            1 : (58.8, 22.7), # \"HIIUMAA\"\n",
    "            2 : (59.1, 27.2), # \"IDA-VIRUMAA\"\n",
    "            3 : (58.8, 25.7), # \"JÄRVAMAA\"\n",
    "            4 : (58.8, 26.2), # \"JÕGEVAMAA\"\n",
    "            5 : (59.1, 23.7), # \"LÄÄNE-VIRUMAA\"\n",
    "            6 : (59.1, 23.7), # \"LÄÄNEMAA\"\n",
    "            7 : (58.5, 24.7), # \"PÄRNUMAA\"\n",
    "            8 : (58.2, 27.2), # \"PÕLVAMAA\"\n",
    "            9 : (58.8, 24.7), # \"RAPLAMAA\"\n",
    "            10 : (58.5, 22.7),# \"SAAREMAA\"\n",
    "            11 : (58.5, 26.7),# \"TARTUMAA\"\n",
    "            12 : (58.5, 25.2),# \"UNKNOWNN\" (center of the map)\n",
    "            13 : (57.9, 26.2),# \"VALGAMAA\"\n",
    "            14 : (58.2, 25.7),# \"VILJANDIMAA\"\n",
    "            15 : (57.9, 27.2) # \"VÕRUMAA\"\n",
    "        }\n",
    "        # Convert the dictionary to a list of tuples\n",
    "        data = [(county_code, lat, lon) for county_code, (lat, lon) in county_point_map.items()]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=['county', 'latitude', 'longitude'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        df['day_of_week'] = df['datetime'].dt.day_of_week\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        df['is_weekend'] = df['datetime'].dt.day_of_week >= 5\n",
    "        df['is_month_start'] = df['datetime'].dt.is_month_start\n",
    "        df['is_month_end'] = df['datetime'].dt.is_month_end\n",
    "        df['is_quarter_start'] = df['datetime'].dt.is_quarter_start\n",
    "        df['is_quarter_end'] = df['datetime'].dt.is_quarter_end\n",
    "        df['is_year_start'] = df['datetime'].dt.is_year_start\n",
    "        df['is_year_end'] = df['datetime'].dt.is_year_end\n",
    "        df['season'] = df['datetime'].dt.month % 12 // 3 + 1\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        # Calculate sin and cos for day of year\n",
    "        days_in_year = 365.25  # accounts for leap year\n",
    "        df['day_of_year_sin'] = np.sin((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        df['day_of_year_cos'] = np.cos((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        return df\n",
    "    \n",
    "    def add_ee_holidays(self, df):\n",
    "        import holidays\n",
    "        # Define Estonia public holidays\n",
    "        ee_holidays = holidays.CountryHoliday('EE')\n",
    "        \n",
    "        print(df['date'].isna().sum())\n",
    "        \n",
    "        def find_problem(x):\n",
    "            try:\n",
    "                return x in ee_holidays\n",
    "            except Exception as e:\n",
    "                print(x)\n",
    "                raise e\n",
    "\n",
    "        # Function to check if the date is a holiday\n",
    "        df['is_ee_holiday'] = df['date'].apply(lambda x: x in ee_holidays)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def create_log_cols(self, df):\n",
    "        log_cols = ['target_lag_1h', 'target_lag_2h', 'target_lag_3h', 'target_lag_4h',\n",
    "       'target_lag_5h', 'target_lag_6h', 'target_lag_7h', 'target_lag_8h',\n",
    "       'target_lag_9h', 'target_lag_10h', 'target_lag_11h', 'target_lag_12h',\n",
    "       'target_lag_13h', 'target_lag_14h', 'target_lag_15h', 'target_lag_16h',\n",
    "       'target_lag_17h', 'target_lag_18h', 'target_lag_19h', 'target_lag_20h',\n",
    "       'target_lag_21h', 'target_lag_22h', 'target_lag_23h', 'target_lag_24h',\n",
    "       'target_lag_48h', 'target_lag_72h', 'target_lag_96h', 'target_lag_120h',\n",
    "       'target_lag_144h', 'target_lag_168h', 'target_lag_264h',\n",
    "       'target_lag_288h', 'eic_count', 'installed_capacity', 'temperature', 'dewpoint', 'rain',\n",
    "       'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n",
    "       'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n",
    "       'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n",
    "       'diffuse_radiation', 'temperature_hw_means', 'dewpoint_hw_means',\n",
    "       'rain_hw_means', 'snowfall_hw_means', 'surface_pressure_hw_means',\n",
    "       'cloudcover_total_hw_means', 'cloudcover_low_hw_means',\n",
    "       'cloudcover_mid_hw_means', 'cloudcover_high_hw_means',\n",
    "       'windspeed_10m_hw_means', 'winddirection_10m_hw_means',\n",
    "       'shortwave_radiation_hw_means', 'direct_solar_radiation_hw_means',\n",
    "       'diffuse_radiation_hw_means', 'temperature_hw_variances',\n",
    "       'dewpoint_hw_variances', 'rain_hw_variances', 'snowfall_hw_variances',\n",
    "       'surface_pressure_hw_variances', 'cloudcover_total_hw_variances',\n",
    "       'cloudcover_low_hw_variances', 'cloudcover_mid_hw_variances',\n",
    "       'cloudcover_high_hw_variances', 'windspeed_10m_hw_variances',\n",
    "       'winddirection_10m_hw_variances', 'shortwave_radiation_hw_variances',\n",
    "       'direct_solar_radiation_hw_variances', 'diffuse_radiation_hw_variances',\n",
    "       'temperature_hw_lagged', 'dewpoint_hw_lagged', 'rain_hw_lagged',\n",
    "       'snowfall_hw_lagged', 'surface_pressure_hw_lagged',\n",
    "       'cloudcover_total_hw_lagged', 'cloudcover_low_hw_lagged', 'cloudcover_mid_hw_lagged',\n",
    "       'cloudcover_high_hw_lagged', 'windspeed_10m_hw_lagged',\n",
    "       'winddirection_10m_hw_lagged', 'shortwave_radiation_hw_lagged',\n",
    "       'direct_solar_radiation_hw_lagged', 'diffuse_radiation_hw_lagged',\n",
    "       'temperature_hw_means_hw_lagged', 'dewpoint_hw_means_hw_lagged',\n",
    "       'rain_hw_means_hw_lagged', 'snowfall_hw_means_hw_lagged',\n",
    "       'surface_pressure_hw_means_hw_lagged',\n",
    "       'cloudcover_total_hw_means_hw_lagged',\n",
    "       'cloudcover_low_hw_means_hw_lagged',\n",
    "       'cloudcover_mid_hw_means_hw_lagged',\n",
    "       'cloudcover_high_hw_means_hw_lagged',\n",
    "       'windspeed_10m_hw_means_hw_lagged',\n",
    "       'winddirection_10m_hw_means_hw_lagged',\n",
    "       'shortwave_radiation_hw_means_hw_lagged',\n",
    "       'direct_solar_radiation_hw_means_hw_lagged',\n",
    "       'diffuse_radiation_hw_means_hw_lagged',\n",
    "       'temperature_hw_variances_hw_lagged', 'dewpoint_hw_variances_hw_lagged',\n",
    "       'rain_hw_variances_hw_lagged', 'snowfall_hw_variances_hw_lagged',\n",
    "       'surface_pressure_hw_variances_hw_lagged',\n",
    "       'cloudcover_total_hw_variances_hw_lagged',\n",
    "       'cloudcover_low_hw_variances_hw_lagged',\n",
    "       'cloudcover_mid_hw_variances_hw_lagged',\n",
    "       'cloudcover_high_hw_variances_hw_lagged',\n",
    "       'windspeed_10m_hw_variances_hw_lagged',\n",
    "       'winddirection_10m_hw_variances_hw_lagged',\n",
    "       'shortwave_radiation_hw_variances_hw_lagged',\n",
    "       'direct_solar_radiation_hw_variances_hw_lagged',\n",
    "       'diffuse_radiation_hw_variances_hw_lagged', 'temperature_fw', 'dewpoint_fw', 'cloudcover_high_fw',\n",
    "       'cloudcover_low_fw', 'cloudcover_mid_fw', 'cloudcover_total_fw',\n",
    "       '10_metre_u_wind_component', '10_metre_v_wind_component',\n",
    "       'direct_solar_radiation_fw', 'surface_solar_radiation_downwards',\n",
    "       'snowfall_fw', 'total_precipitation', 'euros_per_mwh', 'mean_euros_per_mwh_last_week',\n",
    "       'mean_euros_per_mwh_same_hour_last_week', 'yesterdays_euros_per_mwh',\n",
    "       'euros_per_mwh_24h_average_price', 'lowest_price_per_mwh',\n",
    "       'highest_price_per_mwh', 'lowest_price_3d_avg', 'highest_price_3d_avg',\n",
    "       'lowest_price_7d_avg', 'highest_price_7d_avg', 'lowest_price_14d_avg',\n",
    "       'highest_price_14d_avg']\n",
    "        \n",
    "        log_cols = [col for col in log_cols if col in df.columns]\n",
    "        \n",
    "        dff = np.log1p(df[log_cols] )\n",
    "        dff.rename(columns={col: col + \"_log\" for col in log_cols}, inplace=True)\n",
    "        return pd.concat([df, dff], axis=1)\n",
    "        \n",
    "    \n",
    "    def remove_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'row_id',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                    'id',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_test_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                    'id'\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def join_data(self, train, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices):\n",
    "        df = train\n",
    "        df = df.merge(revealed_targets, how='left', on=('datetime', 'county', 'is_business', 'product_type', 'is_consumption'), suffixes=('', '_rt'))\n",
    "        df = df.merge(client, how='left', on=('date', 'county', 'is_business', 'product_type'), suffixes=('', '_client'))\n",
    "        df = df.merge(historical_weather, how='left', on=('datetime', 'county'), suffixes=('', '_hw'))\n",
    "        df = df.merge(forecast_weather, how='left', on=('datetime', 'county'), suffixes=('', '_fw'))\n",
    "        df = df.merge(electricity_prices, how='left', on='datetime', suffixes=('', '_elec'))\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.merge(gas_prices, how='left', on='date', suffixes=('', '_gasp'))\n",
    "        df = self.add_date_features(df)\n",
    "        df = self.add_ee_holidays(df)\n",
    "        return df\n",
    "    \n",
    "    def add_test_data(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        dfs = [test.copy(), revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices]\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "                \n",
    "            self.test_orig_dfs[i] = pd.concat([ self.test_orig_dfs[i], df ])          \n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_test_data_timestep(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        #append test data to test data cache\n",
    "        self.add_test_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        # process test data\n",
    "        test = self.init_train(self.test_orig_dfs[0])\n",
    "        revealed_targets = self.init_revealed_targets(self.test_orig_dfs[1])\n",
    "        client = self.init_client(self.test_orig_dfs[2])\n",
    "        historical_weather = self.init_historical_weather(self.test_orig_dfs[3])\n",
    "        forecast_weather = self.init_forecast_weather(self.test_orig_dfs[4])\n",
    "        electricity_prices = self.init_electricity(self.test_orig_dfs[5])\n",
    "        gas_prices = self.init_gas_prices(self.test_orig_dfs[6])\n",
    "        df_all_cols = self.join_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        if self.add_log_cols:\n",
    "            df_all_cols = self.create_log_cols(df_all_cols)\n",
    "        df = self.remove_test_cols(df_all_cols)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>is_business</th>\n",
       "      <th>product_type</th>\n",
       "      <th>target</th>\n",
       "      <th>is_consumption</th>\n",
       "      <th>target_rt</th>\n",
       "      <th>target_lag_1h</th>\n",
       "      <th>target_lag_2h</th>\n",
       "      <th>target_lag_3h</th>\n",
       "      <th>target_lag_4h</th>\n",
       "      <th>...</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>season</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "      <th>is_ee_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>96.590</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.314</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018609</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197.233</td>\n",
       "      <td>1</td>\n",
       "      <td>184.072</td>\n",
       "      <td>171.092</td>\n",
       "      <td>168.933</td>\n",
       "      <td>174.920</td>\n",
       "      <td>170.068</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018610</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.501</td>\n",
       "      <td>25.884</td>\n",
       "      <td>83.535</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018611</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.404</td>\n",
       "      <td>1</td>\n",
       "      <td>38.646</td>\n",
       "      <td>47.690</td>\n",
       "      <td>34.806</td>\n",
       "      <td>29.202</td>\n",
       "      <td>21.654</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018612</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.512</td>\n",
       "      <td>34.657</td>\n",
       "      <td>122.195</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018613</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>196.240</td>\n",
       "      <td>1</td>\n",
       "      <td>183.756</td>\n",
       "      <td>190.316</td>\n",
       "      <td>172.973</td>\n",
       "      <td>141.646</td>\n",
       "      <td>93.817</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2018614 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         county  is_business  product_type   target  is_consumption  \\\n",
       "0             0            0             1    0.713               0   \n",
       "1             0            0             1   96.590               1   \n",
       "2             0            0             2    0.000               0   \n",
       "3             0            0             2   17.314               1   \n",
       "4             0            0             3    2.904               0   \n",
       "...         ...          ...           ...      ...             ...   \n",
       "2018609      15            1             0  197.233               1   \n",
       "2018610      15            1             1    0.000               0   \n",
       "2018611      15            1             1   28.404               1   \n",
       "2018612      15            1             3    0.000               0   \n",
       "2018613      15            1             3  196.240               1   \n",
       "\n",
       "         target_rt  target_lag_1h  target_lag_2h  target_lag_3h  \\\n",
       "0              NaN            NaN            NaN            NaN   \n",
       "1              NaN            NaN            NaN            NaN   \n",
       "2              NaN            NaN            NaN            NaN   \n",
       "3              NaN            NaN            NaN            NaN   \n",
       "4              NaN            NaN            NaN            NaN   \n",
       "...            ...            ...            ...            ...   \n",
       "2018609    184.072        171.092        168.933        174.920   \n",
       "2018610      0.000          0.000          2.501         25.884   \n",
       "2018611     38.646         47.690         34.806         29.202   \n",
       "2018612      0.000          0.000          4.512         34.657   \n",
       "2018613    183.756        190.316        172.973        141.646   \n",
       "\n",
       "         target_lag_4h  ...  is_quarter_start  is_quarter_end  is_year_start  \\\n",
       "0                  NaN  ...             False           False          False   \n",
       "1                  NaN  ...             False           False          False   \n",
       "2                  NaN  ...             False           False          False   \n",
       "3                  NaN  ...             False           False          False   \n",
       "4                  NaN  ...             False           False          False   \n",
       "...                ...  ...               ...             ...            ...   \n",
       "2018609        170.068  ...             False           False          False   \n",
       "2018610         83.535  ...             False           False          False   \n",
       "2018611         21.654  ...             False           False          False   \n",
       "2018612        122.195  ...             False           False          False   \n",
       "2018613         93.817  ...             False           False          False   \n",
       "\n",
       "         is_year_end  season  hour_sin  hour_cos  day_of_year_sin  \\\n",
       "0              False       4  0.000000  1.000000        -0.861693   \n",
       "1              False       4  0.000000  1.000000        -0.861693   \n",
       "2              False       4  0.000000  1.000000        -0.861693   \n",
       "3              False       4  0.000000  1.000000        -0.861693   \n",
       "4              False       4  0.000000  1.000000        -0.861693   \n",
       "...              ...     ...       ...       ...              ...   \n",
       "2018609        False       2 -0.258819  0.965926         0.532227   \n",
       "2018610        False       2 -0.258819  0.965926         0.532227   \n",
       "2018611        False       2 -0.258819  0.965926         0.532227   \n",
       "2018612        False       2 -0.258819  0.965926         0.532227   \n",
       "2018613        False       2 -0.258819  0.965926         0.532227   \n",
       "\n",
       "         day_of_year_cos  is_ee_holiday  \n",
       "0              -0.507430          False  \n",
       "1              -0.507430          False  \n",
       "2              -0.507430          False  \n",
       "3              -0.507430          False  \n",
       "4              -0.507430          False  \n",
       "...                  ...            ...  \n",
       "2018609        -0.846602          False  \n",
       "2018610        -0.846602          False  \n",
       "2018611        -0.846602          False  \n",
       "2018612        -0.846602          False  \n",
       "2018613        -0.846602          False  \n",
       "\n",
       "[2018614 rows x 240 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_processor_lgbm2_new_pandas.pkl', 'rb') as f:\n",
    "    data_processor = pickle.load(f)\n",
    "data_processor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.12 s\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "county             0\n",
       "is_business        0\n",
       "product_type       0\n",
       "target             0\n",
       "is_consumption     0\n",
       "                  ..\n",
       "hour_sin           0\n",
       "hour_cos           0\n",
       "day_of_year_sin    0\n",
       "day_of_year_cos    0\n",
       "is_ee_holiday      0\n",
       "Length: 240, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "processed_df_no_na, means = fill_drop_na(data_processor.df)\n",
    "processed_df_no_na.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the date strings in cv_ranges to datetime objects\n",
    "datetime_cv_ranges = [(to_datetime(start), to_datetime(end)) for start, end in cv_ranges_corrected]\n",
    "datetime_cv_ranges\n",
    "\n",
    "date_filter = data_processor.df_all_cols.date[processed_df_no_na.index]\n",
    "date_filter\n",
    "\n",
    "cv1_train = processed_df_no_na[date_filter <= datetime_cv_ranges[0][0]]\n",
    "cv1_test = processed_df_no_na[(date_filter <= datetime_cv_ranges[0][1]) & (date_filter > datetime_cv_ranges[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in range(5):\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target']\n",
    "        drop_cols = ['target', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train rows: 1129738\n",
      "Val rows: 171264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 0: Train Mean Absolute Error: 19.841400375564813\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 0: Fold Val Mean Absolute Error: 41.440058510050505\n",
      "\n",
      "\n",
      "Fold 1\n",
      "Train rows: 1304266\n",
      "Val rows: 173328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 1: Train Mean Absolute Error: 20.79814905560108\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 1: Fold Val Mean Absolute Error: 34.717542033906\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train rows: 1480810\n",
      "Val rows: 169632\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 2: Train Mean Absolute Error: 20.95745854913704\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 2: Fold Val Mean Absolute Error: 34.21822101887113\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train rows: 1653658\n",
      "Val rows: 167820\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 3: Train Mean Absolute Error: 20.995168523112337\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 3: Fold Val Mean Absolute Error: 50.78812879489522\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 4: Train Mean Absolute Error: 22.026433544152685\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 4: Fold Val Mean Absolute Error: 67.38630479764562\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in [4]:\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target']\n",
    "        drop_cols = ['target', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 1.9178044505870369, 'lambda_l2': 1.3167397171985806, 'learning_rate': 0.12287415335595941, 'max_bin': 161, 'min_data_in_leaf': 20, 'n_estimators': 5663,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=8888, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_l1': 1.9178044505870369,\n",
       " 'lambda_l2': 1.3167397171985806,\n",
       " 'learning_rate': 0.12287415335595941,\n",
       " 'max_bin': 161,\n",
       " 'min_data_in_leaf': 20,\n",
       " 'n_estimators': 5663}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'lambda_l1': 1.9178044505870369, 'lambda_l2': 1.3167397171985806, 'learning_rate': 0.12287415335595941, 'max_bin': 161, 'min_data_in_leaf': 20, 'n_estimators': 5663}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9178044505870369, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9178044505870369\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3167397171985806, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3167397171985806\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9178044505870369, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9178044505870369\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3167397171985806, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3167397171985806\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9178044505870369, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9178044505870369\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3167397171985806, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3167397171985806\n",
      "For fold 4: Train Mean Absolute Error: 23.043489207900983\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=20, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=20\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.9178044505870369, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.9178044505870369\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.3167397171985806, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.3167397171985806\n",
      "For fold 4: Fold Val Mean Absolute Error: 70.02922482632512\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in [4]:\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target']\n",
    "        drop_cols = ['target', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 1.6305167320916096, 'lambda_l2': 0.6463358547872429, 'learning_rate': 0.3718161124401851, 'max_bin': 140, 'min_data_in_leaf': 39, 'n_estimators': 2869,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=8888, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda_l1': 1.9178044505870369,\n",
       " 'lambda_l2': 1.3167397171985806,\n",
       " 'learning_rate': 0.12287415335595941,\n",
       " 'max_bin': 161,\n",
       " 'min_data_in_leaf': 20,\n",
       " 'n_estimators': 5663}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'lambda_l1': 1.9178044505870369, 'lambda_l2': 1.3167397171985806, 'learning_rate': 0.12287415335595941, 'max_bin': 161, 'min_data_in_leaf': 20, 'n_estimators': 5663}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=39, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=39\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6305167320916096, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6305167320916096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6463358547872429, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6463358547872429\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=39, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=39\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6305167320916096, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6305167320916096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6463358547872429, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6463358547872429\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=39, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=39\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6305167320916096, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6305167320916096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6463358547872429, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6463358547872429\n",
      "For fold 4: Train Mean Absolute Error: 20.73250133201846\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=39, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=39\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.6305167320916096, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.6305167320916096\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6463358547872429, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6463358547872429\n",
      "For fold 4: Fold Val Mean Absolute Error: 74.00838948687351\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "class TrainDataProcessor:\n",
    "    \"\"\"I am rewriting this training data processor to process a few more variables differently.\"\"\"\n",
    "\n",
    "    def __init__(self, train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices, for_testing=False,\n",
    "                add_log_cols=False):\n",
    "        self.add_log_cols = add_log_cols\n",
    "        self.test_orig_dfs = self.get_test_orig_dfs([train, revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices])\n",
    "        \n",
    "        self.weather_mapping = self.init_weather_mapping()\n",
    "        \n",
    "        if not for_testing:\n",
    "            self.train = self.init_train(train)\n",
    "            self.revealed_targets = self.init_revealed_targets(revealed_targets)\n",
    "            self.client = self.init_client(client)\n",
    "            \n",
    "            self.historical_weather = self.init_historical_weather(historical_weather)\n",
    "            self.forecast_weather = self.init_forecast_weather(forecast_weather)\n",
    "            self.electricity_prices = self.init_electricity(electricity_prices)\n",
    "            self.gas_prices = self.init_gas_prices(gas_prices)\n",
    "            \n",
    "            self.df_all_cols = self.join_data(self.train, self.revealed_targets, self.client, self.historical_weather, self.forecast_weather, self.electricity_prices, self.gas_prices)\n",
    "            if self.add_log_cols:\n",
    "                self.df_all_cols = self.create_log_cols(self.df_all_cols)\n",
    "            self.df = self.remove_cols(self.df_all_cols)\n",
    "            \n",
    "        \n",
    "    def get_test_orig_dfs(self, dfs):\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['prediction_datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'prediction_datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df.date).dt.date\n",
    "                col = 'date'\n",
    "\n",
    "            test_date = df[col].iloc[-1]  # Assuming test is a DataFrame\n",
    "            start_date = test_date - pd.Timedelta(days=14)\n",
    "            historical_subset = df[df[col] >= start_date]\n",
    "            dfs[i] = historical_subset.copy()\n",
    "        return dfs\n",
    "        \n",
    "    def init_train(self, df):\n",
    "        \"\"\"Prepares the training data for model training.\"\"\"\n",
    "        try:\n",
    "            df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        except Exception as e:\n",
    "            df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "        df['date'] = df.datetime.dt.date\n",
    "            \n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        return df\n",
    "    \n",
    "    def add_electricity_lag_features(self, df):\n",
    "        ##### mean from entire last week\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        # Use rolling to calculate mean price of the last week\n",
    "        # The window is 7 days, min_periods can be set as per requirement\n",
    "        # 'closed' determines which side of the interval is closed; it can be 'right' or 'left'\n",
    "        df['mean_euros_per_mwh_last_week'] = df['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()\n",
    "        # Shift the results to align with the requirement of lagging\n",
    "        df['mean_euros_per_mwh_last_week'] = df['mean_euros_per_mwh_last_week'].shift()\n",
    "        \n",
    "        ##### mean from last week this hour only\n",
    "        # Extract hour from datetime\n",
    "        df['hour'] = df.index.hour\n",
    "\n",
    "        # Group by hour and apply rolling mean for each group\n",
    "        hourly_groups = df.groupby('hour')\n",
    "        dff = hourly_groups['euros_per_mwh'].rolling(window='7D', min_periods=1, closed='right').mean()#.shift()#.reset_index(level=0, drop=True)\n",
    "        dff = dff.reset_index().set_index('datetime').groupby('hour')['euros_per_mwh'].shift()\n",
    "        dff = dff.rename('mean_euros_per_mwh_same_hour_last_week')\n",
    "        df = df.join(dff)\n",
    "        #### yesterday's power price\n",
    "        df['yesterdays_euros_per_mwh'] = df['euros_per_mwh'].shift(24)\n",
    "        \n",
    "        ### 24h average\n",
    "        # Calculate the 24-hour rolling average\n",
    "        df['euros_per_mwh_24h_average_price'] = df['euros_per_mwh'].rolling(window=24, min_periods=1).mean()\n",
    "\n",
    "        # Resetting the index if needed\n",
    "        df.reset_index(inplace=True)\n",
    "        df = df.drop(['forecast_date', 'origin_date', 'hour'], axis=1)\n",
    "        return df\n",
    "\n",
    "    def init_electricity(self, df):\n",
    "        ## LAG = 1 Day\n",
    "        ## Move forecast datetime ahead by 1 day\n",
    "        ## change name to datetime\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_date'])\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        # df = self.get_data_block_id(df, 'datetime')\n",
    "        df = self.add_electricity_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_historical_weather_lag_features(self, df):\n",
    "        ##### LATEST WEATHER\n",
    "        def add_latest_weather(df):\n",
    "            # Assuming df is your original DataFrame\n",
    "            # Step 1: Convert datetime to a Datetime Object\n",
    "            df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            df.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 2: Sorting the Data\n",
    "            df.sort_values(by=['datetime', 'latitude', 'longitude'], inplace=True)\n",
    "\n",
    "            # Step 3: Creating a Unique Identifier for each location\n",
    "            df['location_id'] = df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Step 4: Filtering for 10:00 AM Entries\n",
    "            df.reset_index(inplace=True)\n",
    "            df_10am = df[df['datetime'].dt.hour == 10]\n",
    "            df_10am.set_index('datetime', inplace=True)\n",
    "\n",
    "            # Step 5: Shifting the Features by 1 day\n",
    "            lagged_features = df_10am.groupby('location_id').shift(periods=1, freq='D')\n",
    "            \n",
    "            # grouped = lagged_features.groupby('county')\n",
    "            # lagged_features = grouped[weather_features].mean()\n",
    "            \n",
    "            \n",
    "            # Renaming columns to indicate lag\n",
    "            lagged_features = lagged_features.add_suffix('_hw_lagged')\n",
    "            lagged_features['location_id'] = lagged_features['location_id_hw_lagged']\n",
    "            lagged_features.reset_index(inplace=True)\n",
    "            lagged_features['date'] = lagged_features.datetime.dt.date\n",
    "\n",
    "            df['date'] = df.datetime.dt.date\n",
    "            return lagged_features\n",
    "            # Step 6: Merging Lagged Features with Original DataFrame\n",
    "            df = df.merge(lagged_features, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "            return df\n",
    "        \n",
    "        ##### mean from last day\n",
    "        def add_24h_mean_var(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances'))\n",
    "\n",
    "            return my_df\n",
    "        \n",
    "        ##### mean from last day all estonia\n",
    "        def add_24h_mean_var_estonia(df, weather_features):\n",
    "            # Calculate the start and end times for each row\n",
    "            # df['start_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=2) + pd.Timedelta(hours=11)\n",
    "            # df['end_time'] = pd.to_datetime(df['datetime'].dt.date) - pd.Timedelta(days=1) + pd.Timedelta(hours=10)\n",
    "            # df['time_code'] = df['start_time'].astype(str) +'_' + df['end_time'].astype(str)\n",
    "            # print(df.time_code)\n",
    "\n",
    "            # Create a helper column for grouping\n",
    "            # If the time is before 11:00 AM, subtract a day\n",
    "            df['group'] = df['datetime'].apply(lambda dt: dt if dt.time() >= pd.to_datetime('11:00').time() else dt - pd.Timedelta(days=1))\n",
    "            df['group'] = df['group'].dt.date  # Keep only the date part for grouping\n",
    "            df['group'] = (pd.to_datetime(df['group']) + pd.Timedelta(hours=11)).astype(str) + '_' + (pd.to_datetime(df['group']) + pd.Timedelta(days=1, hours=10)).astype(str) + '_' + df['latitude'].astype(str) + '_' + df['longitude'].astype(str)\n",
    "\n",
    "            # Now group by this new column\n",
    "            grouped = df.groupby('group')\n",
    "            means = grouped[weather_features].mean()\n",
    "            variances = grouped[weather_features].var()\n",
    "\n",
    "            # Merge means and variances into the original DataFrame\n",
    "            my_df = df.merge(means, on='group', suffixes=('', '_hw_means_estonia'), how='left')\n",
    "            my_df = my_df.merge(variances, on='group', how='left', suffixes=('', '_hw_variances_estonia'))\n",
    "\n",
    "            return my_df\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        weather_features = df.columns.drop(['datetime', 'latitude', 'longitude'])\n",
    "\n",
    "        # Apply the function\n",
    "        df = add_24h_mean_var(df, weather_features)    \n",
    "        df = add_24h_mean_var_estonia(df, weather_features)\n",
    "           \n",
    "        latest = add_latest_weather(df)\n",
    "        df = df.merge(latest, on=['date', 'location_id'], how='left', suffixes=('', '_hw_lagged'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_historical_weather(self, df):\n",
    "        ## LAG: From 11:00 AM 2 days ago to 10:00 AM 1 day ago\n",
    "        ## What to do? Give most recent weather forecast? Give average over the last day?\n",
    "        \"\"\"\n",
    "        Processes the historical weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df = self.add_historical_weather_lag_features(df)\n",
    "        \n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def init_forecast_weather(self, df):\n",
    "        ## LAG: DON't ADJUST\n",
    "        ##      The forecast is from yesterday, but can forecast today, which is 22 hours ahead\n",
    "        ## Drop any columns where:\n",
    "        ##                        hours_ahead < 22 and hours_ahead > 45\n",
    "        ## Then rename forecast_datetime to datetime and join on datetime\n",
    "        \"\"\"\n",
    "        Processes the forecast weather data.\n",
    "        \"\"\"\n",
    "        df['datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "        # keep only datetimes from our relevant period\n",
    "        df = df[(df['hours_ahead'] < 46) & (df['hours_ahead'] > 21)]\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=1)\n",
    "        df = df.merge(self.weather_mapping, how='inner', on=('latitude', 'longitude'))\n",
    "        return df\n",
    "    \n",
    "    def add_gas_prices_lag_features(self, df):\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        # Sort the DataFrame by date, if it's not already sorted\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Calculate rolling averages for different time windows\n",
    "        df['lowest_price_3d_avg'] = df['lowest_price_per_mwh'].rolling(window=3).mean()\n",
    "        df['highest_price_3d_avg'] = df['highest_price_per_mwh'].rolling(window=3).mean()\n",
    "\n",
    "        df['lowest_price_7d_avg'] = df['lowest_price_per_mwh'].rolling(window=7).mean()\n",
    "        df['highest_price_7d_avg'] = df['highest_price_per_mwh'].rolling(window=7).mean()\n",
    "\n",
    "        df['lowest_price_14d_avg'] = df['lowest_price_per_mwh'].rolling(window=14).mean()\n",
    "        df['highest_price_14d_avg'] = df['highest_price_per_mwh'].rolling(window=14).mean()\n",
    "\n",
    "        # Reset the index if you want the 'date' column back\n",
    "        df.reset_index(inplace=True)\n",
    "        return df\n",
    "\n",
    "    def init_gas_prices(self, df):\n",
    "        ## LAG: 1 DAY\n",
    "        ## Predictions are made from 2 days ago and predict for yesterday\n",
    "        ## add one day to forecast_date\n",
    "        ## Rename forecast_date to date, join on date\n",
    "        \"\"\"\n",
    "        Processes the gas prices data.\n",
    "        Implement the logic to handle gas prices data processing here.\n",
    "        \"\"\"\n",
    "        df['date'] = pd.to_datetime(df['forecast_date']).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=1)\n",
    "        df = self.add_gas_prices_lag_features(df)\n",
    "        return df\n",
    "    \n",
    "    def add_revealed_target_features(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df['hour'] = df.datetime.dt.hour\n",
    "        df['day'] = df.datetime.dt.dayofweek\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        # let me add some new features here too\n",
    "        # Adding lag features\n",
    "        # Step 2: Sorting the Data\n",
    "        df.sort_values(by=['datetime'], inplace=True)\n",
    "\n",
    "        # Step 3: Creating a Unique Identifier for each location\n",
    "        df['id'] = df['county'].astype(str) + '_' + df['is_business'].astype(str) + '_' + df['product_type'].astype(str) + '_' + df['is_consumption'].astype(str)\n",
    "        lagged_features = []\n",
    "        lagged_hours = []\n",
    "        ### Defining lagged target features\n",
    "\n",
    "        for lag_hours in range(1, 24):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "\n",
    "        for lag_hours in ([i*24 for i in range(1,8)] + [24*11, 24*12]):\n",
    "            lagged_feature = df.groupby('id').shift(periods=lag_hours, freq='H')\n",
    "            lagged_features.append(lagged_feature)\n",
    "            lagged_hours.append(lag_hours)\n",
    "            \n",
    "        df.reset_index(inplace=True)\n",
    "        for lagged_feature, lag_hours in zip(lagged_features, lagged_hours):\n",
    "            lagged_feature.reset_index(inplace=True)\n",
    "            lagged_feature.dropna(inplace=True)\n",
    "            df = df.merge(lagged_feature[['datetime', 'target', 'id']], on=['id', 'datetime'], how='left', suffixes=('', f'_lag_{lag_hours}h'))\n",
    "\n",
    "        # add exponentially weighted mean features\n",
    "        alphas = [0.95, 0.9, 0.8, 0.7, 0.5]\n",
    "        lags   = [1,2,3] + [i*24 for i in range(1,8)] + [24*11, 24*12]\n",
    "        for alpha in alphas:\n",
    "            print(f'starting alpha: {alpha}')\n",
    "            for lag in lags:\n",
    "                print(f'starting lag: {lag}')\n",
    "                df['target_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = df.groupby('id')['target'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n",
    "\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        window_size = 7\n",
    "        # Group by the specified columns and then apply the rolling mean\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['county', 'is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['county', 'is_business', 'is_consumption', 'hour', 'day'])\n",
    "        df['target_rolling_allp_avg_hour_hour_day_4w'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        #All of estonia\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption'])\n",
    "        df['target_rolling_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'product_type', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption'])\n",
    "        df['target_rolling_allp_avg_24h_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=24, min_periods=1).mean())\n",
    "\n",
    "        grouped = df.groupby(['is_business', 'is_consumption', 'hour'])\n",
    "        df['target_rolling_allp_avg_hour_7d_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "        # grouped = df.groupby(['is_business', 'is_consumption', 'hour', 'day'])\n",
    "        # df['target_rolling_allp_avg_hour_hour_day_4w_estonia'] = grouped['target'].transform(lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "        \n",
    "        df = df.drop(['hour', 'day'], axis=1)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def init_revealed_targets(self, df):\n",
    "        df['datetime'] = pd.to_datetime(df.datetime)\n",
    "        df['datetime'] = df['datetime'] + dt.timedelta(days=2)\n",
    "        df = self.add_revealed_target_features(df)\n",
    "        return df\n",
    "    \n",
    "    def init_client(self, df):\n",
    "        ## LAG: 2 days\n",
    "        ## Add 2 days to date, join on date\n",
    "        df['date'] = pd.to_datetime(df.date).dt.date\n",
    "        df['date'] = df['date'] + dt.timedelta(days=2)\n",
    "        # df = self.get_data_block_id(df, 'date')\n",
    "        return df\n",
    "\n",
    "    def init_weather_mapping(self):\n",
    "        # https://www.kaggle.com/code/tsunotsuno/enefit-eda-baseline/notebook#Baseline\n",
    "        county_point_map = {\n",
    "            0: (59.4, 24.7), # \"HARJUMAA\"\n",
    "            1 : (58.8, 22.7), # \"HIIUMAA\"\n",
    "            2 : (59.1, 27.2), # \"IDA-VIRUMAA\"\n",
    "            3 : (58.8, 25.7), # \"JÄRVAMAA\"\n",
    "            4 : (58.8, 26.2), # \"JÕGEVAMAA\"\n",
    "            5 : (59.1, 23.7), # \"LÄÄNE-VIRUMAA\"\n",
    "            6 : (59.1, 23.7), # \"LÄÄNEMAA\"\n",
    "            7 : (58.5, 24.7), # \"PÄRNUMAA\"\n",
    "            8 : (58.2, 27.2), # \"PÕLVAMAA\"\n",
    "            9 : (58.8, 24.7), # \"RAPLAMAA\"\n",
    "            10 : (58.5, 22.7),# \"SAAREMAA\"\n",
    "            11 : (58.5, 26.7),# \"TARTUMAA\"\n",
    "            12 : (58.5, 25.2),# \"UNKNOWNN\" (center of the map)\n",
    "            13 : (57.9, 26.2),# \"VALGAMAA\"\n",
    "            14 : (58.2, 25.7),# \"VILJANDIMAA\"\n",
    "            15 : (57.9, 27.2) # \"VÕRUMAA\"\n",
    "        }\n",
    "        # Convert the dictionary to a list of tuples\n",
    "        data = [(county_code, lat, lon) for county_code, (lat, lon) in county_point_map.items()]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data, columns=['county', 'latitude', 'longitude'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_date_features(self, df):\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        df['quarter'] = df['datetime'].dt.quarter\n",
    "        df['day_of_week'] = df['datetime'].dt.day_of_week\n",
    "        df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "        df['week_of_year'] = df['datetime'].dt.isocalendar().week\n",
    "        df['is_weekend'] = df['datetime'].dt.day_of_week >= 5\n",
    "        df['is_month_start'] = df['datetime'].dt.is_month_start\n",
    "        df['is_month_end'] = df['datetime'].dt.is_month_end\n",
    "        df['is_quarter_start'] = df['datetime'].dt.is_quarter_start\n",
    "        df['is_quarter_end'] = df['datetime'].dt.is_quarter_end\n",
    "        df['is_year_start'] = df['datetime'].dt.is_year_start\n",
    "        df['is_year_end'] = df['datetime'].dt.is_year_end\n",
    "        df['season'] = df['datetime'].dt.month % 12 // 3 + 1\n",
    "        df['hour_sin'] = np.sin(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['datetime'].dt.hour * (2. * np.pi / 24))\n",
    "        # Calculate sin and cos for day of year\n",
    "        days_in_year = 365.25  # accounts for leap year\n",
    "        df['day_of_year_sin'] = np.sin((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        df['day_of_year_cos'] = np.cos((df['day_of_year'] - 1) * (2 * np.pi / days_in_year))\n",
    "        return df\n",
    "    \n",
    "    def add_ee_holidays(self, df):\n",
    "        import holidays\n",
    "        # Define Estonia public holidays\n",
    "        ee_holidays = holidays.CountryHoliday('EE')\n",
    "        \n",
    "        print(df['date'].isna().sum())\n",
    "        \n",
    "        def find_problem(x):\n",
    "            try:\n",
    "                return x in ee_holidays\n",
    "            except Exception as e:\n",
    "                print(x)\n",
    "                raise e\n",
    "\n",
    "        # Function to check if the date is a holiday\n",
    "        df['is_ee_holiday'] = df['date'].apply(lambda x: x in ee_holidays)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def create_log_cols(self, df):\n",
    "        log_cols = ['target_lag_1h', 'target_lag_2h', 'target_lag_3h', 'target_lag_4h',\n",
    "       'target_lag_5h', 'target_lag_6h', 'target_lag_7h', 'target_lag_8h',\n",
    "       'target_lag_9h', 'target_lag_10h', 'target_lag_11h', 'target_lag_12h',\n",
    "       'target_lag_13h', 'target_lag_14h', 'target_lag_15h', 'target_lag_16h',\n",
    "       'target_lag_17h', 'target_lag_18h', 'target_lag_19h', 'target_lag_20h',\n",
    "       'target_lag_21h', 'target_lag_22h', 'target_lag_23h', 'target_lag_24h',\n",
    "       'target_lag_48h', 'target_lag_72h', 'target_lag_96h', 'target_lag_120h',\n",
    "       'target_lag_144h', 'target_lag_168h', 'target_lag_264h',\n",
    "       'target_lag_288h', 'eic_count', 'installed_capacity', 'temperature', 'dewpoint', 'rain',\n",
    "       'snowfall', 'surface_pressure', 'cloudcover_total', 'cloudcover_low',\n",
    "       'cloudcover_mid', 'cloudcover_high', 'windspeed_10m',\n",
    "       'winddirection_10m', 'shortwave_radiation', 'direct_solar_radiation',\n",
    "       'diffuse_radiation', 'temperature_hw_means', 'dewpoint_hw_means',\n",
    "       'rain_hw_means', 'snowfall_hw_means', 'surface_pressure_hw_means',\n",
    "       'cloudcover_total_hw_means', 'cloudcover_low_hw_means',\n",
    "       'cloudcover_mid_hw_means', 'cloudcover_high_hw_means',\n",
    "       'windspeed_10m_hw_means', 'winddirection_10m_hw_means',\n",
    "       'shortwave_radiation_hw_means', 'direct_solar_radiation_hw_means',\n",
    "       'diffuse_radiation_hw_means', 'temperature_hw_variances',\n",
    "       'dewpoint_hw_variances', 'rain_hw_variances', 'snowfall_hw_variances',\n",
    "       'surface_pressure_hw_variances', 'cloudcover_total_hw_variances',\n",
    "       'cloudcover_low_hw_variances', 'cloudcover_mid_hw_variances',\n",
    "       'cloudcover_high_hw_variances', 'windspeed_10m_hw_variances',\n",
    "       'winddirection_10m_hw_variances', 'shortwave_radiation_hw_variances',\n",
    "       'direct_solar_radiation_hw_variances', 'diffuse_radiation_hw_variances',\n",
    "       'temperature_hw_lagged', 'dewpoint_hw_lagged', 'rain_hw_lagged',\n",
    "       'snowfall_hw_lagged', 'surface_pressure_hw_lagged',\n",
    "       'cloudcover_total_hw_lagged', 'cloudcover_low_hw_lagged', 'cloudcover_mid_hw_lagged',\n",
    "       'cloudcover_high_hw_lagged', 'windspeed_10m_hw_lagged',\n",
    "       'winddirection_10m_hw_lagged', 'shortwave_radiation_hw_lagged',\n",
    "       'direct_solar_radiation_hw_lagged', 'diffuse_radiation_hw_lagged',\n",
    "       'temperature_hw_means_hw_lagged', 'dewpoint_hw_means_hw_lagged',\n",
    "       'rain_hw_means_hw_lagged', 'snowfall_hw_means_hw_lagged',\n",
    "       'surface_pressure_hw_means_hw_lagged',\n",
    "       'cloudcover_total_hw_means_hw_lagged',\n",
    "       'cloudcover_low_hw_means_hw_lagged',\n",
    "       'cloudcover_mid_hw_means_hw_lagged',\n",
    "       'cloudcover_high_hw_means_hw_lagged',\n",
    "       'windspeed_10m_hw_means_hw_lagged',\n",
    "       'winddirection_10m_hw_means_hw_lagged',\n",
    "       'shortwave_radiation_hw_means_hw_lagged',\n",
    "       'direct_solar_radiation_hw_means_hw_lagged',\n",
    "       'diffuse_radiation_hw_means_hw_lagged',\n",
    "       'temperature_hw_variances_hw_lagged', 'dewpoint_hw_variances_hw_lagged',\n",
    "       'rain_hw_variances_hw_lagged', 'snowfall_hw_variances_hw_lagged',\n",
    "       'surface_pressure_hw_variances_hw_lagged',\n",
    "       'cloudcover_total_hw_variances_hw_lagged',\n",
    "       'cloudcover_low_hw_variances_hw_lagged',\n",
    "       'cloudcover_mid_hw_variances_hw_lagged',\n",
    "       'cloudcover_high_hw_variances_hw_lagged',\n",
    "       'windspeed_10m_hw_variances_hw_lagged',\n",
    "       'winddirection_10m_hw_variances_hw_lagged',\n",
    "       'shortwave_radiation_hw_variances_hw_lagged',\n",
    "       'direct_solar_radiation_hw_variances_hw_lagged',\n",
    "       'diffuse_radiation_hw_variances_hw_lagged', 'temperature_fw', 'dewpoint_fw', 'cloudcover_high_fw',\n",
    "       'cloudcover_low_fw', 'cloudcover_mid_fw', 'cloudcover_total_fw',\n",
    "       '10_metre_u_wind_component', '10_metre_v_wind_component',\n",
    "       'direct_solar_radiation_fw', 'surface_solar_radiation_downwards',\n",
    "       'snowfall_fw', 'total_precipitation', 'euros_per_mwh', 'mean_euros_per_mwh_last_week',\n",
    "       'mean_euros_per_mwh_same_hour_last_week', 'yesterdays_euros_per_mwh',\n",
    "       'euros_per_mwh_24h_average_price', 'lowest_price_per_mwh',\n",
    "       'highest_price_per_mwh', 'lowest_price_3d_avg', 'highest_price_3d_avg',\n",
    "       'lowest_price_7d_avg', 'highest_price_7d_avg', 'lowest_price_14d_avg',\n",
    "       'highest_price_14d_avg']\n",
    "        \n",
    "        log_cols = [col for col in log_cols if col in df.columns]\n",
    "        \n",
    "        dff = np.log1p(df[log_cols] )\n",
    "        dff.rename(columns={col: col + \"_log\" for col in log_cols}, inplace=True)\n",
    "        return pd.concat([df, dff], axis=1)\n",
    "        \n",
    "    \n",
    "    def remove_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'row_id',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                    'id',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def remove_test_cols(self, df):\n",
    "        col_list = ['datetime',\n",
    "                   'prediction_unit_id',\n",
    "                    'date_train',\n",
    "                    'hour_part',\n",
    "                   'date_client',\n",
    "                    'forecast_date_elec_price',\n",
    "                    'origin_date_elec_price',\n",
    "                    'forecast_date_gas_price',\n",
    "                    'origin_date_gas_price',\n",
    "                    'datetime_hist_weath',\n",
    "                   'hour_part_hist_weath_latest',\n",
    "                    'datetime_hist_weath_latest',\n",
    "                   'origin_datetime',\n",
    "                   'hour_part_fore_weath',\n",
    "                    'datetime',\n",
    "                     'data_block_id',\n",
    "                     'row_id',\n",
    "                     'prediction_unit_id',\n",
    "                     'date',\n",
    "                    'data_block_id_rt',\n",
    "                     'row_id_rt',\n",
    "                     'prediction_unit_id_rt',\n",
    "                    'data_block_id_client',\n",
    "                    'latitude',\n",
    "                     'longitude',\n",
    "                     'data_block_id_hw',\n",
    "                    'start_time',\n",
    "                     'end_time',\n",
    "                     'time_code',\n",
    "                     'group',\n",
    "                    'data_block_id_hw_means',\n",
    "                    'data_block_id_hw_variances',\n",
    "                     'location_id',\n",
    "                     'date_hw',\n",
    "                     'datetime_hw_lagged',\n",
    "                    'latitude_hw_lagged',\n",
    "                     'longitude_hw_lagged',\n",
    "                     'data_block_id_hw_lagged',\n",
    "                     'start_time_hw_lagged',\n",
    "                     'end_time_hw_lagged',\n",
    "                     'time_code_hw_lagged',\n",
    "                     'group_hw_lagged',\n",
    "                    'data_block_id_hw_means_hw_lagged',\n",
    "                    'data_block_id_hw_variances_hw_lagged',\n",
    "                    'location_id_hw_lagged',\n",
    "                     'latitude_fw',\n",
    "                     'longitude_fw',\n",
    "                     'origin_datetime',\n",
    "                    'data_block_id_fw',\n",
    "                     'forecast_datetime',\n",
    "                    'data_block_id_elec',\n",
    "                    'forecast_date',\n",
    "                    'origin_date',\n",
    "                     'data_block_id_gasp',\n",
    "                    'id'\n",
    "                   ]\n",
    "        columns_to_drop = [col for col in col_list if col in df.columns]\n",
    "        df = df.drop(columns_to_drop, axis=1)\n",
    "        return df\n",
    "    \n",
    "    def join_data(self, train, revealed_targets, client, historical_weather, forecast_weather, electricity_prices, gas_prices):\n",
    "        df = train\n",
    "        df = df.merge(revealed_targets, how='left', on=('datetime', 'county', 'is_business', 'product_type', 'is_consumption'), suffixes=('', '_rt'))\n",
    "        df = df.merge(client, how='left', on=('date', 'county', 'is_business', 'product_type'), suffixes=('', '_client'))\n",
    "        df = df.merge(historical_weather, how='left', on=('datetime', 'county'), suffixes=('', '_hw'))\n",
    "        df = df.merge(forecast_weather, how='left', on=('datetime', 'county'), suffixes=('', '_fw'))\n",
    "        df = df.merge(electricity_prices, how='left', on='datetime', suffixes=('', '_elec'))\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.merge(gas_prices, how='left', on='date', suffixes=('', '_gasp'))\n",
    "        df = self.add_date_features(df)\n",
    "        df = self.add_ee_holidays(df)\n",
    "        return df\n",
    "    \n",
    "    def add_test_data(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        dfs = [test.copy(), revealed_targets, client, historical_weather,\n",
    "                 forecast_weather, electricity_prices, gas_prices]\n",
    "        for i, df in enumerate(dfs):\n",
    "            if 'datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.datetime)\n",
    "                col = 'datetime'\n",
    "            if 'prediction_datetime' in df.columns:\n",
    "                df['datetime'] = pd.to_datetime(df.prediction_datetime)\n",
    "                col = 'datetime'\n",
    "            if 'forecast_date' in df.columns:\n",
    "                df['forecast_date'] = pd.to_datetime(df['forecast_date'])\n",
    "                col = 'forecast_date'\n",
    "            if 'forecast_datetime' in df.columns:\n",
    "                df['forecast_datetime'] = pd.to_datetime(df['forecast_datetime'])\n",
    "                col = 'forecast_datetime'\n",
    "                \n",
    "            self.test_orig_dfs[i] = pd.concat([ self.test_orig_dfs[i], df ])          \n",
    "        \n",
    "        \n",
    "    \n",
    "    def process_test_data_timestep(self, test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices):\n",
    "        #append test data to test data cache\n",
    "        self.add_test_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        # process test data\n",
    "        test = self.init_train(self.test_orig_dfs[0])\n",
    "        revealed_targets = self.init_revealed_targets(self.test_orig_dfs[1])\n",
    "        client = self.init_client(self.test_orig_dfs[2])\n",
    "        historical_weather = self.init_historical_weather(self.test_orig_dfs[3])\n",
    "        forecast_weather = self.init_forecast_weather(self.test_orig_dfs[4])\n",
    "        electricity_prices = self.init_electricity(self.test_orig_dfs[5])\n",
    "        gas_prices = self.init_gas_prices(self.test_orig_dfs[6])\n",
    "        df_all_cols = self.join_data(test, revealed_targets, client, historical_weather,\n",
    "            forecast_weather, electricity_prices, gas_prices)\n",
    "        if self.add_log_cols:\n",
    "            df_all_cols = self.create_log_cols(df_all_cols)\n",
    "        df = self.remove_test_cols(df_all_cols)\n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>is_business</th>\n",
       "      <th>product_type</th>\n",
       "      <th>target</th>\n",
       "      <th>is_consumption</th>\n",
       "      <th>target_rt</th>\n",
       "      <th>target_lag_1h</th>\n",
       "      <th>target_lag_2h</th>\n",
       "      <th>target_lag_3h</th>\n",
       "      <th>target_lag_4h</th>\n",
       "      <th>...</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_year_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>season</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "      <th>is_ee_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>96.590</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17.314</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.861693</td>\n",
       "      <td>-0.507430</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018609</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197.233</td>\n",
       "      <td>1</td>\n",
       "      <td>184.072</td>\n",
       "      <td>171.092</td>\n",
       "      <td>168.933</td>\n",
       "      <td>174.920</td>\n",
       "      <td>170.068</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018610</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.501</td>\n",
       "      <td>25.884</td>\n",
       "      <td>83.535</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018611</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.404</td>\n",
       "      <td>1</td>\n",
       "      <td>38.646</td>\n",
       "      <td>47.690</td>\n",
       "      <td>34.806</td>\n",
       "      <td>29.202</td>\n",
       "      <td>21.654</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018612</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.512</td>\n",
       "      <td>34.657</td>\n",
       "      <td>122.195</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018613</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>196.240</td>\n",
       "      <td>1</td>\n",
       "      <td>183.756</td>\n",
       "      <td>190.316</td>\n",
       "      <td>172.973</td>\n",
       "      <td>141.646</td>\n",
       "      <td>93.817</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>-0.846602</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2018614 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         county  is_business  product_type   target  is_consumption  \\\n",
       "0             0            0             1    0.713               0   \n",
       "1             0            0             1   96.590               1   \n",
       "2             0            0             2    0.000               0   \n",
       "3             0            0             2   17.314               1   \n",
       "4             0            0             3    2.904               0   \n",
       "...         ...          ...           ...      ...             ...   \n",
       "2018609      15            1             0  197.233               1   \n",
       "2018610      15            1             1    0.000               0   \n",
       "2018611      15            1             1   28.404               1   \n",
       "2018612      15            1             3    0.000               0   \n",
       "2018613      15            1             3  196.240               1   \n",
       "\n",
       "         target_rt  target_lag_1h  target_lag_2h  target_lag_3h  \\\n",
       "0              NaN            NaN            NaN            NaN   \n",
       "1              NaN            NaN            NaN            NaN   \n",
       "2              NaN            NaN            NaN            NaN   \n",
       "3              NaN            NaN            NaN            NaN   \n",
       "4              NaN            NaN            NaN            NaN   \n",
       "...            ...            ...            ...            ...   \n",
       "2018609    184.072        171.092        168.933        174.920   \n",
       "2018610      0.000          0.000          2.501         25.884   \n",
       "2018611     38.646         47.690         34.806         29.202   \n",
       "2018612      0.000          0.000          4.512         34.657   \n",
       "2018613    183.756        190.316        172.973        141.646   \n",
       "\n",
       "         target_lag_4h  ...  is_quarter_start  is_quarter_end  is_year_start  \\\n",
       "0                  NaN  ...             False           False          False   \n",
       "1                  NaN  ...             False           False          False   \n",
       "2                  NaN  ...             False           False          False   \n",
       "3                  NaN  ...             False           False          False   \n",
       "4                  NaN  ...             False           False          False   \n",
       "...                ...  ...               ...             ...            ...   \n",
       "2018609        170.068  ...             False           False          False   \n",
       "2018610         83.535  ...             False           False          False   \n",
       "2018611         21.654  ...             False           False          False   \n",
       "2018612        122.195  ...             False           False          False   \n",
       "2018613         93.817  ...             False           False          False   \n",
       "\n",
       "         is_year_end  season  hour_sin  hour_cos  day_of_year_sin  \\\n",
       "0              False       4  0.000000  1.000000        -0.861693   \n",
       "1              False       4  0.000000  1.000000        -0.861693   \n",
       "2              False       4  0.000000  1.000000        -0.861693   \n",
       "3              False       4  0.000000  1.000000        -0.861693   \n",
       "4              False       4  0.000000  1.000000        -0.861693   \n",
       "...              ...     ...       ...       ...              ...   \n",
       "2018609        False       2 -0.258819  0.965926         0.532227   \n",
       "2018610        False       2 -0.258819  0.965926         0.532227   \n",
       "2018611        False       2 -0.258819  0.965926         0.532227   \n",
       "2018612        False       2 -0.258819  0.965926         0.532227   \n",
       "2018613        False       2 -0.258819  0.965926         0.532227   \n",
       "\n",
       "         day_of_year_cos  is_ee_holiday  \n",
       "0              -0.507430          False  \n",
       "1              -0.507430          False  \n",
       "2              -0.507430          False  \n",
       "3              -0.507430          False  \n",
       "4              -0.507430          False  \n",
       "...                  ...            ...  \n",
       "2018609        -0.846602          False  \n",
       "2018610        -0.846602          False  \n",
       "2018611        -0.846602          False  \n",
       "2018612        -0.846602          False  \n",
       "2018613        -0.846602          False  \n",
       "\n",
       "[2018614 rows x 300 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_processor_lgbm3_new_pandas.pkl', 'rb') as f:\n",
    "    data_processor = pickle.load(f)\n",
    "data_processor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.67 s\n",
      "Wall time: 16.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "county             0\n",
       "is_business        0\n",
       "product_type       0\n",
       "target             0\n",
       "is_consumption     0\n",
       "                  ..\n",
       "hour_sin           0\n",
       "hour_cos           0\n",
       "day_of_year_sin    0\n",
       "day_of_year_cos    0\n",
       "is_ee_holiday      0\n",
       "Length: 300, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "processed_df_no_na, means = fill_drop_na(data_processor.df)\n",
    "processed_df_no_na.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the date strings in cv_ranges to datetime objects\n",
    "datetime_cv_ranges = [(to_datetime(start), to_datetime(end)) for start, end in cv_ranges_corrected]\n",
    "datetime_cv_ranges\n",
    "\n",
    "date_filter = data_processor.df_all_cols.date[processed_df_no_na.index]\n",
    "date_filter\n",
    "\n",
    "cv1_train = processed_df_no_na[date_filter <= datetime_cv_ranges[0][0]]\n",
    "cv1_test = processed_df_no_na[(date_filter <= datetime_cv_ranges[0][1]) & (date_filter > datetime_cv_ranges[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in range(5):\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target']\n",
    "        drop_cols = ['target', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train rows: 1129738\n",
      "Val rows: 171264\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 0: Train Mean Absolute Error: 19.73059532203093\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 0: Fold Val Mean Absolute Error: 40.898223366758465\n",
      "\n",
      "\n",
      "Fold 1\n",
      "Train rows: 1304266\n",
      "Val rows: 173328\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 1: Train Mean Absolute Error: 20.791241369742096\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 1: Fold Val Mean Absolute Error: 33.32857701239144\n",
      "\n",
      "\n",
      "Fold 2\n",
      "Train rows: 1480810\n",
      "Val rows: 169632\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 2: Train Mean Absolute Error: 21.019241339103825\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 2: Fold Val Mean Absolute Error: 34.197501999883\n",
      "\n",
      "\n",
      "Fold 3\n",
      "Train rows: 1653658\n",
      "Val rows: 167820\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 3: Train Mean Absolute Error: 20.979197920742504\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 3: Fold Val Mean Absolute Error: 50.75865180471505\n",
      "\n",
      "\n",
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_df_no_na\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 34\u001b[0m, in \u001b[0;36mtrain_cv\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     31\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.7466999841658806\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3.2140838539606458\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.13753679743025782\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_bin\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m250\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_data_in_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m150\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5593\u001b[39m,  \n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m22\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweedie\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m     33\u001b[0m clf2 \u001b[38;5;241m=\u001b[39m LGBMRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mclf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###############   Target   #################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\lgbm\\lib\\site-packages\\lightgbm\\sklearn.py:1092\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1077\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1089\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1092\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\lgbm\\lib\\site-packages\\lightgbm\\sklearn.py:885\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    882\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    883\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 885\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\lgbm\\lib\\site-packages\\lightgbm\\engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    269\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m    270\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    271\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    272\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[0;32m    273\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[0;32m    274\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m--> 276\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\lgbm\\lib\\site-packages\\lightgbm\\basic.py:3891\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   3889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   3890\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 3891\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3892\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   3895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mskel\\.conda\\envs\\lgbm\\lib\\site-packages\\lightgbm\\basic.py:263\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in [4]:\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target']\n",
    "        drop_cols = ['target', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 0.7466999841658806, 'lambda_l2': 3.2140838539606458, 'learning_rate': 0.13753679743025782, 'max_bin': 250, 'min_data_in_leaf': 150, 'n_estimators': 5593,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 4: Train Mean Absolute Error: 22.034074867253608\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=150, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=150\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.7466999841658806, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7466999841658806\n",
      "[LightGBM] [Warning] lambda_l2 is set=3.2140838539606458, reg_lambda=0.0 will be ignored. Current value: lambda_l2=3.2140838539606458\n",
      "For fold 4: Fold Val Mean Absolute Error: 68.90704488946972\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'lambda_l1': 2.388935540285825, 'lambda_l2': 0.6070283817241626, 'learning_rate': 0.2514296168463765, 'max_bin': 188, 'min_data_in_leaf': 124, 'n_estimators': 4569}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "def inverse_tic(preds, train):\n",
    "    return preds/1000 * train.installed_capacity\n",
    "\n",
    "def train_cv(df):\n",
    "    for i in [4]:\n",
    "        train = df[date_filter <= datetime_cv_ranges[i][0]]\n",
    "        val = df[(date_filter <= datetime_cv_ranges[i][1]) & (date_filter > datetime_cv_ranges[i][0])]\n",
    "        print(f\"Fold {i}\")\n",
    "        print(f\"Train rows: {len(train)}\")\n",
    "        print(f\"Val rows: {len(val)}\")\n",
    "        \n",
    "        target_cols = ['target']\n",
    "        drop_cols = ['target', 'quarter', 'season', 'is_year_end', 'is_year_start', 'is_month_end', 'is_quarter_end', 'is_quarter_start', 'is_month_start', 'snowfall_hw_lagged', 'snowfall_hw_variances',\n",
    "                    'snowfall_fw', 'snowfall_hw_means']\n",
    "        \n",
    "        df_train_target = train[target_cols]\n",
    "        df_train_data = train.drop(drop_cols, axis=1)\n",
    "        \n",
    "        df_val_target2 = val[target_cols]\n",
    "        df_val_data2 = val.drop(drop_cols, axis=1)\n",
    "        \n",
    "        cat_features = [\"county\", \"is_business\", \"product_type\", \"is_consumption\", 'month', 'hour', 'quarter',\n",
    "               'day_of_week', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start' ,'is_quarter_end', \n",
    "                'is_year_start', 'is_year_end', 'season', ] + list(df_train_data.columns[df_train_data.columns.str.contains('is_na')])\n",
    "        cat_features = [c for c in cat_features if c in df_train_data.columns]\n",
    "        \n",
    "        # We leave max_depth as -1\n",
    "        # Tune num_leaves, default is 31, let's double it\n",
    "        params = {'lambda_l1': 2.388935540285825, 'lambda_l2': 0.6070283817241626, 'learning_rate': 0.2514296168463765, 'max_bin': 188, 'min_data_in_leaf': 124, 'n_estimators': 4569,  \n",
    "                'metric': 'mae', 'n_jobs': 22, 'boosting': 'dart', 'objective': 'tweedie', 'device':'gpu'}\n",
    "        clf2 = LGBMRegressor(**params, random_state=42, verbose=0, importance_type='gain')\n",
    "        clf2.fit(df_train_data, df_train_target.target, categorical_feature=cat_features)\n",
    "\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        \n",
    "        print(\"###############   Target   #################\")\n",
    "        y_pred = clf2.predict(df_train_data)\n",
    "        y_pred\n",
    "        # Assuming you have two pandas Series: y_true and y_pred\n",
    "        mae = mean_absolute_error(df_train_target.target, y_pred)\n",
    "        print(f\"For fold {i}: Train Mean Absolute Error:\", mae)\n",
    "\n",
    "        y_pred_val = clf2.predict(df_val_data2)\n",
    "        y_pred_val\n",
    "\n",
    "        mae = mean_absolute_error(df_val_target2.target, y_pred_val)\n",
    "        print(f\"For fold {i}: Fold Val Mean Absolute Error:\", mae)\n",
    "        \n",
    "        importance = pd.DataFrame({'importance':clf2.feature_importances_, 'name':clf2.feature_name_})\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        # display(importance.head(30))\n",
    "        # display(importance.tail(30))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Train rows: 1824598\n",
      "Val rows: 176496\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.388935540285825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.388935540285825\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6070283817241626, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6070283817241626\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.388935540285825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.388935540285825\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6070283817241626, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6070283817241626\n",
      "###############   Target   #################\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.388935540285825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.388935540285825\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6070283817241626, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6070283817241626\n",
      "For fold 4: Train Mean Absolute Error: 19.848150075496363\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=124, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=124\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.388935540285825, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.388935540285825\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.6070283817241626, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.6070283817241626\n",
      "For fold 4: Fold Val Mean Absolute Error: 69.1607015215553\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cv(processed_df_no_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enefit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
